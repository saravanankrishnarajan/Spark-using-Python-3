{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7275d3",
   "metadata": {},
   "source": [
    "## Section 14 Apache spark using PYthon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49857272",
   "metadata": {},
   "source": [
    "### 158 Starting Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b744a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fbc1588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef5a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config(\"spark.ui.port\",\"0\"). \\\n",
    "        config(\"spark.sql.warehouse.dir\",f\"/user/{username}/warehouse\"). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f\"{username} | Python - Data processing - Overview\"). \\\n",
    "        master(\"yarn\"). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2161af70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bab83cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g02.itversity.com:41579\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>itv011204 | Python - Data processing - Overview</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7eff013f1160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ac5e543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameReader at 0x7f3c678a96d8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c9bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9acaf4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark. \\\n",
    "    read. \\\n",
    "    csv('/public/retail_db/orders',\n",
    "        schema='''\n",
    "            order_id INT, \n",
    "            order_date STRING, \n",
    "            order_customer_id INT, \n",
    "            order_status STRING\n",
    "        '''\n",
    "       ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3148169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n",
      "|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n",
      "|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n",
      "|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n",
      "|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n",
      "|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n",
      "|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n",
      "|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n",
      "|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n",
      "|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n",
      "|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n",
      "|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n",
      "|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n",
      "|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n",
      "|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n",
      "|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark. \\\n",
    "    read. \\\n",
    "    json('/public/retail_db_json/orders'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806c8b1",
   "metadata": {},
   "source": [
    "### 160 Understanding Airlines data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6add555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9de4729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fcb0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\",\"0\"). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c9545cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g02.itversity.com:41579\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>itv011204 | Python - Data processing - Overview</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7eff013f1160>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /public/airlines_all/airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b3f7803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   2 hdfs supergroup     64.0 M 2021-01-28 08:56 /public/airlines_all/airlines/part-00000\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls -h /public/airlines_all/airlines/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8295da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = spark.read. \\\n",
    "    text(\"/public/airlines_all/airlines/part-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2228a6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(airlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2c6b632",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method show in module pyspark.sql.dataframe:\n",
      "\n",
      "show(n=20, truncate=True, vertical=False) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Prints the first ``n`` rows to the console.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    n : int, optional\n",
      "        Number of rows to show.\n",
      "    truncate : bool, optional\n",
      "        If set to ``True``, truncate strings longer than 20 chars by default.\n",
      "        If set to a number greater than one, truncates long strings to length ``truncate``\n",
      "        and align cells right.\n",
      "    vertical : bool, optional\n",
      "        If set to ``True``, print output rows vertically (one line\n",
      "        per column value).\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df\n",
      "    DataFrame[age: int, name: string]\n",
      "    >>> df.show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  2|Alice|\n",
      "    |  5|  Bob|\n",
      "    +---+-----+\n",
      "    >>> df.show(truncate=3)\n",
      "    +---+----+\n",
      "    |age|name|\n",
      "    +---+----+\n",
      "    |  2| Ali|\n",
      "    |  5| Bob|\n",
      "    +---+----+\n",
      "    >>> df.show(vertical=True)\n",
      "    -RECORD 0-----\n",
      "     age  | 2\n",
      "     name | Alice\n",
      "    -RECORD 1-----\n",
      "     age  | 5\n",
      "     name | Bob\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(airlines.show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fe56976",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                                                                                                                |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay,IsArrDelayed,IsDepDelayed|\n",
      "|1987,10,14,3,741,730,912,849,PS,1451,NA,91,79,NA,23,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES                                                                                                                                                                                                                               |\n",
      "|1987,10,15,4,729,730,903,849,PS,1451,NA,94,79,NA,14,-1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,NO                                                                                                                                                                                                                                |\n",
      "|1987,10,17,6,741,730,918,849,PS,1451,NA,97,79,NA,29,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES                                                                                                                                                                                                                               |\n",
      "|1987,10,18,7,729,730,847,849,PS,1451,NA,78,79,NA,-2,-1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,NO,NO                                                                                                                                                                                                                                 |\n",
      "|1987,10,19,1,749,730,922,849,PS,1451,NA,93,79,NA,33,19,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES                                                                                                                                                                                                                               |\n",
      "|1987,10,21,3,728,730,848,849,PS,1451,NA,80,79,NA,-1,-2,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,NO,NO                                                                                                                                                                                                                                 |\n",
      "|1987,10,22,4,728,730,852,849,PS,1451,NA,84,79,NA,3,-2,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,NO                                                                                                                                                                                                                                 |\n",
      "|1987,10,23,5,731,730,902,849,PS,1451,NA,91,79,NA,13,1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES                                                                                                                                                                                                                                |\n",
      "|1987,10,24,6,744,730,908,849,PS,1451,NA,84,79,NA,19,14,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES                                                                                                                                                                                                                               |\n",
      "|1987,10,25,7,729,730,851,849,PS,1451,NA,82,79,NA,2,-1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,NO                                                                                                                                                                                                                                 |\n",
      "|1987,10,26,1,735,730,904,849,PS,1451,NA,89,79,NA,15,5,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES                                                                                                                                                                                                                                |\n",
      "|1987,10,28,3,741,725,919,855,PS,1451,NA,98,90,NA,24,16,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES                                                                                                                                                                                                                               |\n",
      "|1987,10,29,4,742,725,906,855,PS,1451,NA,84,90,NA,11,17,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES                                                                                                                                                                                                                               |\n",
      "|1987,10,31,6,726,725,848,855,PS,1451,NA,82,90,NA,-7,1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,NO,YES                                                                                                                                                                                                                                 |\n",
      "|1987,10,1,4,936,915,1035,1001,PS,1451,NA,59,46,NA,34,21,SFO,RNO,192,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES                                                                                                                                                                                                                              |\n",
      "|1987,10,2,5,918,915,1017,1001,PS,1451,NA,59,46,NA,16,3,SFO,RNO,192,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES                                                                                                                                                                                                                               |\n",
      "|1987,10,3,6,928,915,1037,1001,PS,1451,NA,69,46,NA,36,13,SFO,RNO,192,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES                                                                                                                                                                                                                              |\n",
      "|1987,10,4,7,914,915,1003,1001,PS,1451,NA,49,46,NA,2,-1,SFO,RNO,192,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,NO                                                                                                                                                                                                                                |\n",
      "|1987,10,5,1,1042,915,1129,1001,PS,1451,NA,47,46,NA,88,87,SFO,RNO,192,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES                                                                                                                                                                                                                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988da3aa",
   "metadata": {},
   "source": [
    "### 161 Inferring Schema using Spark DataFrame APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33067f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_part_00000 = spark.read. \\\n",
    "    csv(\"/public/airlines_all/airlines/part-00000\",\n",
    "        header=True,\n",
    "        inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a629b392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(airlines_part_00000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699f2a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|IsArrDelayed|IsDepDelayed|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+\n",
      "|1987|10   |14        |3        |741    |730       |912    |849       |PS           |1451     |NA     |91               |79            |NA     |23      |11      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|10   |15        |4        |729    |730       |903    |849       |PS           |1451     |NA     |94               |79            |NA     |14      |-1      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |NO          |\n",
      "|1987|10   |17        |6        |741    |730       |918    |849       |PS           |1451     |NA     |97               |79            |NA     |29      |11      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|10   |18        |7        |729    |730       |847    |849       |PS           |1451     |NA     |78               |79            |NA     |-2      |-1      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |NO          |NO          |\n",
      "|1987|10   |19        |1        |749    |730       |922    |849       |PS           |1451     |NA     |93               |79            |NA     |33      |19      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|10   |21        |3        |728    |730       |848    |849       |PS           |1451     |NA     |80               |79            |NA     |-1      |-2      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |NO          |NO          |\n",
      "|1987|10   |22        |4        |728    |730       |852    |849       |PS           |1451     |NA     |84               |79            |NA     |3       |-2      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |NO          |\n",
      "|1987|10   |23        |5        |731    |730       |902    |849       |PS           |1451     |NA     |91               |79            |NA     |13      |1       |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|10   |24        |6        |744    |730       |908    |849       |PS           |1451     |NA     |84               |79            |NA     |19      |14      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|10   |25        |7        |729    |730       |851    |849       |PS           |1451     |NA     |82               |79            |NA     |2       |-1      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |NO          |\n",
      "|1987|10   |26        |1        |735    |730       |904    |849       |PS           |1451     |NA     |89               |79            |NA     |15      |5       |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|10   |28        |3        |741    |725       |919    |855       |PS           |1451     |NA     |98               |90            |NA     |24      |16      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|10   |29        |4        |742    |725       |906    |855       |PS           |1451     |NA     |84               |90            |NA     |11      |17      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|10   |31        |6        |726    |725       |848    |855       |PS           |1451     |NA     |82               |90            |NA     |-7      |1       |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |NO          |YES         |\n",
      "|1987|10   |1         |4        |936    |915       |1035   |1001      |PS           |1451     |NA     |59               |46            |NA     |34      |21      |SFO   |RNO |192     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|10   |2         |5        |918    |915       |1017   |1001      |PS           |1451     |NA     |59               |46            |NA     |16      |3       |SFO   |RNO |192     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|10   |3         |6        |928    |915       |1037   |1001      |PS           |1451     |NA     |69               |46            |NA     |36      |13      |SFO   |RNO |192     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|10   |4         |7        |914    |915       |1003   |1001      |PS           |1451     |NA     |49               |46            |NA     |2       |-1      |SFO   |RNO |192     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |NO          |\n",
      "|1987|10   |5         |1        |1042   |915       |1129   |1001      |PS           |1451     |NA     |47               |46            |NA     |88      |87      |SFO   |RNO |192     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|10   |6         |2        |934    |915       |1024   |1001      |PS           |1451     |NA     |50               |46            |NA     |23      |19      |SFO   |RNO |192     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines_part_00000.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f97c47b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      " |-- IsArrDelayed: string (nullable = true)\n",
      " |-- IsDepDelayed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines_part_00000.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b9fafa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Year,IntegerType,true),StructField(Month,IntegerType,true),StructField(DayofMonth,IntegerType,true),StructField(DayOfWeek,IntegerType,true),StructField(DepTime,StringType,true),StructField(CRSDepTime,IntegerType,true),StructField(ArrTime,StringType,true),StructField(CRSArrTime,IntegerType,true),StructField(UniqueCarrier,StringType,true),StructField(FlightNum,IntegerType,true),StructField(TailNum,StringType,true),StructField(ActualElapsedTime,StringType,true),StructField(CRSElapsedTime,IntegerType,true),StructField(AirTime,StringType,true),StructField(ArrDelay,StringType,true),StructField(DepDelay,StringType,true),StructField(Origin,StringType,true),StructField(Dest,StringType,true),StructField(Distance,StringType,true),StructField(TaxiIn,StringType,true),StructField(TaxiOut,StringType,true),StructField(Cancelled,IntegerType,true),StructField(CancellationCode,StringType,true),StructField(Diverted,IntegerType,true),StructField(CarrierDelay,StringType,true),StructField(WeatherDelay,StringType,true),StructField(NASDelay,StringType,true),StructField(SecurityDelay,StringType,true),StructField(LateAircraftDelay,StringType,true),StructField(IsArrDelayed,StringType,true),StructField(IsDepDelayed,StringType,true)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_part_00000.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "325f0d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.StructType"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(airlines_part_00000.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e73ff32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_schema = spark.read. \\\n",
    "    csv(\"/public/airlines_all/airlines/part-00000\",\n",
    "       header=True,\n",
    "       inferSchema=True). \\\n",
    "    schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a457cb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.StructType"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(airlines_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57bb431a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n",
      "\n",
      "csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None, locale=None, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None, modifiedBefore=None, modifiedAfter=None, unescapedQuoteHandling=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      "    \n",
      "    This function will go through the input once to determine the input schema if\n",
      "    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      "    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path : str or list\n",
      "        string, or list of strings, for input path(s),\n",
      "        or RDD of Strings storing CSV rows.\n",
      "    schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      "        an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "    sep : str, optional\n",
      "        sets a separator (one or more characters) for each field and value. If None is\n",
      "        set, it uses the default value, ``,``.\n",
      "    encoding : str, optional\n",
      "        decodes the CSV files by the given encoding type. If None is set,\n",
      "        it uses the default value, ``UTF-8``.\n",
      "    quote : str, optional\n",
      "        sets a single character used for escaping quoted values where the\n",
      "        separator can be part of the value. If None is set, it uses the default\n",
      "        value, ``\"``. If you would like to turn off quotations, you need to set an\n",
      "        empty string.\n",
      "    escape : str, optional\n",
      "        sets a single character used for escaping quotes inside an already\n",
      "        quoted value. If None is set, it uses the default value, ``\\``.\n",
      "    comment : str, optional\n",
      "        sets a single character used for skipping lines beginning with this\n",
      "        character. By default (None), it is disabled.\n",
      "    header : str or bool, optional\n",
      "        uses the first line as names of columns. If None is set, it uses the\n",
      "        default value, ``false``.\n",
      "    \n",
      "        .. note:: if the given path is a RDD of Strings, this header\n",
      "            option will remove all lines same with the header if exists.\n",
      "    \n",
      "    inferSchema : str or bool, optional\n",
      "        infers the input schema automatically from data. It requires one extra\n",
      "        pass over the data. If None is set, it uses the default value, ``false``.\n",
      "    enforceSchema : str or bool, optional\n",
      "        If it is set to ``true``, the specified or inferred schema will be\n",
      "        forcibly applied to datasource files, and headers in CSV files will be\n",
      "        ignored. If the option is set to ``false``, the schema will be\n",
      "        validated against all headers in CSV files or the first header in RDD\n",
      "        if the ``header`` option is set to ``true``. Field names in the schema\n",
      "        and column names in CSV headers are checked by their positions\n",
      "        taking into account ``spark.sql.caseSensitive``. If None is set,\n",
      "        ``true`` is used by default. Though the default value is ``true``,\n",
      "        it is recommended to disable the ``enforceSchema`` option\n",
      "        to avoid incorrect results.\n",
      "    ignoreLeadingWhiteSpace : str or bool, optional\n",
      "        A flag indicating whether or not leading whitespaces from\n",
      "        values being read should be skipped. If None is set, it\n",
      "        uses the default value, ``false``.\n",
      "    ignoreTrailingWhiteSpace : str or bool, optional\n",
      "        A flag indicating whether or not trailing whitespaces from\n",
      "        values being read should be skipped. If None is set, it\n",
      "        uses the default value, ``false``.\n",
      "    nullValue : str, optional\n",
      "        sets the string representation of a null value. If None is set, it uses\n",
      "        the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
      "        applies to all supported types including the string type.\n",
      "    nanValue : str, optional\n",
      "        sets the string representation of a non-number value. If None is set, it\n",
      "        uses the default value, ``NaN``.\n",
      "    positiveInf : str, optional\n",
      "        sets the string representation of a positive infinity value. If None\n",
      "        is set, it uses the default value, ``Inf``.\n",
      "    negativeInf : str, optional\n",
      "        sets the string representation of a negative infinity value. If None\n",
      "        is set, it uses the default value, ``Inf``.\n",
      "    dateFormat : str, optional\n",
      "        sets the string that indicates a date format. Custom date formats\n",
      "        follow the formats at\n",
      "        `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      "        This applies to date type. If None is set, it uses the\n",
      "        default value, ``yyyy-MM-dd``.\n",
      "    timestampFormat : str, optional\n",
      "        sets the string that indicates a timestamp format.\n",
      "        Custom date formats follow the formats at\n",
      "        `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      "        This applies to timestamp type. If None is set, it uses the\n",
      "        default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      "    maxColumns : str or int, optional\n",
      "        defines a hard limit of how many columns a record can have. If None is\n",
      "        set, it uses the default value, ``20480``.\n",
      "    maxCharsPerColumn : str or int, optional\n",
      "        defines the maximum number of characters allowed for any given\n",
      "        value being read. If None is set, it uses the default value,\n",
      "        ``-1`` meaning unlimited length.\n",
      "    maxMalformedLogPerPartition : str or int, optional\n",
      "        this parameter is no longer used since Spark 2.2.0.\n",
      "        If specified, it is ignored.\n",
      "    mode : str, optional\n",
      "        allows a mode for dealing with corrupt records during parsing. If None is\n",
      "        set, it uses the default value, ``PERMISSIVE``. Note that Spark tries to\n",
      "        parse only required columns in CSV under column pruning. Therefore, corrupt\n",
      "        records can be different based on required set of fields. This behavior can\n",
      "        be controlled by ``spark.sql.csv.parser.columnPruning.enabled``\n",
      "        (enabled by default).\n",
      "    \n",
      "        * ``PERMISSIVE``: when it meets a corrupted record, puts the malformed string \\\n",
      "          into a field configured by ``columnNameOfCorruptRecord``, and sets malformed \\\n",
      "          fields to ``null``. To keep corrupt records, an user can set a string type \\\n",
      "          field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\n",
      "          schema does not have the field, it drops corrupt records during parsing. \\\n",
      "          A record with less/more tokens than schema is not a corrupted record to CSV. \\\n",
      "          When it meets a record having fewer tokens than the length of the schema, \\\n",
      "          sets ``null`` to extra fields. When the record has more tokens than the \\\n",
      "          length of the schema, it drops extra tokens.\n",
      "        * ``DROPMALFORMED``: ignores the whole corrupted records.\n",
      "        * ``FAILFAST``: throws an exception when it meets corrupted records.\n",
      "    \n",
      "    columnNameOfCorruptRecord : str, optional\n",
      "        allows renaming the new field having malformed string\n",
      "        created by ``PERMISSIVE`` mode. This overrides\n",
      "        ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      "        it uses the value specified in\n",
      "        ``spark.sql.columnNameOfCorruptRecord``.\n",
      "    multiLine : str or bool, optional\n",
      "        parse records, which may span multiple lines. If None is\n",
      "        set, it uses the default value, ``false``.\n",
      "    charToEscapeQuoteEscaping : str, optional\n",
      "        sets a single character used for escaping the escape for\n",
      "        the quote character. If None is set, the default value is\n",
      "        escape character when escape and quote characters are\n",
      "        different, ``\\0`` otherwise.\n",
      "    samplingRatio : str or float, optional\n",
      "        defines fraction of rows used for schema inferring.\n",
      "        If None is set, it uses the default value, ``1.0``.\n",
      "    emptyValue : str, optional\n",
      "        sets the string representation of an empty value. If None is set, it uses\n",
      "        the default value, empty string.\n",
      "    locale : str, optional\n",
      "        sets a locale as language tag in IETF BCP 47 format. If None is set,\n",
      "        it uses the default value, ``en-US``. For instance, ``locale`` is used while\n",
      "        parsing dates and timestamps.\n",
      "    lineSep : str, optional\n",
      "        defines the line separator that should be used for parsing. If None is\n",
      "        set, it covers all ``\\\\r``, ``\\\\r\\\\n`` and ``\\\\n``.\n",
      "        Maximum length is 1 character.\n",
      "    pathGlobFilter : str or bool, optional\n",
      "        an optional glob pattern to only include files with paths matching\n",
      "        the pattern. The syntax follows `org.apache.hadoop.fs.GlobFilter`.\n",
      "        It does not change the behavior of\n",
      "        `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n",
      "    recursiveFileLookup : str or bool, optional\n",
      "        recursively scan a directory for files. Using this option disables\n",
      "        `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n",
      "    \n",
      "        modification times occurring before the specified time. The provided timestamp\n",
      "        must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      "    modifiedBefore (batch only) : an optional timestamp to only include files with\n",
      "        modification times occurring before the specified time. The provided timestamp\n",
      "        must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      "    modifiedAfter (batch only) : an optional timestamp to only include files with\n",
      "        modification times occurring after the specified time. The provided timestamp\n",
      "        must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      "    unescapedQuoteHandling : str, optional\n",
      "        defines how the CsvParser will handle values with unescaped quotes. If None is\n",
      "        set, it uses the default value, ``STOP_AT_DELIMITER``.\n",
      "    \n",
      "        * ``STOP_AT_CLOSING_QUOTE``: If unescaped quotes are found in the input, accumulate\n",
      "          the quote character and proceed parsing the value as a quoted value, until a closing\n",
      "          quote is found.\n",
      "        * ``BACK_TO_DELIMITER``: If unescaped quotes are found in the input, consider the value\n",
      "          as an unquoted value. This will make the parser accumulate all characters of the current\n",
      "          parsed value until the delimiter is found. If no delimiter is found in the value, the\n",
      "          parser will continue accumulating characters from the input until a delimiter or line\n",
      "          ending is found.\n",
      "        * ``STOP_AT_DELIMITER``: If unescaped quotes are found in the input, consider the value\n",
      "          as an unquoted value. This will make the parser accumulate all characters until the\n",
      "          delimiter or a line ending is found in the input.\n",
      "        * ``SKIP_VALUE``: If unescaped quotes are found in the input, the content parsed\n",
      "          for the given value will be skipped and the value set in nullValue will be produced\n",
      "          instead.\n",
      "        * ``RAISE_ERROR``: If unescaped quotes are found in the input, a TextParsingException\n",
      "          will be thrown.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      "    >>> df.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "    >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
      "    >>> df2 = spark.read.csv(rdd)\n",
      "    >>> df2.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51a41542",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = spark.read. \\\n",
    "    schema(airlines_schema). \\\n",
    "    csv(\"/public/airlines_all/airlines/part*\",\n",
    "           header=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e85f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = spark.read. \\\n",
    "    csv(\"/public/airlines_all/airlines/part*\",\n",
    "           header=True,\n",
    "           schema=airlines_schema\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "769867a9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrame in module pyspark.sql.dataframe object:\n",
      "\n",
      "class DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      " |  A distributed collection of data grouped into named columns.\n",
      " |  \n",
      " |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
      " |  and can be created using various functions in :class:`SparkSession`::\n",
      " |  \n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |  Once created, it can be manipulated using the various domain-specific-language\n",
      " |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
      " |  \n",
      " |  To select a column from the :class:`DataFrame`, use the apply method::\n",
      " |  \n",
      " |      ageCol = people.age\n",
      " |  \n",
      " |  A more concrete example::\n",
      " |  \n",
      " |      # To create DataFrame using SparkSession\n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |      department = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |      people.filter(people.age > 30).join(department, people.deptId == department.id) \\\n",
      " |        .groupBy(department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n",
      " |  \n",
      " |  .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrame\n",
      " |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n",
      " |      pyspark.sql.pandas.conversion.PandasConversionMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |      Returns the :class:`Column` denoted by ``name``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.age).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |  \n",
      " |  __getitem__(self, item)\n",
      " |      Returns the column as a :class:`Column`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df['age']).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |      >>> df[ [\"name\", \"age\"]].collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df[ df.age > 3 ].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df[0] > 3].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  __init__(self, jdf, sql_ctx)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  agg(self, *exprs)\n",
      " |      Aggregate on the entire :class:`DataFrame` without groups\n",
      " |      (shorthand for ``df.groupBy().agg()``).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.agg({\"age\": \"max\"}).collect()\n",
      " |      [Row(max(age)=5)]\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.agg(F.min(df.age)).collect()\n",
      " |      [Row(min(age)=2)]\n",
      " |  \n",
      " |  alias(self, alias)\n",
      " |      Returns a new :class:`DataFrame` with an alias set.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      alias : str\n",
      " |          an alias name to be set for the :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df_as1 = df.alias(\"df_as1\")\n",
      " |      >>> df_as2 = df.alias(\"df_as2\")\n",
      " |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      " |      >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\")                 .sort(desc(\"df_as1.name\")).collect()\n",
      " |      [Row(name='Bob', name='Bob', age=5), Row(name='Alice', name='Alice', age=2)]\n",
      " |  \n",
      " |  approxQuantile(self, col, probabilities, relativeError)\n",
      " |      Calculates the approximate quantiles of numerical columns of a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The result of this algorithm has the following deterministic bound:\n",
      " |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      " |      probability `p` up to error `err`, then the algorithm will return\n",
      " |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      " |      close to (p * N). More precisely,\n",
      " |      \n",
      " |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      " |      \n",
      " |      This method implements a variation of the Greenwald-Khanna\n",
      " |      algorithm (with some speed optimizations). The algorithm was first\n",
      " |      present in [[https://doi.org/10.1145/375663.375670\n",
      " |      Space-efficient Online Computation of Quantile Summaries]]\n",
      " |      by Greenwald and Khanna.\n",
      " |      \n",
      " |      Note that null values will be ignored in numerical columns before calculation.\n",
      " |      For columns only containing null values, an empty list is returned.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col: str, tuple or list\n",
      " |          Can be a single column name, or a list of names for multiple columns.\n",
      " |      \n",
      " |          .. versionchanged:: 2.2\n",
      " |             Added support for multiple columns.\n",
      " |      probabilities : list or tuple\n",
      " |          a list of quantile probabilities\n",
      " |          Each number must belong to [0, 1].\n",
      " |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      " |      relativeError : float\n",
      " |          The relative target precision to achieve\n",
      " |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
      " |          could be very expensive. Note that values greater than 1 are\n",
      " |          accepted but give the same result as 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          the approximate quantiles at the given probabilities. If\n",
      " |          the input `col` is a string, the output is a list of floats. If the\n",
      " |          input `col` is a list or tuple of strings, the output is also a\n",
      " |          list, but each element in it is a list of floats, i.e., the output\n",
      " |          is a list of list of floats.\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      " |  \n",
      " |  checkpoint(self, eager=True)\n",
      " |      Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n",
      " |      logical plan of this :class:`DataFrame`, which is especially useful in iterative algorithms\n",
      " |      where the plan may grow exponentially. It will be saved to files inside the checkpoint\n",
      " |      directory set with :meth:`SparkContext.setCheckpointDir`.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eager : bool, optional\n",
      " |          Whether to checkpoint this :class:`DataFrame` immediately\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental.\n",
      " |  \n",
      " |  coalesce(self, numPartitions)\n",
      " |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      " |      \n",
      " |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      " |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      " |      there will not be a shuffle, instead each of the 100 new partitions will\n",
      " |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      " |      it will stay at the current number of partitions.\n",
      " |      \n",
      " |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      " |      this may result in your computation taking place on fewer nodes than\n",
      " |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      " |      you can call repartition(). This will add a shuffle step, but means the\n",
      " |      current upstream partitions will be executed in parallel (per whatever\n",
      " |      the current partitioning is).\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          specify the target number of partitions\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
      " |      1\n",
      " |  \n",
      " |  colRegex(self, colName)\n",
      " |      Selects column based on the column name specified as a regex and returns it\n",
      " |      as :class:`Column`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colName : str\n",
      " |          string, column name specified as a regex.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      " |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      " |      +----+\n",
      " |      |Col2|\n",
      " |      +----+\n",
      " |      |   1|\n",
      " |      |   2|\n",
      " |      |   3|\n",
      " |      +----+\n",
      " |  \n",
      " |  collect(self)\n",
      " |      Returns all the records as a list of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  corr(self, col1, col2, method=None)\n",
      " |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      " |      Currently only supports the Pearson Correlation Coefficient.\n",
      " |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column\n",
      " |      col2 : str\n",
      " |          The name of the second column\n",
      " |      method : str, optional\n",
      " |          The correlation method. Currently only supports \"pearson\"\n",
      " |  \n",
      " |  count(self)\n",
      " |      Returns the number of rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.count()\n",
      " |      2\n",
      " |  \n",
      " |  cov(self, col1, col2)\n",
      " |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      " |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column\n",
      " |      col2 : str\n",
      " |          The name of the second column\n",
      " |  \n",
      " |  createGlobalTempView(self, name)\n",
      " |      Creates a global temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createGlobalTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |  \n",
      " |  createOrReplaceGlobalTempView(self, name)\n",
      " |      Creates or replaces a global temporary view using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |  \n",
      " |  createOrReplaceTempView(self, name)\n",
      " |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createOrReplaceTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |  \n",
      " |  createTempView(self, name)\n",
      " |      Creates a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |  \n",
      " |  crossJoin(self, other)\n",
      " |      Returns the cartesian product with another :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Right side of the cartesian product.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(\"age\", \"name\").collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df2.select(\"name\", \"height\").collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85)]\n",
      " |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n",
      " |      [Row(age=2, name='Alice', height=80), Row(age=2, name='Alice', height=85),\n",
      " |       Row(age=5, name='Bob', height=80), Row(age=5, name='Bob', height=85)]\n",
      " |  \n",
      " |  crosstab(self, col1, col2)\n",
      " |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      " |      table. The number of distinct values for each column should be less than 1e4. At most 1e6\n",
      " |      non-zero pair frequencies will be returned.\n",
      " |      The first column of each row will be the distinct values of `col1` and the column names\n",
      " |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      " |      Pairs that have no occurrences will have zero as their counts.\n",
      " |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column. Distinct items will make the first item of\n",
      " |          each row.\n",
      " |      col2 : str\n",
      " |          The name of the second column. Distinct items will make the column names\n",
      " |          of the :class:`DataFrame`.\n",
      " |  \n",
      " |  cube(self, *cols)\n",
      " |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregations on them.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      | null|   2|    1|\n",
      " |      | null|   5|    1|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |  \n",
      " |  describe(self, *cols)\n",
      " |      Computes basic statistics for numeric and string columns.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      This include count, mean, stddev, min, and max. If no columns are\n",
      " |      given, this function computes statistics for all numerical or string columns.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      Use summary for expanded statistics and control over which statistics to compute.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.describe(['age']).show()\n",
      " |      +-------+------------------+\n",
      " |      |summary|               age|\n",
      " |      +-------+------------------+\n",
      " |      |  count|                 2|\n",
      " |      |   mean|               3.5|\n",
      " |      | stddev|2.1213203435596424|\n",
      " |      |    min|                 2|\n",
      " |      |    max|                 5|\n",
      " |      +-------+------------------+\n",
      " |      >>> df.describe().show()\n",
      " |      +-------+------------------+-----+\n",
      " |      |summary|               age| name|\n",
      " |      +-------+------------------+-----+\n",
      " |      |  count|                 2|    2|\n",
      " |      |   mean|               3.5| null|\n",
      " |      | stddev|2.1213203435596424| null|\n",
      " |      |    min|                 2|Alice|\n",
      " |      |    max|                 5|  Bob|\n",
      " |      +-------+------------------+-----+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.summary\n",
      " |  \n",
      " |  distinct(self)\n",
      " |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.distinct().count()\n",
      " |      2\n",
      " |  \n",
      " |  drop(self, *cols)\n",
      " |      Returns a new :class:`DataFrame` that drops the specified column.\n",
      " |      This is a no-op if schema doesn't contain the given column name(s).\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols: str or :class:`Column`\n",
      " |          a name of the column, or the :class:`Column` to drop\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.drop('age').collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.drop(df.age).collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      " |      [Row(age=5, height=85, name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      " |      [Row(age=5, name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      " |      [Row(name='Bob')]\n",
      " |  \n",
      " |  dropDuplicates(self, subset=None)\n",
      " |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      " |      optionally only considering certain columns.\n",
      " |      \n",
      " |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      " |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      " |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      " |      be and system will accordingly limit the state. In addition, too late data older than\n",
      " |      watermark will be dropped to avoid any possibility of duplicates.\n",
      " |      \n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = sc.parallelize([ \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      " |      >>> df.dropDuplicates().show()\n",
      " |      +-----+---+------+\n",
      " |      | name|age|height|\n",
      " |      +-----+---+------+\n",
      " |      |Alice|  5|    80|\n",
      " |      |Alice| 10|    80|\n",
      " |      +-----+---+------+\n",
      " |      \n",
      " |      >>> df.dropDuplicates(['name', 'height']).show()\n",
      " |      +-----+---+------+\n",
      " |      | name|age|height|\n",
      " |      +-----+---+------+\n",
      " |      |Alice|  5|    80|\n",
      " |      +-----+---+------+\n",
      " |  \n",
      " |  drop_duplicates = dropDuplicates(self, subset=None)\n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  dropna(self, how='any', thresh=None, subset=None)\n",
      " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      how : str, optional\n",
      " |          'any' or 'all'.\n",
      " |          If 'any', drop a row if it contains any nulls.\n",
      " |          If 'all', drop a row only if all its values are null.\n",
      " |      thresh: int, optional\n",
      " |          default None\n",
      " |          If specified, drop rows that have less than `thresh` non-null values.\n",
      " |          This overwrites the `how` parameter.\n",
      " |      subset : str, tuple or list, optional\n",
      " |          optional list of column names to consider.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.drop().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |  \n",
      " |  exceptAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      " |      not in another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT ALL` in SQL.\n",
      " |      As standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame(\n",
      " |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.exceptAll(df2).show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  a|  2|\n",
      " |      |  c|  4|\n",
      " |      +---+---+\n",
      " |  \n",
      " |  explain(self, extended=None, mode=None)\n",
      " |      Prints the (logical and physical) plans to the console for debugging purpose.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      parameters\n",
      " |      ----------\n",
      " |      extended : bool, optional\n",
      " |          default ``False``. If ``False``, prints only the physical plan.\n",
      " |          When this is a string without specifying the ``mode``, it works as the mode is\n",
      " |          specified.\n",
      " |      mode : str, optional\n",
      " |          specifies the expected output format of plans.\n",
      " |      \n",
      " |          * ``simple``: Print only a physical plan.\n",
      " |          * ``extended``: Print both logical and physical plans.\n",
      " |          * ``codegen``: Print a physical plan and generated codes if they are available.\n",
      " |          * ``cost``: Print a logical plan and statistics if they are available.\n",
      " |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n",
      " |      \n",
      " |          .. versionchanged:: 3.0.0\n",
      " |             Added optional argument `mode` to specify the expected output format of plans.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.explain()\n",
      " |      == Physical Plan ==\n",
      " |      *(1) Scan ExistingRDD[age#0,name#1]\n",
      " |      \n",
      " |      >>> df.explain(True)\n",
      " |      == Parsed Logical Plan ==\n",
      " |      ...\n",
      " |      == Analyzed Logical Plan ==\n",
      " |      ...\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...\n",
      " |      == Physical Plan ==\n",
      " |      ...\n",
      " |      \n",
      " |      >>> df.explain(mode=\"formatted\")\n",
      " |      == Physical Plan ==\n",
      " |      * Scan ExistingRDD (1)\n",
      " |      (1) Scan ExistingRDD [codegen id : 1]\n",
      " |      Output [2]: [age#0, name#1]\n",
      " |      ...\n",
      " |      \n",
      " |      >>> df.explain(\"cost\")\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...Statistics...\n",
      " |      ...\n",
      " |  \n",
      " |  fillna(self, value, subset=None)\n",
      " |      Replace null values, alias for ``na.fill()``.\n",
      " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : int, float, string, bool or dict\n",
      " |          Value to replace null values with.\n",
      " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      " |          from column name (string) to replacement value. The replacement value must be\n",
      " |          an int, float, boolean, or string.\n",
      " |      subset : str, tuple or list, optional\n",
      " |          optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.fill(50).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      |  5|    50|  Bob|\n",
      " |      | 50|    50|  Tom|\n",
      " |      | 50|    50| null|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df5.na.fill(False).show()\n",
      " |      +----+-------+-----+\n",
      " |      | age|   name|  spy|\n",
      " |      +----+-------+-----+\n",
      " |      |  10|  Alice|false|\n",
      " |      |   5|    Bob|false|\n",
      " |      |null|Mallory| true|\n",
      " |      +----+-------+-----+\n",
      " |      \n",
      " |      >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      " |      +---+------+-------+\n",
      " |      |age|height|   name|\n",
      " |      +---+------+-------+\n",
      " |      | 10|    80|  Alice|\n",
      " |      |  5|  null|    Bob|\n",
      " |      | 50|  null|    Tom|\n",
      " |      | 50|  null|unknown|\n",
      " |      +---+------+-------+\n",
      " |  \n",
      " |  filter(self, condition)\n",
      " |      Filters rows using the given condition.\n",
      " |      \n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      condition : :class:`Column` or str\n",
      " |          a :class:`Column` of :class:`types.BooleanType`\n",
      " |          or a string of SQL expression.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.age > 3).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(df.age == 2).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      >>> df.filter(\"age > 3\").collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(\"age = 2\").collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  first(self)\n",
      " |      Returns the first row as a :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.first()\n",
      " |      Row(age=2, name='Alice')\n",
      " |  \n",
      " |  foreach(self, f)\n",
      " |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a shorthand for ``df.rdd.foreach()``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def f(person):\n",
      " |      ...     print(person.name)\n",
      " |      >>> df.foreach(f)\n",
      " |  \n",
      " |  foreachPartition(self, f)\n",
      " |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def f(people):\n",
      " |      ...     for person in people:\n",
      " |      ...         print(person.name)\n",
      " |      >>> df.foreachPartition(f)\n",
      " |  \n",
      " |  freqItems(self, cols, support=None)\n",
      " |      Finding frequent items for columns, possibly with false positives. Using the\n",
      " |      frequent element count algorithm described in\n",
      " |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      " |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : list or tuple\n",
      " |          Names of the columns to calculate frequent items for as a list or tuple of\n",
      " |          strings.\n",
      " |      support : float, optional\n",
      " |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      " |          The support must be greater than 1e-4.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |  \n",
      " |  groupBy(self, *cols)\n",
      " |      Groups the :class:`DataFrame` using the specified columns,\n",
      " |      so we can run aggregation on them. See :class:`GroupedData`\n",
      " |      for all the available aggregate functions.\n",
      " |      \n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : list, str or :class:`Column`\n",
      " |          columns to group by.\n",
      " |          Each element should be a column name (string) or an expression (:class:`Column`).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.groupBy().avg().collect()\n",
      " |      [Row(avg(age)=3.5)]\n",
      " |      >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(df.name).avg().collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      " |      [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      " |  \n",
      " |  groupby = groupBy(self, *cols)\n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  head(self, n=None)\n",
      " |      Returns the first ``n`` rows.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, optional\n",
      " |          default 1. Number of rows to return.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      If n is greater than 1, return a list of :class:`Row`.\n",
      " |      If n is 1, return a single Row.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.head()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      >>> df.head(1)\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  hint(self, name, *parameters)\n",
      " |      Specifies some hint on the current :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          A name of the hint.\n",
      " |      parameters : str, list, float or int\n",
      " |          Optional parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\n",
      " |      +----+---+------+\n",
      " |      |name|age|height|\n",
      " |      +----+---+------+\n",
      " |      | Bob|  5|    85|\n",
      " |      +----+---+------+\n",
      " |  \n",
      " |  inputFiles(self)\n",
      " |      Returns a best-effort snapshot of the files that compose this :class:`DataFrame`.\n",
      " |      This method simply asks each constituent BaseRelation for its respective files and\n",
      " |      takes the union of all results. Depending on the source relations, this may not find\n",
      " |      all input files. Duplicates are removed.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
      " |      >>> len(df.inputFiles())\n",
      " |      1\n",
      " |  \n",
      " |  intersect(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows only in\n",
      " |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  intersectAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
      " |      and another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT ALL` in SQL. As standard in SQL, this function\n",
      " |      resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  b|  3|\n",
      " |      +---+---+\n",
      " |  \n",
      " |  isLocal(self)\n",
      " |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      " |      (without any Spark executors).\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  join(self, other, on=None, how=None)\n",
      " |      Joins with another :class:`DataFrame`, using the given join expression.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Right side of the join\n",
      " |      on : str, list or :class:`Column`, optional\n",
      " |          a string for the join column name, a list of column names,\n",
      " |          a join expression (Column), or a list of Columns.\n",
      " |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      " |          the column(s) must exist on both sides, and this performs an equi-join.\n",
      " |      how : str, optional\n",
      " |          default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      " |          ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      " |          ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      " |          ``anti``, ``leftanti`` and ``left_anti``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      The following performs a full outer join between ``df1`` and ``df2``.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import desc\n",
      " |      >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height)                 .sort(desc(\"name\")).collect()\n",
      " |      [Row(name='Bob', height=85), Row(name='Alice', height=None), Row(name=None, height=80)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      " |      \n",
      " |      >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      " |      >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      " |      [Row(name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      " |      [Row(name='Bob', age=5)]\n",
      " |  \n",
      " |  limit(self, num)\n",
      " |      Limits the result count to the number specified.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.limit(1).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.limit(0).collect()\n",
      " |      []\n",
      " |  \n",
      " |  localCheckpoint(self, eager=True)\n",
      " |      Returns a locally checkpointed version of this Dataset. Checkpointing can be used to\n",
      " |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      " |      iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
      " |      stored in the executors using the caching subsystem and therefore they are not reliable.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eager : bool, optional\n",
      " |          Whether to checkpoint this :class:`DataFrame` immediately\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental.\n",
      " |  \n",
      " |  orderBy = sort(self, *cols, **kwargs)\n",
      " |  \n",
      " |  persist(self, storageLevel=StorageLevel(True, True, False, True, 1))\n",
      " |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      " |      operations after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
      " |  \n",
      " |  printSchema(self)\n",
      " |      Prints out the schema in the tree format.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.printSchema()\n",
      " |      root\n",
      " |       |-- age: integer (nullable = true)\n",
      " |       |-- name: string (nullable = true)\n",
      " |      <BLANKLINE>\n",
      " |  \n",
      " |  randomSplit(self, weights, seed=None)\n",
      " |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      weights : list\n",
      " |          list of doubles as weights with which to split the :class:`DataFrame`.\n",
      " |          Weights will be normalized if they don't sum up to 1.0.\n",
      " |      seed : int, optional\n",
      " |          The seed for sampling.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> splits = df4.randomSplit([1.0, 2.0], 24)\n",
      " |      >>> splits[0].count()\n",
      " |      2\n",
      " |      \n",
      " |      >>> splits[1].count()\n",
      " |      2\n",
      " |  \n",
      " |  registerTempTable(self, name)\n",
      " |      Registers this DataFrame as a temporary table using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. deprecated:: 2.0.0\n",
      " |          Use :meth:`DataFrame.createOrReplaceTempView` instead.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.registerTempTable(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |  \n",
      " |  repartition(self, numPartitions, *cols)\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is hash partitioned.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      cols : str or :class:`Column`\n",
      " |          partitioning columns.\n",
      " |      \n",
      " |          .. versionchanged:: 1.6\n",
      " |             Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      " |             optional if partitioning columns are specified.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.repartition(10).rdd.getNumPartitions()\n",
      " |      10\n",
      " |      >>> data = df.union(df).repartition(\"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      >>> data = data.repartition(7, \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> data.rdd.getNumPartitions()\n",
      " |      7\n",
      " |      >>> data = data.repartition(\"name\", \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |  \n",
      " |  repartitionByRange(self, numPartitions, *cols)\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is range partitioned.\n",
      " |      \n",
      " |      At least one partition-by expression must be specified.\n",
      " |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      cols : str or :class:`Column`\n",
      " |          partitioning columns.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Due to performance reasons this method uses sampling to estimate the ranges.\n",
      " |      Hence, the output may not be consistent, since sampling can return different values.\n",
      " |      The sample size can be controlled by the config\n",
      " |      `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      " |      2\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.repartitionByRange(1, \"age\").rdd.getNumPartitions()\n",
      " |      1\n",
      " |      >>> data = df.repartitionByRange(\"age\")\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |  \n",
      " |  replace(self, to_replace, value=<no value>, subset=None)\n",
      " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      " |      aliases of each other.\n",
      " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      " |      or strings. Value can have None. When replacing, the new value will be cast\n",
      " |      to the type of the existing column.\n",
      " |      For numeric replacements all values to be replaced should have unique\n",
      " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      " |      and arbitrary replacement will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      to_replace : bool, int, float, string, list or dict\n",
      " |          Value to be replaced.\n",
      " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      " |          must be a mapping between a value and a replacement.\n",
      " |      value : bool, int, float, string or None, optional\n",
      " |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      " |          list, `value` should be of the same length and type as `to_replace`.\n",
      " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      " |          used as a replacement for each item in `to_replace`.\n",
      " |      subset : list, optional\n",
      " |          optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.replace(10, 20).show()\n",
      " |      +----+------+-----+\n",
      " |      | age|height| name|\n",
      " |      +----+------+-----+\n",
      " |      |  20|    80|Alice|\n",
      " |      |   5|  null|  Bob|\n",
      " |      |null|  null|  Tom|\n",
      " |      |null|  null| null|\n",
      " |      +----+------+-----+\n",
      " |      \n",
      " |      >>> df4.na.replace('Alice', None).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace({'Alice': None}).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|   A|\n",
      " |      |   5|  null|   B|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |  \n",
      " |  rollup(self, *cols)\n",
      " |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregation on them.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |  \n",
      " |  sameSemantics(self, other)\n",
      " |      Returns `True` when the logical query plans inside both :class:`DataFrame`\\s are equal and\n",
      " |      therefore return same results.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The equality comparison here is simplified by tolerating the cosmetic differences\n",
      " |      such as attribute names.\n",
      " |      \n",
      " |      This API can compare both :class:`DataFrame`\\s very fast but can still return\n",
      " |      `False` on the :class:`DataFrame` that return the same results, for instance, from\n",
      " |      different plans. Such false negative semantic can be useful when caching as an example.\n",
      " |      \n",
      " |      This API is a developer API.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.range(10)\n",
      " |      >>> df2 = spark.range(10)\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\n",
      " |      True\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\n",
      " |      False\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n",
      " |      True\n",
      " |  \n",
      " |  sample(self, withReplacement=None, fraction=None, seed=None)\n",
      " |      Returns a sampled subset of this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      withReplacement : bool, optional\n",
      " |          Sample with replacement or not (default ``False``).\n",
      " |      fraction : float, optional\n",
      " |          Fraction of rows to generate, range [0.0, 1.0].\n",
      " |      seed : int, optional\n",
      " |          Seed for sampling (default a random seed).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |      count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.sample(0.5, 3).count()\n",
      " |      7\n",
      " |      >>> df.sample(fraction=0.5, seed=3).count()\n",
      " |      7\n",
      " |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n",
      " |      1\n",
      " |      >>> df.sample(1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(fraction=1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(False, fraction=1.0).count()\n",
      " |      10\n",
      " |  \n",
      " |  sampleBy(self, col, fractions, seed=None)\n",
      " |      Returns a stratified sample without replacement based on the\n",
      " |      fraction given on each stratum.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col : :class:`Column` or str\n",
      " |          column that defines strata\n",
      " |      \n",
      " |          .. versionchanged:: 3.0\n",
      " |             Added sampling by a column of :class:`Column`\n",
      " |      fractions : dict\n",
      " |          sampling fraction for each stratum. If a stratum is not\n",
      " |          specified, we treat its fraction as zero.\n",
      " |      seed : int, optional\n",
      " |          random seed\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a new :class:`DataFrame` that represents the stratified sample\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      " |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      " |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      " |      +---+-----+\n",
      " |      |key|count|\n",
      " |      +---+-----+\n",
      " |      |  0|    3|\n",
      " |      |  1|    6|\n",
      " |      +---+-----+\n",
      " |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      " |      33\n",
      " |  \n",
      " |  select(self, *cols)\n",
      " |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, :class:`Column`, or list\n",
      " |          column names (string) or expressions (:class:`Column`).\n",
      " |          If one of the column names is '*', that column is expanded to include all columns\n",
      " |          in the current :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select('*').collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.select('name', 'age').collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df.select(df.name, (df.age + 10).alias('age')).collect()\n",
      " |      [Row(name='Alice', age=12), Row(name='Bob', age=15)]\n",
      " |  \n",
      " |  selectExpr(self, *expr)\n",
      " |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a variant of :func:`select` that accepts SQL expressions.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n",
      " |      [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n",
      " |  \n",
      " |  semanticHash(self)\n",
      " |      Returns a hash code of the logical query plan against this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Unlike the standard hash code, the hash is calculated against the query plan\n",
      " |      simplified by tolerating the cosmetic differences such as attribute names.\n",
      " |      \n",
      " |      This API is a developer API.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.range(10).selectExpr(\"id as col0\").semanticHash()  # doctest: +SKIP\n",
      " |      1855039936\n",
      " |      >>> spark.range(10).selectExpr(\"id as col1\").semanticHash()  # doctest: +SKIP\n",
      " |      1855039936\n",
      " |  \n",
      " |  show(self, n=20, truncate=True, vertical=False)\n",
      " |      Prints the first ``n`` rows to the console.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, optional\n",
      " |          Number of rows to show.\n",
      " |      truncate : bool, optional\n",
      " |          If set to ``True``, truncate strings longer than 20 chars by default.\n",
      " |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
      " |          and align cells right.\n",
      " |      vertical : bool, optional\n",
      " |          If set to ``True``, print output rows vertically (one line\n",
      " |          per column value).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df\n",
      " |      DataFrame[age: int, name: string]\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.show(truncate=3)\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  2| Ali|\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |      >>> df.show(vertical=True)\n",
      " |      -RECORD 0-----\n",
      " |       age  | 2\n",
      " |       name | Alice\n",
      " |      -RECORD 1-----\n",
      " |       age  | 5\n",
      " |       name | Bob\n",
      " |  \n",
      " |  sort(self, *cols, **kwargs)\n",
      " |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, list, or :class:`Column`, optional\n",
      " |           list of :class:`Column` or column names to sort by.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      ascending : bool or list, optional\n",
      " |          boolean or list of boolean (default ``True``).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.sort(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.sort(\"age\", ascending=False).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df.sort(asc(\"age\")).collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  sortWithinPartitions(self, *cols, **kwargs)\n",
      " |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, list or :class:`Column`, optional\n",
      " |          list of :class:`Column` or column names to sort by.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      ascending : bool or list, optional\n",
      " |          boolean or list of boolean (default ``True``).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |  \n",
      " |  subtract(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
      " |      but not in another :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  summary(self, *statistics)\n",
      " |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
      " |      - count\n",
      " |      - mean\n",
      " |      - stddev\n",
      " |      - min\n",
      " |      - max\n",
      " |      - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
      " |      \n",
      " |      If no statistics are given, this function computes count, mean, stddev, min,\n",
      " |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.summary().show()\n",
      " |      +-------+------------------+-----+\n",
      " |      |summary|               age| name|\n",
      " |      +-------+------------------+-----+\n",
      " |      |  count|                 2|    2|\n",
      " |      |   mean|               3.5| null|\n",
      " |      | stddev|2.1213203435596424| null|\n",
      " |      |    min|                 2|Alice|\n",
      " |      |    25%|                 2| null|\n",
      " |      |    50%|                 2| null|\n",
      " |      |    75%|                 5| null|\n",
      " |      |    max|                 5|  Bob|\n",
      " |      +-------+------------------+-----+\n",
      " |      \n",
      " |      >>> df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      " |      +-------+---+-----+\n",
      " |      |summary|age| name|\n",
      " |      +-------+---+-----+\n",
      " |      |  count|  2|    2|\n",
      " |      |    min|  2|Alice|\n",
      " |      |    25%|  2| null|\n",
      " |      |    75%|  5| null|\n",
      " |      |    max|  5|  Bob|\n",
      " |      +-------+---+-----+\n",
      " |      \n",
      " |      To do a summary for specific columns first select them:\n",
      " |      \n",
      " |      >>> df.select(\"age\", \"name\").summary(\"count\").show()\n",
      " |      +-------+---+----+\n",
      " |      |summary|age|name|\n",
      " |      +-------+---+----+\n",
      " |      |  count|  2|   2|\n",
      " |      +-------+---+----+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.display\n",
      " |  \n",
      " |  tail(self, num)\n",
      " |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      Running tail requires moving data into the application's driver process, and doing so with\n",
      " |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.tail(1)\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  take(self, num)\n",
      " |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.take(2)\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  toDF(self, *cols)\n",
      " |      Returns a new :class:`DataFrame` that with new specified column names\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str\n",
      " |          new column names\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.toDF('f1', 'f2').collect()\n",
      " |      [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\n",
      " |  \n",
      " |  toJSON(self, use_unicode=True)\n",
      " |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      " |      \n",
      " |      Each row is turned into a JSON document as one element in the returned RDD.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.toJSON().first()\n",
      " |      '{\"age\":2,\"name\":\"Alice\"}'\n",
      " |  \n",
      " |  toLocalIterator(self, prefetchPartitions=False)\n",
      " |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      " |      The iterator will consume as much memory as the largest partition in this\n",
      " |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
      " |      partitions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prefetchPartitions : bool, optional\n",
      " |          If Spark should pre-fetch the next partition  before it is needed.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> list(df.toLocalIterator())\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  transform(self, func)\n",
      " |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          a function that takes and returns a :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
      " |      >>> def cast_all_to_int(input_df):\n",
      " |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
      " |      >>> def sort_columns_asc(input_df):\n",
      " |      ...     return input_df.select(*sorted(input_df.columns))\n",
      " |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
      " |      +-----+---+\n",
      " |      |float|int|\n",
      " |      +-----+---+\n",
      " |      |    1|  1|\n",
      " |      |    2|  2|\n",
      " |      +-----+---+\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  unionAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  unionByName(self, other, allowMissingColumns=False)\n",
      " |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n",
      " |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      The difference between this function and :func:`union` is that this function\n",
      " |      resolves columns by name (not by position):\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      " |      >>> df1.unionByName(df2).show()\n",
      " |      +----+----+----+\n",
      " |      |col0|col1|col2|\n",
      " |      +----+----+----+\n",
      " |      |   1|   2|   3|\n",
      " |      |   6|   4|   5|\n",
      " |      +----+----+----+\n",
      " |      \n",
      " |      When the parameter `allowMissingColumns` is ``True``, the set of column names\n",
      " |      in this and other :class:`DataFrame` can differ; missing columns will be filled with null.\n",
      " |      Further, the missing columns of this :class:`DataFrame` will be added at the end\n",
      " |      in the schema of the union result:\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
      " |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
      " |      +----+----+----+----+\n",
      " |      |col0|col1|col2|col3|\n",
      " |      +----+----+----+----+\n",
      " |      |   1|   2|   3|null|\n",
      " |      |null|   4|   5|   6|\n",
      " |      +----+----+----+----+\n",
      " |      \n",
      " |      .. versionchanged:: 3.1.0\n",
      " |         Added optional argument `allowMissingColumns` to specify whether to allow\n",
      " |         missing columns.\n",
      " |  \n",
      " |  unpersist(self, blocking=False)\n",
      " |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
      " |  \n",
      " |  where = filter(self, condition)\n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withColumn(self, colName, col)\n",
      " |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      " |      existing column that has the same name.\n",
      " |      \n",
      " |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      " |      a column from some other :class:`DataFrame` will raise an error.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colName : str\n",
      " |          string, name of the new column.\n",
      " |      col : :class:`Column`\n",
      " |          a :class:`Column` expression for the new column.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method introduces a projection internally. Therefore, calling it multiple\n",
      " |      times, for instance, via loops in order to add multiple columns can generate big\n",
      " |      plans which can cause performance issues and even `StackOverflowException`.\n",
      " |      To avoid this, use :func:`select` with the multiple columns at once.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.withColumn('age2', df.age + 2).collect()\n",
      " |      [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
      " |  \n",
      " |  withColumnRenamed(self, existing, new)\n",
      " |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
      " |      This is a no-op if schema doesn't contain the given column name.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      existing : str\n",
      " |          string, name of the existing column to rename.\n",
      " |      new : str\n",
      " |          string, new name of the column.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.withColumnRenamed('age', 'age2').collect()\n",
      " |      [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n",
      " |  \n",
      " |  withWatermark(self, eventTime, delayThreshold)\n",
      " |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      " |      in time before which we assume no more late data is going to arrive.\n",
      " |      \n",
      " |      Spark will use this watermark for several purposes:\n",
      " |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      " |          when using output modes that do not allow updates.\n",
      " |      \n",
      " |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      " |      \n",
      " |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      " |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      " |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      " |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      " |      process records that arrive more than `delayThreshold` late.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eventTime : str\n",
      " |          the name of the column that contains the event time of the row.\n",
      " |      delayThreshold : str\n",
      " |          the minimum delay to wait to data to arrive late, relative to the\n",
      " |          latest record that has been processed in the form of an interval\n",
      " |          (e.g. \"1 minute\" or \"5 hours\").\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import timestamp_seconds\n",
      " |      >>> sdf.select(\n",
      " |      ...    'name',\n",
      " |      ...    timestamp_seconds(sdf.time).alias('time')).withWatermark('time', '10 minutes')\n",
      " |      DataFrame[name: string, time: timestamp]\n",
      " |  \n",
      " |  writeTo(self, table)\n",
      " |      Create a write configuration builder for v2 sources.\n",
      " |      \n",
      " |      This builder is used to configure and execute write operations.\n",
      " |      \n",
      " |      For example, to append or create or replace existing tables.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n",
      " |      >>> df.writeTo(                              # doctest: +SKIP\n",
      " |      ...     \"catalog.db.table\"\n",
      " |      ... ).partitionedBy(\"col\").createOrReplace()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  columns\n",
      " |      Returns all column names as a list.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.columns\n",
      " |      ['age', 'name']\n",
      " |  \n",
      " |  dtypes\n",
      " |      Returns all column names and their data types as a list.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'int'), ('name', 'string')]\n",
      " |  \n",
      " |  isStreaming\n",
      " |      Returns ``True`` if this :class:`Dataset` contains one or more sources that continuously\n",
      " |      return data as it arrives. A :class:`Dataset` that reads data from a streaming source\n",
      " |      must be executed as a :class:`StreamingQuery` using the :func:`start` method in\n",
      " |      :class:`DataStreamWriter`.  Methods that return a single answer, (e.g., :func:`count` or\n",
      " |      :func:`collect`) will throw an :class:`AnalysisException` when there is a streaming\n",
      " |      source present.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |  \n",
      " |  na\n",
      " |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  rdd\n",
      " |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  schema\n",
      " |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.schema\n",
      " |      StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n",
      " |  \n",
      " |  stat\n",
      " |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  storageLevel\n",
      " |      Get the :class:`DataFrame`'s current storage level.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.storageLevel\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> df.cache().storageLevel\n",
      " |      StorageLevel(True, True, False, True, 1)\n",
      " |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
      " |      StorageLevel(True, False, False, False, 2)\n",
      " |  \n",
      " |  write\n",
      " |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrameWriter`\n",
      " |  \n",
      " |  writeStream\n",
      " |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataStreamWriter`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |  \n",
      " |  mapInPandas(self, func, schema)\n",
      " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      " |      function that takes and outputs a pandas DataFrame, and returns the result as a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The function should take an iterator of `pandas.DataFrame`\\s and return\n",
      " |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n",
      " |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n",
      " |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n",
      " |      Each `pandas.DataFrame` size can be controlled by\n",
      " |      `spark.sql.execution.arrow.maxRecordsPerBatch`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n",
      " |          outputs an iterator of `pandas.DataFrame`\\s.\n",
      " |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      " |          the return type of the `func` in PySpark. The value can be either a\n",
      " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import pandas_udf\n",
      " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      " |      >>> def filter_func(iterator):\n",
      " |      ...     for pdf in iterator:\n",
      " |      ...         yield pdf[pdf.id == 1]\n",
      " |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
      " |      +---+---+\n",
      " |      | id|age|\n",
      " |      +---+---+\n",
      " |      |  1| 21|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.pandas_udf\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n",
      " |  \n",
      " |  toPandas(self)\n",
      " |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      " |      \n",
      " |      This is only available if Pandas is installed and available.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting Pandas's :class:`DataFrame` is\n",
      " |      expected to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.toPandas()  # doctest: +SKIP\n",
      " |         age   name\n",
      " |      0    2  Alice\n",
      " |      1    5    Bob\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(airlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a8bd066",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e3d45c4fc637>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mairlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \"\"\"\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "airlines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29331636",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|IsArrDelayed|IsDepDelayed|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+\n",
      "|2001|    8|         3|        5|   1048|      1047|   1210|      1222|           AA|     1056| N274A1|               82|            95|     66|     -12|       1|   MCI| ORD|     403|     6|     10|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|         YES|\n",
      "|2001|    8|         4|        6|   1043|      1047|   1159|      1222|           AA|     1056| N513A1|               76|            95|     61|     -23|      -4|   MCI| ORD|     403|     4|     11|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|\n",
      "|2001|    8|         5|        7|   1043|      1047|   1203|      1222|           AA|     1056| N532A1|               80|            95|     65|     -19|      -4|   MCI| ORD|     403|     6|      9|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|\n",
      "|2001|    8|         6|        1|   1045|      1047|   1159|      1222|           AA|     1056| N521A1|               74|            95|     62|     -23|      -2|   MCI| ORD|     403|     4|      8|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|\n",
      "|2001|    8|         7|        2|   1047|      1047|   1208|      1222|           AA|     1056| N417A1|               81|            95|     65|     -14|       0|   MCI| ORD|     403|     6|     10|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|\n",
      "|2001|    8|         8|        3|   1047|      1047|   1203|      1222|           AA|     1056| N440A1|               76|            95|     60|     -19|       0|   MCI| ORD|     403|     7|      9|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|\n",
      "|2001|    8|         9|        4|   1054|      1047|   1224|      1222|           AA|     1056| N483A1|               90|            95|     66|       2|       7|   MCI| ORD|     403|     7|     17|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|         YES|         YES|\n",
      "|2001|    8|        10|        5|   1052|      1047|   1205|      1222|           AA|     1056| N431A1|               73|            95|     53|     -17|       5|   MCI| ORD|     403|     5|     15|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|         YES|\n",
      "|2001|    8|        11|        6|   1045|      1047|   1205|      1222|           AA|     1056| N424A1|               80|            95|     54|     -17|      -2|   MCI| ORD|     403|    16|     10|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|\n",
      "|2001|    8|        12|        7|   1122|      1047|   1235|      1222|           AA|     1056| N404A1|               73|            95|     57|      13|      35|   MCI| ORD|     403|     5|     11|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|         YES|         YES|\n",
      "|2001|    8|        13|        1|   1055|      1047|   1210|      1222|           AA|     1056| N582A1|               75|            95|     56|     -12|       8|   MCI| ORD|     403|     5|     14|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|         YES|\n",
      "|2001|    8|        14|        2|   1045|      1047|   1154|      1222|           AA|     1056| N285A1|               69|            95|     57|     -28|      -2|   MCI| ORD|     403|     5|      7|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|\n",
      "|2001|    8|        15|        3|   1046|      1047|   1203|      1222|           AA|     1056| N232A1|               77|            95|     63|     -19|      -1|   MCI| ORD|     403|     6|      8|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|\n",
      "|2001|    8|        16|        4|   1157|      1047|   1314|      1222|           AA|     1056| N430A1|               77|            95|     62|      52|      70|   MCI| ORD|     403|     5|     10|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|         YES|         YES|\n",
      "|2001|    8|        17|        5|   1048|      1047|   1214|      1222|           AA|     1056| N224A1|               86|            95|     55|      -8|       1|   MCI| ORD|     403|    11|     20|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|         YES|\n",
      "|2001|    8|        18|        6|   1046|      1047|   1202|      1222|           AA|     1056| N471A1|               76|            95|     57|     -20|      -1|   MCI| ORD|     403|     6|     13|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|\n",
      "|2001|    8|        19|        7|   1054|      1047|   1233|      1222|           AA|     1056| N420A1|               99|            95|     65|      11|       7|   MCI| ORD|     403|     9|     25|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|         YES|         YES|\n",
      "|2001|    8|        20|        1|   1045|      1047|   1205|      1222|           AA|     1056| N580A1|               80|            95|     59|     -17|      -2|   MCI| ORD|     403|     8|     13|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|\n",
      "|2001|    8|        21|        2|   1042|      1047|   1159|      1222|           AA|     1056| N421A1|               77|            95|     58|     -23|      -5|   MCI| ORD|     403|     9|     10|        0|              NA|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|\n",
      "|2001|    8|        22|        3|     NA|      1047|     NA|      1222|           AA|     1056| �NKNO�|               NA|            95|     NA|      NA|      NA|   MCI| ORD|     403|     0|      0|        1|              NA|       0|          NA|          NA|      NA|           NA|               NA|         YES|         YES|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb9783ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Year,IntegerType,true),StructField(Month,IntegerType,true),StructField(DayofMonth,IntegerType,true),StructField(DayOfWeek,IntegerType,true),StructField(DepTime,StringType,true),StructField(CRSDepTime,IntegerType,true),StructField(ArrTime,StringType,true),StructField(CRSArrTime,IntegerType,true),StructField(UniqueCarrier,StringType,true),StructField(FlightNum,IntegerType,true),StructField(TailNum,StringType,true),StructField(ActualElapsedTime,StringType,true),StructField(CRSElapsedTime,IntegerType,true),StructField(AirTime,StringType,true),StructField(ArrDelay,StringType,true),StructField(DepDelay,StringType,true),StructField(Origin,StringType,true),StructField(Dest,StringType,true),StructField(Distance,StringType,true),StructField(TaxiIn,StringType,true),StructField(TaxiOut,StringType,true),StructField(Cancelled,IntegerType,true),StructField(CancellationCode,StringType,true),StructField(Diverted,IntegerType,true),StructField(CarrierDelay,StringType,true),StructField(WeatherDelay,StringType,true),StructField(NASDelay,StringType,true),StructField(SecurityDelay,StringType,true),StructField(LateAircraftDelay,StringType,true),StructField(IsArrDelayed,StringType,true),StructField(IsDepDelayed,StringType,true)))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb7ccb",
   "metadata": {},
   "source": [
    "### 162 Previewing Airlings Data using Spark Data Frame APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "918a1a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e344327",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\",\"0\"). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678591d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_schema = spark.read. \\\n",
    "    csv(\"/public/airlines_all/airlines/part-00000\",\n",
    "       header=True,\n",
    "       inferSchema=True). \\\n",
    "    schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8007544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = spark.read. \\\n",
    "    csv(\"/public/airlines_all/airlines/part-0000*\",\n",
    "           header=True,\n",
    "           schema=airlines_schema\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "160671fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|IsArrDelayed|IsDepDelayed|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+\n",
      "|1987|12   |20        |7        |1330   |1305      |1502   |1445      |AS           |84       |NA     |92               |100           |NA     |17      |25      |PDX   |SFO |550     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|12   |21        |1        |1326   |1305      |1514   |1445      |AS           |84       |NA     |108              |100           |NA     |29      |21      |PDX   |SFO |550     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|12   |22        |2        |1314   |1305      |1447   |1445      |AS           |84       |NA     |93               |100           |NA     |2       |9       |PDX   |SFO |550     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|12   |23        |3        |1336   |1305      |1455   |1445      |AS           |84       |NA     |79               |100           |NA     |10      |31      |PDX   |SFO |550     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|12   |24        |4        |1332   |1305      |1454   |1445      |AS           |84       |NA     |82               |100           |NA     |9       |27      |PDX   |SFO |550     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|12   |26        |6        |1305   |1305      |1443   |1445      |AS           |84       |NA     |98               |100           |NA     |-2      |0       |PDX   |SFO |550     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |NO          |NO          |\n",
      "|1987|12   |27        |7        |1314   |1305      |1506   |1445      |AS           |84       |NA     |112              |100           |NA     |21      |9       |PDX   |SFO |550     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|12   |29        |2        |1410   |1305      |1543   |1445      |AS           |84       |NA     |93               |100           |NA     |58      |65      |PDX   |SFO |550     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |YES         |YES         |\n",
      "|1987|12   |30        |3        |1315   |1305      |1443   |1445      |AS           |84       |NA     |88               |100           |NA     |-2      |10      |PDX   |SFO |550     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |NO          |YES         |\n",
      "|1987|12   |31        |4        |1305   |1305      |1440   |1445      |AS           |84       |NA     |95               |100           |NA     |-5      |0       |PDX   |SFO |550     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |NO          |NO          |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fb63084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      " |-- IsArrDelayed: string (nullable = true)\n",
      " |-- IsDepDelayed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee37d3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6489231"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cfc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d56d1a4",
   "metadata": {},
   "source": [
    "### 163  Overview of DataFrame APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef0a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be06ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \"united states\"),\n",
    "             (2, \"Henry\", \"Ford\", 1250.0, \"India\"),\n",
    "             (3, \"Nick\", \"Junior\", 750.0, \"united KINGDOM\"),\n",
    "             (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b7ad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c0c67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'Scott', 'Tiger', 1000.0, 'united states')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aa376eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(employees[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1eb39d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msamplingRatio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverifySchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
       "\n",
       "When ``schema`` is a list of column names, the type of each column\n",
       "will be inferred from ``data``.\n",
       "\n",
       "When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
       "from ``data``, which should be an RDD of either :class:`Row`,\n",
       ":class:`namedtuple`, or :class:`dict`.\n",
       "\n",
       "When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
       "the real data, or an exception will be thrown at runtime. If the given schema is not\n",
       ":class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
       ":class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
       "Each record will also be wrapped into a tuple, which can be converted to row later.\n",
       "\n",
       "If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
       "rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
       "\n",
       ".. versionadded:: 2.0.0\n",
       "\n",
       ".. versionchanged:: 2.1.0\n",
       "   Added verifySchema.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : :class:`RDD` or iterable\n",
       "    an RDD of any kind of SQL data representation (:class:`Row`,\n",
       "    :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
       "    :class:`pandas.DataFrame`.\n",
       "schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
       "    a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
       "    column names, default is None.  The data type string format equals to\n",
       "    :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
       "    omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
       "    ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
       "    We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
       "samplingRatio : float, optional\n",
       "    the sample ratio of rows used for inferring\n",
       "verifySchema : bool, optional\n",
       "    verify data types of every row against schema. Enabled by default.\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> l = [('Alice', 1)]\n",
       ">>> spark.createDataFrame(l).collect()\n",
       "[Row(_1='Alice', _2=1)]\n",
       ">>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
       "[Row(name='Alice', age=1)]\n",
       "\n",
       ">>> d = [{'name': 'Alice', 'age': 1}]\n",
       ">>> spark.createDataFrame(d).collect()\n",
       "[Row(age=1, name='Alice')]\n",
       "\n",
       ">>> rdd = sc.parallelize(l)\n",
       ">>> spark.createDataFrame(rdd).collect()\n",
       "[Row(_1='Alice', _2=1)]\n",
       ">>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
       ">>> df.collect()\n",
       "[Row(name='Alice', age=1)]\n",
       "\n",
       ">>> from pyspark.sql import Row\n",
       ">>> Person = Row('name', 'age')\n",
       ">>> person = rdd.map(lambda r: Person(*r))\n",
       ">>> df2 = spark.createDataFrame(person)\n",
       ">>> df2.collect()\n",
       "[Row(name='Alice', age=1)]\n",
       "\n",
       ">>> from pyspark.sql.types import *\n",
       ">>> schema = StructType([\n",
       "...    StructField(\"name\", StringType(), True),\n",
       "...    StructField(\"age\", IntegerType(), True)])\n",
       ">>> df3 = spark.createDataFrame(rdd, schema)\n",
       ">>> df3.collect()\n",
       "[Row(name='Alice', age=1)]\n",
       "\n",
       ">>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
       "[Row(name='Alice', age=1)]\n",
       ">>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
       "[Row(0=1, 1=2)]\n",
       "\n",
       ">>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
       "[Row(a='Alice', b=1)]\n",
       ">>> rdd = rdd.map(lambda row: row[1])\n",
       ">>> spark.createDataFrame(rdd, \"int\").collect()\n",
       "[Row(value=1)]\n",
       ">>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
       "Traceback (most recent call last):\n",
       "    ...\n",
       "Py4JJavaError: ...\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.createDataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6417ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.createDataFrame(\n",
    "    employees,\n",
    "    schema = \"\"\"employee_id INT, first_name STRING,\n",
    "                last_name STRING, salary FLOAT, nationality STRING\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1cb5d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02543cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01d621c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|\n",
      "|          2|     Henry|     Ford|1250.0|         India|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "187696ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th></tr>\n",
       "<tr><td>1</td><td>Scott</td><td>Tiger</td><td>1000.0</td><td>united states</td></tr>\n",
       "<tr><td>2</td><td>Henry</td><td>Ford</td><td>1250.0</td><td>India</td></tr>\n",
       "<tr><td>3</td><td>Nick</td><td>Junior</td><td>750.0</td><td>united KINGDOM</td></tr>\n",
       "<tr><td>4</td><td>Bill</td><td>Gomes</td><td>1500.0</td><td>AUSTRALIA</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+--------------+\n",
       "|employee_id|first_name|last_name|salary|   nationality|\n",
       "+-----------+----------+---------+------+--------------+\n",
       "|          1|     Scott|    Tiger|1000.0| united states|\n",
       "|          2|     Henry|     Ford|1250.0|         India|\n",
       "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
       "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
       "+-----------+----------+---------+------+--------------+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92c2e857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th></tr>\n",
       "<tr><td>1</td><td>Scott</td><td>Tiger</td><td>1000.0</td><td>united states</td></tr>\n",
       "<tr><td>2</td><td>Henry</td><td>Ford</td><td>1250.0</td><td>India</td></tr>\n",
       "<tr><td>3</td><td>Nick</td><td>Junior</td><td>750.0</td><td>united KINGDOM</td></tr>\n",
       "<tr><td>4</td><td>Bill</td><td>Gomes</td><td>1500.0</td><td>AUSTRALIA</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+--------------+\n",
       "|employee_id|first_name|last_name|salary|   nationality|\n",
       "+-----------+----------+---------+------+--------------+\n",
       "|          1|     Scott|    Tiger|1000.0| united states|\n",
       "|          2|     Henry|     Ford|1250.0|         India|\n",
       "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
       "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
       "+-----------+----------+---------+------+--------------+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85dae03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|\n",
      "|          2|     Henry|     Ford|1250.0|         India|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d5ed930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(\"first_name\",\"last_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edd1a63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+\n",
      "|employee_id|first_name|last_name|salary|\n",
      "+-----------+----------+---------+------+\n",
      "|          1|     Scott|    Tiger|1000.0|\n",
      "|          2|     Henry|     Ford|1250.0|\n",
      "|          3|      Nick|   Junior| 750.0|\n",
      "|          4|      Bill|    Gomes|1500.0|\n",
      "+-----------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.drop(\"nationality\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6e7dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f22ec5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th><th>full_name</th></tr>\n",
       "<tr><td>1</td><td>Scott</td><td>Tiger</td><td>1000.0</td><td>united states</td><td>Scott Tiger</td></tr>\n",
       "<tr><td>2</td><td>Henry</td><td>Ford</td><td>1250.0</td><td>India</td><td>Henry Ford</td></tr>\n",
       "<tr><td>3</td><td>Nick</td><td>Junior</td><td>750.0</td><td>united KINGDOM</td><td>Nick Junior</td></tr>\n",
       "<tr><td>4</td><td>Bill</td><td>Gomes</td><td>1500.0</td><td>AUSTRALIA</td><td>Bill Gomes</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+--------------+-----------+\n",
       "|employee_id|first_name|last_name|salary|   nationality|  full_name|\n",
       "+-----------+----------+---------+------+--------------+-----------+\n",
       "|          1|     Scott|    Tiger|1000.0| united states|Scott Tiger|\n",
       "|          2|     Henry|     Ford|1250.0|         India| Henry Ford|\n",
       "|          3|      Nick|   Junior| 750.0|united KINGDOM|Nick Junior|\n",
       "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA| Bill Gomes|\n",
       "+-----------+----------+---------+------+--------------+-----------+"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.withColumn('full_name',concat('first_name',lit(' '), 'last_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dfd4b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|  full_name|\n",
      "+-----------+----------+---------+------+--------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|Scott Tiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India| Henry Ford|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|Nick Junior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA| Bill Gomes|\n",
      "+-----------+----------+---------+------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.withColumn('full_name',concat('first_name',lit(' '), 'last_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94814641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|  full_name|\n",
      "+-----------+----------+---------+------+--------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|Scott Tiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India| Henry Ford|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|Nick Junior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA| Bill Gomes|\n",
      "+-----------+----------+---------+------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.selectExpr('*','concat(first_name, \" \", last_name) AS full_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c260ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th><th>full_name</th></tr>\n",
       "<tr><td>1</td><td>Scott</td><td>Tiger</td><td>1000.0</td><td>united states</td><td>Scott Tiger</td></tr>\n",
       "<tr><td>2</td><td>Henry</td><td>Ford</td><td>1250.0</td><td>India</td><td>Henry Ford</td></tr>\n",
       "<tr><td>3</td><td>Nick</td><td>Junior</td><td>750.0</td><td>united KINGDOM</td><td>Nick Junior</td></tr>\n",
       "<tr><td>4</td><td>Bill</td><td>Gomes</td><td>1500.0</td><td>AUSTRALIA</td><td>Bill Gomes</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+--------------+-----------+\n",
       "|employee_id|first_name|last_name|salary|   nationality|  full_name|\n",
       "+-----------+----------+---------+------+--------------+-----------+\n",
       "|          1|     Scott|    Tiger|1000.0| united states|Scott Tiger|\n",
       "|          2|     Henry|     Ford|1250.0|         India| Henry Ford|\n",
       "|          3|      Nick|   Junior| 750.0|united KINGDOM|Nick Junior|\n",
       "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA| Bill Gomes|\n",
       "+-----------+----------+---------+------+--------------+-----------+"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.selectExpr('*','concat(first_name, \" \", last_name) AS full_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b601b42",
   "metadata": {},
   "source": [
    "### 164 Functions on DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b82ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac1e484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \"united states\"),\n",
    "             (2, \"Henry\", \"Ford\", 1250.0, \"India\"),\n",
    "             (3, \"Nick\", \"Junior\", 750.0, \"united KINGDOM\"),\n",
    "             (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0f63338",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema = \"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, \n",
    "                    nationality STRING\"\"\"\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14251522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th></tr>\n",
       "<tr><td>1</td><td>Scott</td><td>Tiger</td><td>1000.0</td><td>united states</td></tr>\n",
       "<tr><td>2</td><td>Henry</td><td>Ford</td><td>1250.0</td><td>India</td></tr>\n",
       "<tr><td>3</td><td>Nick</td><td>Junior</td><td>750.0</td><td>united KINGDOM</td></tr>\n",
       "<tr><td>4</td><td>Bill</td><td>Gomes</td><td>1500.0</td><td>AUSTRALIA</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+--------------+\n",
       "|employee_id|first_name|last_name|salary|   nationality|\n",
       "+-----------+----------+---------+------+--------------+\n",
       "|          1|     Scott|    Tiger|1000.0| united states|\n",
       "|          2|     Henry|     Ford|1250.0|         India|\n",
       "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
       "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
       "+-----------+----------+---------+------+--------------+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3846bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e90ab4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|\n",
      "|          2|     Henry|     Ford|1250.0|         India|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "833b9675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------+------------+\n",
      "|employee_id|salary|   nationality|   full_name|\n",
      "+-----------+------+--------------+------------+\n",
      "|          1|1000.0| united states|Scott, Tiger|\n",
      "|          2|1250.0|         India| Henry, Ford|\n",
      "|          3| 750.0|united KINGDOM|Nick, Junior|\n",
      "|          4|1500.0|     AUSTRALIA| Bill, Gomes|\n",
      "+-----------+------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, concat\n",
    "\n",
    "employeesDF.withColumn(\"full_name\",concat(\"first_name\",lit(\", \"), \"last_name\")). \\\n",
    "    drop('first_name','last_name'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e187eed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+------+--------------+\n",
      "|employee_id|concat(first_name, , , last_name)|salary|   nationality|\n",
      "+-----------+---------------------------------+------+--------------+\n",
      "|          1|                     Scott, Tiger|1000.0| united states|\n",
      "|          2|                      Henry, Ford|1250.0|         India|\n",
      "|          3|                     Nick, Junior| 750.0|united KINGDOM|\n",
      "|          4|                      Bill, Gomes|1500.0|     AUSTRALIA|\n",
      "+-----------+---------------------------------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(\"employee_id\",concat(\"first_name\",lit(\", \"),\"last_name\"),\"salary\",\"nationality\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41110081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+--------------+\n",
      "|employee_id|  full_name|salary|   nationality|\n",
      "+-----------+-----------+------+--------------+\n",
      "|          1|Scott,Tiger|1000.0| united states|\n",
      "|          2| Henry,Ford|1250.0|         India|\n",
      "|          3|Nick,Junior| 750.0|united KINGDOM|\n",
      "|          4| Bill,Gomes|1500.0|     AUSTRALIA|\n",
      "+-----------+-----------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(\"employee_id\",concat(\"first_name\",lit(\",\"),\"last_name\").alias(\"full_name\"),\"salary\",\"nationality\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef9e2214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------+--------------+\n",
      "|employee_id|   full_name|salary|   nationality|\n",
      "+-----------+------------+------+--------------+\n",
      "|          1|Scott, Tiger|1000.0| united states|\n",
      "|          2| Henry, Ford|1250.0|         India|\n",
      "|          3|Nick, Junior| 750.0|united KINGDOM|\n",
      "|          4| Bill, Gomes|1500.0|     AUSTRALIA|\n",
      "+-----------+------------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.selectExpr(\"employee_id\",\n",
    "                   \"concat(first_name, ', ', last_name) AS full_name\",\n",
    "                   \"salary\",\n",
    "                   \"nationality\"). \\\n",
    "        show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38173cf1",
   "metadata": {},
   "source": [
    "### 165 Overview of Spark Write APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f397c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a68779",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -rm -R -skipTrash /user/${USER}/retail_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eeb3f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark. \\\n",
    "    read. \\\n",
    "    csv('/public/retail_db/orders',\n",
    "           schema='''\n",
    "           order_id INT, order_date STRING,\n",
    "           order_customer_id INT,\n",
    "           order_status STRING\n",
    "           ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d16f12fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5233fb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39856052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135a77ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders. \\\n",
    "    write. \\\n",
    "    parquet(f'/user/{username}/retail_db/orders',\n",
    "           mode='overwrite',\n",
    "           compression='none'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e41d1481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-06 01:57 /user/itv011204/retail_db/orders/_SUCCESS\n",
      "-rw-r--r--   3 itv011204 supergroup     495238 2024-02-06 01:57 /user/itv011204/retail_db/orders/part-00000-a9a3470e-08a5-42d3-9438-5038540d7952-c000.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/retail_db/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ac5a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.selectExpr(\"employee_id\",\n",
    "                   \"concat(first_name, ', ', last_name) AS full_name\",\n",
    "                   \"salary\",\n",
    "                   \"nationality\"). \\\n",
    "            write. \\\n",
    "            parquet(f'/user/{username}/retail_db/employees',\n",
    "            mode='overwrite',\n",
    "            compression='none'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83b250d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-06 02:00 /user/itv011204/retail_db/employees/_SUCCESS\n",
      "-rw-r--r--   3 itv011204 supergroup       1157 2024-02-06 02:00 /user/itv011204/retail_db/employees/part-00000-f707a78c-b602-40a8-bdac-9e3695181dc5-c000.parquet\n",
      "-rw-r--r--   3 itv011204 supergroup       1172 2024-02-06 02:00 /user/itv011204/retail_db/employees/part-00001-f707a78c-b602-40a8-bdac-9e3695181dc5-c000.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/retail_db/employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41163c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders. \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    option('compression','none'). \\\n",
    "    parquet(f'/user/{username}/retail_db/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b72ac393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-06 02:02 /user/itv011204/retail_db/orders/_SUCCESS\n",
      "-rw-r--r--   3 itv011204 supergroup     495238 2024-02-06 02:02 /user/itv011204/retail_db/orders/part-00000-a681d504-fb58-47b4-8c41-17e84ba3f44a-c000.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/retail_db/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "527ed2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders. \\\n",
    "    write. \\\n",
    "    mode('overwrite').\\\n",
    "    option('compression', 'none'). \\\n",
    "    format('parquet'). \\\n",
    "    save(f'/user/{username}/retail_db/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e97d00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-06 02:06 /user/itv011204/retail_db/orders/_SUCCESS\n",
      "-rw-r--r--   3 itv011204 supergroup     495238 2024-02-06 02:06 /user/itv011204/retail_db/orders/part-00000-b6a9b40b-b9a5-47c2-a5e0-0cf13e1aee81-c000.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/retail_db/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78c97ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = spark. \\\n",
    "                read. \\\n",
    "                json('/public/retail_db_json/order_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ec83d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172198"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_items.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66ca8c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|            1|                  1|                  957|                  299.98|                  1|             299.98|\n",
      "|            2|                  2|                 1073|                  199.99|                  1|             199.99|\n",
      "|            3|                  2|                  502|                    50.0|                  5|              250.0|\n",
      "|            4|                  2|                  403|                  129.99|                  1|             129.99|\n",
      "|            5|                  4|                  897|                   24.99|                  2|              49.98|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99179435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- order_item_order_id: long (nullable = true)\n",
      " |-- order_item_product_id: long (nullable = true)\n",
      " |-- order_item_product_price: double (nullable = true)\n",
      " |-- order_item_quantity: long (nullable = true)\n",
      " |-- order_item_subtotal: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c00b5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    mode('ignore'). \\\n",
    "    option('compression', 'gzip'). \\\n",
    "    option('sep','|'). \\\n",
    "    format('csv'). \\\n",
    "    save(f'/user/{username}/retail_db/order_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcad4c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-06 02:12 /user/itv011204/retail_db/order_items/_SUCCESS\n",
      "-rw-r--r--   3 itv011204 supergroup    1032820 2024-02-06 02:12 /user/itv011204/retail_db/order_items/part-00000-d3794630-189b-43e2-956e-03bf04eb1a36-c000.csv.gz\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/retail_db/order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18213ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    csv(f'/user/{username}/retail_db/order_items',\n",
    "        sep='|',\n",
    "        mode='overwrite',\n",
    "        compression='gzip'\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d04820e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-06 02:26 /user/itv011204/retail_db/order_items/_SUCCESS\n",
      "-rw-r--r--   3 itv011204 supergroup    1032820 2024-02-06 02:26 /user/itv011204/retail_db/order_items/part-00000-a1395079-89c3-408e-b91f-583c788062f5-c000.csv.gz\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/retail_db/order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93efdf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    csv(f'/user/{username}/retail_db/order_items',\n",
    "        sep='|',\n",
    "        mode='ignore',\n",
    "        compression='gzip'\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a28d30b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-06 02:26 /user/itv011204/retail_db/order_items/_SUCCESS\n",
      "-rw-r--r--   3 itv011204 supergroup    1032820 2024-02-06 02:26 /user/itv011204/retail_db/order_items/part-00000-a1395079-89c3-408e-b91f-583c788062f5-c000.csv.gz\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/retail_db/order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39061348",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    csv(f'/user/{username}/retail_db/order_items',\n",
    "        sep='|',\n",
    "        mode='append',\n",
    "        compression='gzip'\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83098a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-06 02:27 /user/itv011204/retail_db/order_items/_SUCCESS\n",
      "-rw-r--r--   3 itv011204 supergroup    1032820 2024-02-06 02:27 /user/itv011204/retail_db/order_items/part-00000-0cb24dd5-c392-4bc7-bc18-075bfacc5f31-c000.csv.gz\n",
      "-rw-r--r--   3 itv011204 supergroup    1032820 2024-02-06 02:26 /user/itv011204/retail_db/order_items/part-00000-a1395079-89c3-408e-b91f-583c788062f5-c000.csv.gz\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/retail_db/order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93140fd2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path hdfs://m01.itversity.com:9000/user/itv011204/retail_db/order_items already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-55016fd51953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m        )\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1370\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0morc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path hdfs://m01.itversity.com:9000/user/itv011204/retail_db/order_items already exists."
     ]
    }
   ],
   "source": [
    "order_items. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    csv(f'/user/{username}/retail_db/order_items',\n",
    "        sep='|',\n",
    "        mode='error',\n",
    "        compression='gzip'\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad215a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    csv(f'/user/{username}/retail_db/order_items',\n",
    "        sep='|',\n",
    "        mode='overwrite',\n",
    "        compression='gzip'\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55496ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-06 02:28 /user/itv011204/retail_db/order_items/_SUCCESS\n",
      "-rw-r--r--   3 itv011204 supergroup    1032820 2024-02-06 02:28 /user/itv011204/retail_db/order_items/part-00000-1d4e3b93-b41b-4ccb-9d88-be4c32edbd05-c000.csv.gz\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}/retail_db/order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5519ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
