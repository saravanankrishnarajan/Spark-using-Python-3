{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caece799",
   "metadata": {},
   "source": [
    "##  Section 17 Apache Spark using Python - Joining Data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a31a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200 Prepare Datasets for Joining Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04619dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 17 Joining Data Sets'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b5b784b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.dynamicAllocation.minExecutors', '4'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.sql.repl.eagerEval.enabled', 'true'),\n",
       " ('spark.eventLog.dir', 'hdfs:///spark-logs'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://m02.itversity.com:19088/proxy/application_1707552082651_2938'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1707552082651_2938'),\n",
       " ('spark.app.startTime', '1707865966456'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '10'),\n",
       " ('spark.shuffle.io.connectionTimeout', '6000'),\n",
       " ('spark.driver.port', '39103'),\n",
       " ('spark.yarn.historyServer.address', 'm02.itversity.com:18080'),\n",
       " ('spark.driver.memory', '6g'),\n",
       " ('spark.yarn.jars', ''),\n",
       " ('spark.driver.appUIAddress', 'http://g02.itversity.com:45177'),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///spark-logs'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.history.fs.update.interval', '10s'),\n",
       " ('spark.driver.extraJavaOptions', '-Dderby.system.home=/tmp/derby/'),\n",
       " ('spark.app.name', 'itv011204 | Section 17 Joining Data Sets'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'm02.itversity.com'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.executor.extraLibraryPath', '/opt/hadoop/lib/native'),\n",
       " ('spark.history.ui.port', '18080'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.driver.host', 'g02.itversity.com'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.sql.warehouse.dir', '/user/itv011204/warehouse'),\n",
       " ('spark.app.id', 'application_1707552082651_2938'),\n",
       " ('spark.history.fs.cleaner.enabled', 'true'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip:/opt/spark-3.1.2-bin-hadoop3.2/python<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip'),\n",
       " ('spark.executor.memory', '6g'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.ui.port', '0'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4bda2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34 items\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:08 /public/Black_Friday\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:08 /public/Tableau\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:38 /public/Tableau_stocks\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:11 /public/TetrasoftBigDataHackathon\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:09 /public/addresses\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:04 /public/airlines_all\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:02 /public/black_friday\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:04 /public/cards\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:13 /public/citibike\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:13 /public/connect-distributed-config\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 07:49 /public/covid19\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:08 /public/crime\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-02-15 00:15 /public/gharchive\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:39 /public/githubactivity\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 10:51 /public/h1b\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 10:46 /public/hr_db\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 10:43 /public/icds_poc\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 10:43 /public/markov_data\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:56 /public/mocktest\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:04 /public/nyse\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:11 /public/nyse_all\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:28 /public/nyse_seq\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:31 /public/nyse_symbols\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:29 /public/randomtextwriter\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 10:59 /public/randomtextwritercompressed\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-08-21 03:48 /public/retail_db\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:09 /public/retail_db_json\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:05 /public/sms\n",
      "drwxrwxr-x+  - hdfs supergroup          0 2023-09-15 14:37 /public/trendytech\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:20 /public/used_vehicles\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:25 /public/weather\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-09-12 21:04 /public/wm_data\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:21 /public/yelp-dataset\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:26 /public/yelp-dataset-json\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /public/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2c08328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:33 /public/airlines_all/airlines\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:29 /public/airlines_all/airlines-part\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 10:48 /public/airlines_all/airport-codes\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /public/airlines_all/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca813a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 hdfs supergroup      11411 2021-01-28 10:48 /public/airlines_all/airport-codes/airport-codes-na.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /public/airlines_all/airport-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8b8886e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yuma\tAZ\tUSA\tYUM\tCanada\tYZFLa\tYWKCanada\tYQYada\tYZP"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat  /public/airlines_all/airport-codes/airport-codes-na.txt | tail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc17967",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodesPath = \"/public/airlines_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d6a8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodes = spark. \\\n",
    "    read. \\\n",
    "    option(\"sep\",\"\\t\"). \\\n",
    "    option(\"header\",True). \\\n",
    "    option(\"inferSchema\",True). \\\n",
    "    csv(airportCodesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3e4677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- IATA: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCodes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3a00d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21d4cb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|      City|State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|   BC| Canada| YXX|\n",
      "|  Aberdeen|   SD|    USA| ABR|\n",
      "|   Abilene|   TX|    USA| ABI|\n",
      "|     Akron|   OH|    USA| CAK|\n",
      "|   Alamosa|   CO|    USA| ALS|\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCodes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71caee52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 257 items\n",
      "-rw-r--r--   2 hdfs supergroup          0 2021-01-28 09:28 /public/airlines_all/airlines-part/_SUCCESS\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:32 /public/airlines_all/airlines-part/flightmonth=198710\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:40 /public/airlines_all/airlines-part/flightmonth=198711\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:41 /public/airlines_all/airlines-part/flightmonth=198712\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:37 /public/airlines_all/airlines-part/flightmonth=198801\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:44 /public/airlines_all/airlines-part/flightmonth=198802\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:30 /public/airlines_all/airlines-part/flightmonth=198803\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:02 /public/airlines_all/airlines-part/flightmonth=198804\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:30 /public/airlines_all/airlines-part/flightmonth=198805\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:32 /public/airlines_all/airlines-part/flightmonth=198806\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:37 /public/airlines_all/airlines-part/flightmonth=198807\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:37 /public/airlines_all/airlines-part/flightmonth=198808\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:19 /public/airlines_all/airlines-part/flightmonth=198809\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:44 /public/airlines_all/airlines-part/flightmonth=198810\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:42 /public/airlines_all/airlines-part/flightmonth=198811\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:41 /public/airlines_all/airlines-part/flightmonth=198812\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:09 /public/airlines_all/airlines-part/flightmonth=198901\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:17 /public/airlines_all/airlines-part/flightmonth=198902\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:48 /public/airlines_all/airlines-part/flightmonth=198903\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:09 /public/airlines_all/airlines-part/flightmonth=198904\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:01 /public/airlines_all/airlines-part/flightmonth=198905\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:21 /public/airlines_all/airlines-part/flightmonth=198906\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:56 /public/airlines_all/airlines-part/flightmonth=198907\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 10:41 /public/airlines_all/airlines-part/flightmonth=198908\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:31 /public/airlines_all/airlines-part/flightmonth=198909\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:57 /public/airlines_all/airlines-part/flightmonth=198910\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:06 /public/airlines_all/airlines-part/flightmonth=198911\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:01 /public/airlines_all/airlines-part/flightmonth=198912\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:42 /public/airlines_all/airlines-part/flightmonth=199001\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:02 /public/airlines_all/airlines-part/flightmonth=199002\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:51 /public/airlines_all/airlines-part/flightmonth=199003\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:37 /public/airlines_all/airlines-part/flightmonth=199004\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:00 /public/airlines_all/airlines-part/flightmonth=199005\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:59 /public/airlines_all/airlines-part/flightmonth=199006\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:36 /public/airlines_all/airlines-part/flightmonth=199007\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:31 /public/airlines_all/airlines-part/flightmonth=199008\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:32 /public/airlines_all/airlines-part/flightmonth=199009\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:09 /public/airlines_all/airlines-part/flightmonth=199010\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:17 /public/airlines_all/airlines-part/flightmonth=199011\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:00 /public/airlines_all/airlines-part/flightmonth=199012\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:21 /public/airlines_all/airlines-part/flightmonth=199101\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:36 /public/airlines_all/airlines-part/flightmonth=199102\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:41 /public/airlines_all/airlines-part/flightmonth=199103\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:41 /public/airlines_all/airlines-part/flightmonth=199104\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:37 /public/airlines_all/airlines-part/flightmonth=199105\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:35 /public/airlines_all/airlines-part/flightmonth=199106\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:06 /public/airlines_all/airlines-part/flightmonth=199107\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:17 /public/airlines_all/airlines-part/flightmonth=199108\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:30 /public/airlines_all/airlines-part/flightmonth=199109\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:30 /public/airlines_all/airlines-part/flightmonth=199110\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:31 /public/airlines_all/airlines-part/flightmonth=199111\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:42 /public/airlines_all/airlines-part/flightmonth=199112\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:32 /public/airlines_all/airlines-part/flightmonth=199201\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:28 /public/airlines_all/airlines-part/flightmonth=199202\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:35 /public/airlines_all/airlines-part/flightmonth=199203\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:53 /public/airlines_all/airlines-part/flightmonth=199204\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:21 /public/airlines_all/airlines-part/flightmonth=199205\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:24 /public/airlines_all/airlines-part/flightmonth=199206\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:49 /public/airlines_all/airlines-part/flightmonth=199207\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:26 /public/airlines_all/airlines-part/flightmonth=199208\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:25 /public/airlines_all/airlines-part/flightmonth=199209\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:21 /public/airlines_all/airlines-part/flightmonth=199210\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:32 /public/airlines_all/airlines-part/flightmonth=199211\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:41 /public/airlines_all/airlines-part/flightmonth=199212\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:21 /public/airlines_all/airlines-part/flightmonth=199301\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:19 /public/airlines_all/airlines-part/flightmonth=199302\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:54 /public/airlines_all/airlines-part/flightmonth=199303\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:32 /public/airlines_all/airlines-part/flightmonth=199304\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:37 /public/airlines_all/airlines-part/flightmonth=199305\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:12 /public/airlines_all/airlines-part/flightmonth=199306\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:35 /public/airlines_all/airlines-part/flightmonth=199307\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:43 /public/airlines_all/airlines-part/flightmonth=199308\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:41 /public/airlines_all/airlines-part/flightmonth=199309\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:28 /public/airlines_all/airlines-part/flightmonth=199310\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:59 /public/airlines_all/airlines-part/flightmonth=199311\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:07 /public/airlines_all/airlines-part/flightmonth=199312\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:49 /public/airlines_all/airlines-part/flightmonth=199401\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:18 /public/airlines_all/airlines-part/flightmonth=199402\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:07 /public/airlines_all/airlines-part/flightmonth=199403\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:24 /public/airlines_all/airlines-part/flightmonth=199404\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:43 /public/airlines_all/airlines-part/flightmonth=199405\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:42 /public/airlines_all/airlines-part/flightmonth=199406\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:22 /public/airlines_all/airlines-part/flightmonth=199407\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:37 /public/airlines_all/airlines-part/flightmonth=199408\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:37 /public/airlines_all/airlines-part/flightmonth=199409\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:30 /public/airlines_all/airlines-part/flightmonth=199410\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:41 /public/airlines_all/airlines-part/flightmonth=199411\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:09 /public/airlines_all/airlines-part/flightmonth=199412\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:20 /public/airlines_all/airlines-part/flightmonth=199501\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:24 /public/airlines_all/airlines-part/flightmonth=199502\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:42 /public/airlines_all/airlines-part/flightmonth=199503\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:23 /public/airlines_all/airlines-part/flightmonth=199504\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:28 /public/airlines_all/airlines-part/flightmonth=199505\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:21 /public/airlines_all/airlines-part/flightmonth=199506\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:36 /public/airlines_all/airlines-part/flightmonth=199507\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:09 /public/airlines_all/airlines-part/flightmonth=199508\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:22 /public/airlines_all/airlines-part/flightmonth=199509\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:35 /public/airlines_all/airlines-part/flightmonth=199510\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:26 /public/airlines_all/airlines-part/flightmonth=199511\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:31 /public/airlines_all/airlines-part/flightmonth=199512\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:41 /public/airlines_all/airlines-part/flightmonth=199601\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:31 /public/airlines_all/airlines-part/flightmonth=199602\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:30 /public/airlines_all/airlines-part/flightmonth=199603\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:40 /public/airlines_all/airlines-part/flightmonth=199604\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:41 /public/airlines_all/airlines-part/flightmonth=199605\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:21 /public/airlines_all/airlines-part/flightmonth=199606\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:25 /public/airlines_all/airlines-part/flightmonth=199607\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:48 /public/airlines_all/airlines-part/flightmonth=199608\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:07 /public/airlines_all/airlines-part/flightmonth=199609\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:40 /public/airlines_all/airlines-part/flightmonth=199610\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:41 /public/airlines_all/airlines-part/flightmonth=199611\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:40 /public/airlines_all/airlines-part/flightmonth=199612\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:41 /public/airlines_all/airlines-part/flightmonth=199701\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:10 /public/airlines_all/airlines-part/flightmonth=199702\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:20 /public/airlines_all/airlines-part/flightmonth=199703\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:09 /public/airlines_all/airlines-part/flightmonth=199704\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:26 /public/airlines_all/airlines-part/flightmonth=199705\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:32 /public/airlines_all/airlines-part/flightmonth=199706\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:32 /public/airlines_all/airlines-part/flightmonth=199707\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:19 /public/airlines_all/airlines-part/flightmonth=199708\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:48 /public/airlines_all/airlines-part/flightmonth=199709\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:37 /public/airlines_all/airlines-part/flightmonth=199710\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:49 /public/airlines_all/airlines-part/flightmonth=199711\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:41 /public/airlines_all/airlines-part/flightmonth=199712\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:17 /public/airlines_all/airlines-part/flightmonth=199801\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:30 /public/airlines_all/airlines-part/flightmonth=199802\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:37 /public/airlines_all/airlines-part/flightmonth=199803\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:42 /public/airlines_all/airlines-part/flightmonth=199804\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:25 /public/airlines_all/airlines-part/flightmonth=199805\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:27 /public/airlines_all/airlines-part/flightmonth=199806\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:00 /public/airlines_all/airlines-part/flightmonth=199807\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:36 /public/airlines_all/airlines-part/flightmonth=199808\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:41 /public/airlines_all/airlines-part/flightmonth=199809\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:01 /public/airlines_all/airlines-part/flightmonth=199810\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:09 /public/airlines_all/airlines-part/flightmonth=199811\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:37 /public/airlines_all/airlines-part/flightmonth=199812\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:40 /public/airlines_all/airlines-part/flightmonth=199901\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:11 /public/airlines_all/airlines-part/flightmonth=199902\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:48 /public/airlines_all/airlines-part/flightmonth=199903\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:41 /public/airlines_all/airlines-part/flightmonth=199904\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:36 /public/airlines_all/airlines-part/flightmonth=199905\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:00 /public/airlines_all/airlines-part/flightmonth=199906\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:59 /public/airlines_all/airlines-part/flightmonth=199907\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:19 /public/airlines_all/airlines-part/flightmonth=199908\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:56 /public/airlines_all/airlines-part/flightmonth=199909\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:36 /public/airlines_all/airlines-part/flightmonth=199910\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:20 /public/airlines_all/airlines-part/flightmonth=199911\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:26 /public/airlines_all/airlines-part/flightmonth=199912\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:42 /public/airlines_all/airlines-part/flightmonth=200001\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:48 /public/airlines_all/airlines-part/flightmonth=200002\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:08 /public/airlines_all/airlines-part/flightmonth=200003\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:32 /public/airlines_all/airlines-part/flightmonth=200004\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:19 /public/airlines_all/airlines-part/flightmonth=200005\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:20 /public/airlines_all/airlines-part/flightmonth=200006\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:57 /public/airlines_all/airlines-part/flightmonth=200007\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:25 /public/airlines_all/airlines-part/flightmonth=200008\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:25 /public/airlines_all/airlines-part/flightmonth=200009\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:19 /public/airlines_all/airlines-part/flightmonth=200010\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:44 /public/airlines_all/airlines-part/flightmonth=200011\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:20 /public/airlines_all/airlines-part/flightmonth=200012\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:00 /public/airlines_all/airlines-part/flightmonth=200101\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:19 /public/airlines_all/airlines-part/flightmonth=200102\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:32 /public/airlines_all/airlines-part/flightmonth=200103\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:57 /public/airlines_all/airlines-part/flightmonth=200104\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:19 /public/airlines_all/airlines-part/flightmonth=200105\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:49 /public/airlines_all/airlines-part/flightmonth=200106\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:37 /public/airlines_all/airlines-part/flightmonth=200107\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:03 /public/airlines_all/airlines-part/flightmonth=200108\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:20 /public/airlines_all/airlines-part/flightmonth=200109\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:23 /public/airlines_all/airlines-part/flightmonth=200110\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:00 /public/airlines_all/airlines-part/flightmonth=200111\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:26 /public/airlines_all/airlines-part/flightmonth=200112\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:23 /public/airlines_all/airlines-part/flightmonth=200201\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:36 /public/airlines_all/airlines-part/flightmonth=200202\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:28 /public/airlines_all/airlines-part/flightmonth=200203\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:49 /public/airlines_all/airlines-part/flightmonth=200204\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:37 /public/airlines_all/airlines-part/flightmonth=200205\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:24 /public/airlines_all/airlines-part/flightmonth=200206\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:53 /public/airlines_all/airlines-part/flightmonth=200207\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:47 /public/airlines_all/airlines-part/flightmonth=200208\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:36 /public/airlines_all/airlines-part/flightmonth=200209\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:23 /public/airlines_all/airlines-part/flightmonth=200210\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:21 /public/airlines_all/airlines-part/flightmonth=200211\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:30 /public/airlines_all/airlines-part/flightmonth=200212\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:20 /public/airlines_all/airlines-part/flightmonth=200301\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:03 /public/airlines_all/airlines-part/flightmonth=200302\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:21 /public/airlines_all/airlines-part/flightmonth=200303\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:31 /public/airlines_all/airlines-part/flightmonth=200304\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:30 /public/airlines_all/airlines-part/flightmonth=200305\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:57 /public/airlines_all/airlines-part/flightmonth=200306\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:32 /public/airlines_all/airlines-part/flightmonth=200307\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:36 /public/airlines_all/airlines-part/flightmonth=200308\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:31 /public/airlines_all/airlines-part/flightmonth=200309\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:21 /public/airlines_all/airlines-part/flightmonth=200310\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:19 /public/airlines_all/airlines-part/flightmonth=200311\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:43 /public/airlines_all/airlines-part/flightmonth=200312\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:31 /public/airlines_all/airlines-part/flightmonth=200401\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:28 /public/airlines_all/airlines-part/flightmonth=200402\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:20 /public/airlines_all/airlines-part/flightmonth=200403\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:26 /public/airlines_all/airlines-part/flightmonth=200404\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:00 /public/airlines_all/airlines-part/flightmonth=200405\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:42 /public/airlines_all/airlines-part/flightmonth=200406\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:25 /public/airlines_all/airlines-part/flightmonth=200407\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:17 /public/airlines_all/airlines-part/flightmonth=200408\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:51 /public/airlines_all/airlines-part/flightmonth=200409\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:25 /public/airlines_all/airlines-part/flightmonth=200410\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:07 /public/airlines_all/airlines-part/flightmonth=200411\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:17 /public/airlines_all/airlines-part/flightmonth=200412\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 10:41 /public/airlines_all/airlines-part/flightmonth=200501\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:30 /public/airlines_all/airlines-part/flightmonth=200502\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:37 /public/airlines_all/airlines-part/flightmonth=200503\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:07 /public/airlines_all/airlines-part/flightmonth=200504\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:51 /public/airlines_all/airlines-part/flightmonth=200505\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:51 /public/airlines_all/airlines-part/flightmonth=200506\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:03 /public/airlines_all/airlines-part/flightmonth=200507\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:21 /public/airlines_all/airlines-part/flightmonth=200508\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:30 /public/airlines_all/airlines-part/flightmonth=200509\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:43 /public/airlines_all/airlines-part/flightmonth=200510\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:26 /public/airlines_all/airlines-part/flightmonth=200511\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:48 /public/airlines_all/airlines-part/flightmonth=200512\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:21 /public/airlines_all/airlines-part/flightmonth=200601\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:40 /public/airlines_all/airlines-part/flightmonth=200602\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:24 /public/airlines_all/airlines-part/flightmonth=200603\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:26 /public/airlines_all/airlines-part/flightmonth=200604\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:09 /public/airlines_all/airlines-part/flightmonth=200605\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:55 /public/airlines_all/airlines-part/flightmonth=200606\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:48 /public/airlines_all/airlines-part/flightmonth=200607\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:42 /public/airlines_all/airlines-part/flightmonth=200608\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:21 /public/airlines_all/airlines-part/flightmonth=200609\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:47 /public/airlines_all/airlines-part/flightmonth=200610\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:31 /public/airlines_all/airlines-part/flightmonth=200611\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:20 /public/airlines_all/airlines-part/flightmonth=200612\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:23 /public/airlines_all/airlines-part/flightmonth=200701\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:30 /public/airlines_all/airlines-part/flightmonth=200702\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:36 /public/airlines_all/airlines-part/flightmonth=200703\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:40 /public/airlines_all/airlines-part/flightmonth=200704\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:25 /public/airlines_all/airlines-part/flightmonth=200705\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:31 /public/airlines_all/airlines-part/flightmonth=200706\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:53 /public/airlines_all/airlines-part/flightmonth=200707\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:36 /public/airlines_all/airlines-part/flightmonth=200708\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:51 /public/airlines_all/airlines-part/flightmonth=200709\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:43 /public/airlines_all/airlines-part/flightmonth=200710\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:19 /public/airlines_all/airlines-part/flightmonth=200711\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:48 /public/airlines_all/airlines-part/flightmonth=200712\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:28 /public/airlines_all/airlines-part/flightmonth=200801\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:48 /public/airlines_all/airlines-part/flightmonth=200802\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:37 /public/airlines_all/airlines-part/flightmonth=200803\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:41 /public/airlines_all/airlines-part/flightmonth=200804\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 11:21 /public/airlines_all/airlines-part/flightmonth=200805\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 08:57 /public/airlines_all/airlines-part/flightmonth=200806\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:19 /public/airlines_all/airlines-part/flightmonth=200807\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:41 /public/airlines_all/airlines-part/flightmonth=200808\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:31 /public/airlines_all/airlines-part/flightmonth=200809\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:26 /public/airlines_all/airlines-part/flightmonth=200810\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:26 /public/airlines_all/airlines-part/flightmonth=200811\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:35 /public/airlines_all/airlines-part/flightmonth=200812\n",
      "drwxr-xr-x   - hdfs supergroup          0 2021-01-28 09:42 /public/airlines_all/airlines-part/flightmonth=__HIVE_DEFAULT_PARTITION__\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /public/airlines_all/airlines-part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67d1c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 hdfs supergroup   14654075 2021-01-28 11:28 /public/airlines_all/airlines-part/flightmonth=200801/part-00252-5cde1303-4ebf-4a12-8fad-f5d9f9c9124a.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /public/airlines_all/airlines-part/flightmonth=200801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e112f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/public/retail_db_json\n",
      "/public/retail_db_json/categories\n",
      "/public/retail_db_json/categories/_SUCCESS\n",
      "/public/retail_db_json/categories/part-r-00000-ce1d8208-178d-48d3-bfb2-1a97d9c05094\n",
      "/public/retail_db_json/customers\n",
      "/public/retail_db_json/customers/_SUCCESS\n",
      "/public/retail_db_json/customers/part-r-00000-70554560-527b-44f6-9e80-4e2031af5994\n",
      "/public/retail_db_json/departments\n",
      "/public/retail_db_json/departments/_SUCCESS\n",
      "/public/retail_db_json/departments/part-r-00000-3db7cfae-3ad2-4fc7-88ff-afe0ec709f49\n",
      "/public/retail_db_json/order_details\n",
      "/public/retail_db_json/order_details/order_details.json\n",
      "/public/retail_db_json/order_items\n",
      "/public/retail_db_json/order_items/_SUCCESS\n",
      "/public/retail_db_json/order_items/part-r-00000-6b83977e-3f20-404b-9b5f-29376ab1419e\n",
      "/public/retail_db_json/orders\n",
      "/public/retail_db_json/orders/_SUCCESS\n",
      "/public/retail_db_json/orders/part-r-00000-990f5773-9005-49ba-b670-631286032674\n",
      "/public/retail_db_json/products\n",
      "/public/retail_db_json/products/_SUCCESS\n",
      "/public/retail_db_json/products/part-r-00000-158b7037-4a23-47e6-8cb3-8cbf878beff7\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -find /public/retail_db_json/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b712c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.json(\"/public/retail_db_json/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eca4a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a147adea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26e4c9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|      City|State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|   BC| Canada| YXX|\n",
      "|  Aberdeen|   SD|    USA| ABR|\n",
      "|   Abilene|   TX|    USA| ABI|\n",
      "|     Akron|   OH|    USA| CAK|\n",
      "|   Alamosa|   CO|    USA| ALS|\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCodes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c757a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abe31a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.dynamicAllocation.minExecutors', '4'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.sql.repl.eagerEval.enabled', 'true'),\n",
       " ('spark.eventLog.dir', 'hdfs:///spark-logs'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://m02.itversity.com:19088/proxy/application_1707552082651_2938'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1707552082651_2938'),\n",
       " ('spark.app.startTime', '1707865966456'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '10'),\n",
       " ('spark.shuffle.io.connectionTimeout', '6000'),\n",
       " ('spark.driver.port', '39103'),\n",
       " ('spark.yarn.historyServer.address', 'm02.itversity.com:18080'),\n",
       " ('spark.driver.memory', '6g'),\n",
       " ('spark.yarn.jars', ''),\n",
       " ('spark.driver.appUIAddress', 'http://g02.itversity.com:45177'),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///spark-logs'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.history.fs.update.interval', '10s'),\n",
       " ('spark.driver.extraJavaOptions', '-Dderby.system.home=/tmp/derby/'),\n",
       " ('spark.app.name', 'itv011204 | Section 17 Joining Data Sets'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'm02.itversity.com'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.executor.extraLibraryPath', '/opt/hadoop/lib/native'),\n",
       " ('spark.history.ui.port', '18080'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.driver.host', 'g02.itversity.com'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.sql.warehouse.dir', '/user/itv011204/warehouse'),\n",
       " ('spark.app.id', 'application_1707552082651_2938'),\n",
       " ('spark.history.fs.cleaner.enabled', 'true'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip:/opt/spark-3.1.2-bin-hadoop3.2/python<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip'),\n",
       " ('spark.executor.memory', '6g'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.ui.port', '0'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "329fb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark. \\\n",
    "    read. \\\n",
    "    parquet(\"/public/airlines_all/airlines-part/flightmonth=200801\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ac9e6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605659"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "366ba7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      " |-- IsArrDelayed: string (nullable = true)\n",
      " |-- IsDepDelayed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41414a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "885e684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|      City|State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|   BC| Canada| YXX|\n",
      "|  Aberdeen|   SD|    USA| ABR|\n",
      "|   Abilene|   TX|    USA| ABI|\n",
      "|     Akron|   OH|    USA| CAK|\n",
      "|   Alamosa|   CO|    USA| ALS|\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCodes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "171f8c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- IATA: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCodes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "514aab06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes. \\\n",
    "    select(\"IATA\"). \\\n",
    "    distinct(). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b67f9b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|IATA|iata_count|\n",
      "+----+----------+\n",
      "| Big|         3|\n",
      "| BHM|         1|\n",
      "| ABR|         1|\n",
      "| ABI|         1|\n",
      "| ALB|         1|\n",
      "| AEX|         1|\n",
      "| ABE|         1|\n",
      "| APN|         1|\n",
      "| YAA|         1|\n",
      "| ATW|         1|\n",
      "| YEK|         1|\n",
      "| ASE|         1|\n",
      "| ACY|         1|\n",
      "| AGS|         1|\n",
      "| AUS|         1|\n",
      "| YBG|         1|\n",
      "| BRW|         1|\n",
      "| BKW|         1|\n",
      "| BLI|         1|\n",
      "| BTT|         1|\n",
      "+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, lit,col\n",
    "\n",
    "airportCodes. \\\n",
    "    groupBy('IATA'). \\\n",
    "    agg(count(lit(1)).alias(\"iata_count\")). \\\n",
    "    orderBy(col('Iata_count').desc()). \\\n",
    "    show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2feeb30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|IATA|iata_count|\n",
      "+----+----------+\n",
      "| Big|         3|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCodes. \\\n",
    "    groupBy('IATA'). \\\n",
    "    agg(count(lit(1)).alias(\"iata_count\")). \\\n",
    "    filter(col('iata_count')>1). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e0a96a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-------+----+\n",
      "|       City| State|Country|IATA|\n",
      "+-----------+------+-------+----+\n",
      "|       Hilo|    HI|    USA| Big|\n",
      "|Kailua-Kona|Hawaii|    USA| Big|\n",
      "|    Kamuela|Hawaii|    USA| Big|\n",
      "+-----------+------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCodes. \\\n",
    "    filter(col(\"IATA\")=='Big'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "122b0351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-------+----+\n",
      "|       City| State|Country|IATA|\n",
      "+-----------+------+-------+----+\n",
      "|       Hilo|    HI|    USA| Big|\n",
      "|Kailua-Kona|Hawaii|    USA| Big|\n",
      "|    Kamuela|Hawaii|    USA| Big|\n",
      "+-----------+------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCodes. \\\n",
    "    filter(\"IATA = 'Big'\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb9d0bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "|   Alliance|   NE|    USA| AIA|\n",
      "|     Alpena|   MI|    USA| APN|\n",
      "|    Altoona|   PA|    USA| AOO|\n",
      "|   Amarillo|   TX|    USA| AMA|\n",
      "|Anahim Lake|   BC| Canada| YAA|\n",
      "|  Anchorage|   AK|    USA| ANC|\n",
      "|   Appleton|   WI|    USA| ATW|\n",
      "|     Arviat|  NWT| Canada| YEK|\n",
      "|  Asheville|   NC|    USA| AVL|\n",
      "|      Aspen|   CO|    USA| ASE|\n",
      "+-----------+-----+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCodes. \\\n",
    "    filter(\"!(State='Hawaii' AND IATA='Big')\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5622357b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes. \\\n",
    "    filter(\"!(State='Hawaii' AND IATA='Big')\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "220c00e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/public/airlines_all/airport-codes'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodesPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b280f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.\\\n",
    "    read. \\\n",
    "    option(\"sep\",\"\\t\"). \\\n",
    "    option(\"header\",True). \\\n",
    "    option(\"inferSchema\",True). \\\n",
    "    csv(airportCodesPath). \\\n",
    "    filter(\"!(State='Hawaii' AND IATA='Big') AND Country='USA'\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a07050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodes = spark.\\\n",
    "        read. \\\n",
    "        option(\"sep\",\"\\t\"). \\\n",
    "        option(\"header\",True). \\\n",
    "        option(\"inferSchema\",True). \\\n",
    "        csv(airportCodesPath). \\\n",
    "        filter(\"!(State='Hawaii' AND IATA='Big') AND Country='USA'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db885503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb7efc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCountByState = airportCodes. \\\n",
    "    groupBy(\"Country\",\"State\"). \\\n",
    "    agg(count(lit(1)).alias(\"IATACount\")). \\\n",
    "    orderBy(col(\"IATACount\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23c825c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+\n",
      "|Country|State|IATACount|\n",
      "+-------+-----+---------+\n",
      "|    USA|   CA|       29|\n",
      "|    USA|   TX|       26|\n",
      "|    USA|   AK|       25|\n",
      "|    USA|   MI|       18|\n",
      "|    USA|   FL|       18|\n",
      "|    USA|   NY|       18|\n",
      "|    USA|   MT|       14|\n",
      "|    USA|   PA|       13|\n",
      "|    USA|   CO|       12|\n",
      "|    USA|   IL|       12|\n",
      "|    USA|   WY|       10|\n",
      "|    USA|   NC|       10|\n",
      "|    USA|   GA|        9|\n",
      "|    USA|   NM|        9|\n",
      "|    USA|   WA|        9|\n",
      "|    USA|   WI|        9|\n",
      "|    USA|   HI|        9|\n",
      "|    USA|   NE|        9|\n",
      "|    USA|   KS|        9|\n",
      "|    USA|   IA|        8|\n",
      "+-------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCountByState.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c494387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCountByState.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ec2a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    " orders = spark.read.json(\"/public/retail_db_json/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "389f61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = spark.read.json(\"/public/retail_db_json/order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6040838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb020ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- order_item_order_id: long (nullable = true)\n",
      " |-- order_item_product_id: long (nullable = true)\n",
      " |-- order_item_product_price: double (nullable = true)\n",
      " |-- order_item_quantity: long (nullable = true)\n",
      " |-- order_item_subtotal: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c9ef0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12405"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "orders.\\\n",
    "    select(col(\"order_customer_id\")). \\\n",
    "    distinct(). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1bed68d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "30e348d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|order_customer_id|order_count|\n",
      "+-----------------+-----------+\n",
      "|              569|         16|\n",
      "|             5897|         16|\n",
      "|            12431|         16|\n",
      "|             6316|         16|\n",
      "|             5283|         15|\n",
      "|             5654|         15|\n",
      "|            12284|         15|\n",
      "|              221|         15|\n",
      "|             4320|         15|\n",
      "|             5624|         15|\n",
      "|             4517|         14|\n",
      "|             3710|         14|\n",
      "|             4116|         14|\n",
      "|             1011|         14|\n",
      "|             1940|         14|\n",
      "|             6248|         14|\n",
      "|              791|         14|\n",
      "|            10591|         14|\n",
      "|             4249|         14|\n",
      "|            11689|         14|\n",
      "+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy(col(\"order_customer_id\")). \\\n",
    "    agg(count(lit(1)).alias(\"order_count\")). \\\n",
    "    orderBy(col(\"order_count\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37dc64a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method join in module pyspark.sql.dataframe:\n",
      "\n",
      "join(other, on=None, how=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Joins with another :class:`DataFrame`, using the given join expression.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    other : :class:`DataFrame`\n",
      "        Right side of the join\n",
      "    on : str, list or :class:`Column`, optional\n",
      "        a string for the join column name, a list of column names,\n",
      "        a join expression (Column), or a list of Columns.\n",
      "        If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "        the column(s) must exist on both sides, and this performs an equi-join.\n",
      "    how : str, optional\n",
      "        default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      "        ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      "        ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      "        ``anti``, ``leftanti`` and ``left_anti``.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    The following performs a full outer join between ``df1`` and ``df2``.\n",
      "    \n",
      "    >>> from pyspark.sql.functions import desc\n",
      "    >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height)                 .sort(desc(\"name\")).collect()\n",
      "    [Row(name='Bob', height=85), Row(name='Alice', height=None), Row(name=None, height=80)]\n",
      "    \n",
      "    >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).collect()\n",
      "    [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      "    \n",
      "    >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      "    >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      "    [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "    \n",
      "    >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      "    [Row(name='Bob', height=85)]\n",
      "    \n",
      "    >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      "    [Row(name='Bob', age=5)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(orders.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc7b768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_join = orders.join(\n",
    "    order_items,\n",
    "    orders.order_id == order_items.order_item_order_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4cef16da",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_join = orders.join(\n",
    "    order_items,\n",
    "    on=orders.order_id == order_items.order_item_order_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c09ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_join = orders.join(\n",
    "    order_items,\n",
    "    on = col(\"order_id\") == col(\"order_item_order_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7fbabb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_join = orders.join(\n",
    "    order_items,\n",
    "    on=orders['order_id']==order_items[\"order_item_order_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c27dbfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_join = orders.join(\n",
    "    order_items,\n",
    "    on=orders['order_id']==order_items[\"order_item_order_id\"],\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7382225c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- order_item_order_id: long (nullable = true)\n",
      " |-- order_item_product_id: long (nullable = true)\n",
      " |-- order_item_product_price: double (nullable = true)\n",
      " |-- order_item_quantity: long (nullable = true)\n",
      " |-- order_item_subtotal: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_join.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "446ad812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-----------------+--------------------+--------+---------------+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|            1|                  1|                  957|                  299.98|                  1|             299.98|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|            2|                  2|                 1073|                  199.99|                  1|             199.99|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|            3|                  2|                  502|                    50.0|                  5|              250.0|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|            4|                  2|                  403|                  129.99|                  1|             129.99|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|            5|                  4|                  897|                   24.99|                  2|              49.98|\n",
      "+-----------------+--------------------+--------+---------------+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_join.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "932bc477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "314e8770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172198"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_items.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2f2c6269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172198"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cbd45484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+-------------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|order_item_subtotal|\n",
      "+-----------------+--------------------+--------+---------------+-------------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|             299.98|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|             199.99|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|              250.0|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|             129.99|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|              49.98|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|             299.95|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|              150.0|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|             199.92|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|             299.98|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|             299.95|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|              99.96|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|             299.98|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|             129.99|\n",
      "|             4530|2013-07-25 00:00:...|       7|       COMPLETE|             199.99|\n",
      "|             4530|2013-07-25 00:00:...|       7|       COMPLETE|             299.98|\n",
      "|             4530|2013-07-25 00:00:...|       7|       COMPLETE|              79.95|\n",
      "|             2911|2013-07-25 00:00:...|       8|     PROCESSING|             179.97|\n",
      "|             2911|2013-07-25 00:00:...|       8|     PROCESSING|             299.95|\n",
      "|             2911|2013-07-25 00:00:...|       8|     PROCESSING|             199.92|\n",
      "|             2911|2013-07-25 00:00:...|       8|     PROCESSING|               50.0|\n",
      "+-----------------+--------------------+--------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    join(order_items,\n",
    "        on = orders['order_id'] == order_items['order_item_order_id'],\n",
    "        how='inner'\n",
    "    ). \\\n",
    "    select(orders['*'],order_items['order_item_subtotal']). \\\n",
    "    show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bfe9d954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------+-------------------+\n",
      "|order_id|          order_date|   order_status|order_item_subtotal|\n",
      "+--------+--------------------+---------------+-------------------+\n",
      "|       1|2013-07-25 00:00:...|         CLOSED|             299.98|\n",
      "|       2|2013-07-25 00:00:...|PENDING_PAYMENT|             199.99|\n",
      "|       2|2013-07-25 00:00:...|PENDING_PAYMENT|              250.0|\n",
      "|       2|2013-07-25 00:00:...|PENDING_PAYMENT|             129.99|\n",
      "|       4|2013-07-25 00:00:...|         CLOSED|              49.98|\n",
      "|       4|2013-07-25 00:00:...|         CLOSED|             299.95|\n",
      "|       4|2013-07-25 00:00:...|         CLOSED|              150.0|\n",
      "|       4|2013-07-25 00:00:...|         CLOSED|             199.92|\n",
      "|       5|2013-07-25 00:00:...|       COMPLETE|             299.98|\n",
      "|       5|2013-07-25 00:00:...|       COMPLETE|             299.95|\n",
      "|       5|2013-07-25 00:00:...|       COMPLETE|              99.96|\n",
      "|       5|2013-07-25 00:00:...|       COMPLETE|             299.98|\n",
      "|       5|2013-07-25 00:00:...|       COMPLETE|             129.99|\n",
      "|       7|2013-07-25 00:00:...|       COMPLETE|             199.99|\n",
      "|       7|2013-07-25 00:00:...|       COMPLETE|             299.98|\n",
      "|       7|2013-07-25 00:00:...|       COMPLETE|              79.95|\n",
      "|       8|2013-07-25 00:00:...|     PROCESSING|             179.97|\n",
      "|       8|2013-07-25 00:00:...|     PROCESSING|             299.95|\n",
      "|       8|2013-07-25 00:00:...|     PROCESSING|             199.92|\n",
      "|       8|2013-07-25 00:00:...|     PROCESSING|               50.0|\n",
      "+--------+--------------------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    join(\n",
    "        order_items,\n",
    "        on = orders.order_id == order_items.order_item_order_id,\n",
    "        how ='inner'\n",
    "    ). \\\n",
    "    select( orders.order_id, orders.order_date, orders.order_status, order_items.order_item_subtotal). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e60c1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------+-------------------+\n",
      "|order_id|          order_date|   order_status|order_item_subtotal|\n",
      "+--------+--------------------+---------------+-------------------+\n",
      "|       1|2013-07-25 00:00:...|         CLOSED|             299.98|\n",
      "|       2|2013-07-25 00:00:...|PENDING_PAYMENT|             199.99|\n",
      "|       2|2013-07-25 00:00:...|PENDING_PAYMENT|              250.0|\n",
      "|       2|2013-07-25 00:00:...|PENDING_PAYMENT|             129.99|\n",
      "|       4|2013-07-25 00:00:...|         CLOSED|              49.98|\n",
      "|       4|2013-07-25 00:00:...|         CLOSED|             299.95|\n",
      "|       4|2013-07-25 00:00:...|         CLOSED|              150.0|\n",
      "|       4|2013-07-25 00:00:...|         CLOSED|             199.92|\n",
      "|       5|2013-07-25 00:00:...|       COMPLETE|             299.98|\n",
      "|       5|2013-07-25 00:00:...|       COMPLETE|             299.95|\n",
      "|       5|2013-07-25 00:00:...|       COMPLETE|              99.96|\n",
      "|       5|2013-07-25 00:00:...|       COMPLETE|             299.98|\n",
      "|       5|2013-07-25 00:00:...|       COMPLETE|             129.99|\n",
      "|       7|2013-07-25 00:00:...|       COMPLETE|             199.99|\n",
      "|       7|2013-07-25 00:00:...|       COMPLETE|             299.98|\n",
      "|       7|2013-07-25 00:00:...|       COMPLETE|              79.95|\n",
      "|       8|2013-07-25 00:00:...|     PROCESSING|             179.97|\n",
      "|       8|2013-07-25 00:00:...|     PROCESSING|             299.95|\n",
      "|       8|2013-07-25 00:00:...|     PROCESSING|             199.92|\n",
      "|       8|2013-07-25 00:00:...|     PROCESSING|               50.0|\n",
      "+--------+--------------------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    join(\n",
    "        order_items,\n",
    "        on = orders[\"order_id\"] == order_items[\"order_item_order_id\"],\n",
    "        how ='inner'\n",
    "    ). \\\n",
    "    select( orders[\"order_id\"], orders[\"order_date\"], orders[\"order_status\"], order_items[\"order_item_subtotal\"]). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b50b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.json('/public/retail_db_json/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ffb8470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = spark.read.json('/public/retail_db_json/customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2eaaeb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "73ead4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- customer_fname: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_lname: string (nullable = true)\n",
      " |-- customer_password: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- customer_street: string (nullable = true)\n",
      " |-- customer_zipcode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "990f096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method join in module pyspark.sql.dataframe:\n",
      "\n",
      "join(other, on=None, how=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Joins with another :class:`DataFrame`, using the given join expression.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    other : :class:`DataFrame`\n",
      "        Right side of the join\n",
      "    on : str, list or :class:`Column`, optional\n",
      "        a string for the join column name, a list of column names,\n",
      "        a join expression (Column), or a list of Columns.\n",
      "        If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "        the column(s) must exist on both sides, and this performs an equi-join.\n",
      "    how : str, optional\n",
      "        default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      "        ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      "        ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      "        ``anti``, ``leftanti`` and ``left_anti``.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    The following performs a full outer join between ``df1`` and ``df2``.\n",
      "    \n",
      "    >>> from pyspark.sql.functions import desc\n",
      "    >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height)                 .sort(desc(\"name\")).collect()\n",
      "    [Row(name='Bob', height=85), Row(name='Alice', height=None), Row(name=None, height=80)]\n",
      "    \n",
      "    >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).collect()\n",
      "    [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      "    \n",
      "    >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      "    >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      "    [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      "    \n",
      "    >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      "    [Row(name='Bob', height=85)]\n",
      "    \n",
      "    >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      "    [Row(name='Bob', age=5)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(customers.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aebf7b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+--------------------+---------------+\n",
      "|customer_id|order_customer_id|          order_date|   order_status|\n",
      "+-----------+-----------------+--------------------+---------------+\n",
      "|          1|                1|2013-12-13 00:00:...|       COMPLETE|\n",
      "|          2|                2|2013-11-30 00:00:...|       COMPLETE|\n",
      "|          2|                2|2013-08-02 00:00:...|        ON_HOLD|\n",
      "|          2|                2|2014-02-18 00:00:...|       COMPLETE|\n",
      "|          2|                2|2013-10-29 00:00:...|PENDING_PAYMENT|\n",
      "+-----------+-----------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers.join(\n",
    "    orders,\n",
    "    on = customers.customer_id == orders.order_customer_id,\n",
    "    how = 'left_outer'\n",
    "    ). \\\n",
    "    select(customers.customer_id, orders.order_customer_id,orders.order_date, orders.order_status). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba513408",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Resolved attribute(s) customer_fname#901,customer_lname#903 missing from customer_id#902L,order_customer_id#884L,order_date#885,order_status#887 in operator !Project [customer_id#902L, order_customer_id#884L, order_date#885, order_status#887, concat(customer_fname#901,  , customer_lname#903) AS customer_fullname#998].;\n!Project [customer_id#902L, order_customer_id#884L, order_date#885, order_status#887, concat(customer_fname#901,  , customer_lname#903) AS customer_fullname#998]\n+- Project [customer_id#902L, order_customer_id#884L, order_date#885, order_status#887]\n   +- Join LeftOuter, (customer_id#902L = order_customer_id#884L)\n      :- Relation[customer_city#899,customer_email#900,customer_fname#901,customer_id#902L,customer_lname#903,customer_password#904,customer_state#905,customer_street#906,customer_zipcode#907] json\n      +- Relation[order_customer_id#884L,order_date#885,order_id#886L,order_status#887] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-353dbd287b81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     ). \\\n\u001b[1;32m      8\u001b[0m     \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustomers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustomer_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_customer_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'customer_fullname'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustomers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustomer_fname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustomers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustomer_lname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \"\"\"\n\u001b[1;32m   2454\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Resolved attribute(s) customer_fname#901,customer_lname#903 missing from customer_id#902L,order_customer_id#884L,order_date#885,order_status#887 in operator !Project [customer_id#902L, order_customer_id#884L, order_date#885, order_status#887, concat(customer_fname#901,  , customer_lname#903) AS customer_fullname#998].;\n!Project [customer_id#902L, order_customer_id#884L, order_date#885, order_status#887, concat(customer_fname#901,  , customer_lname#903) AS customer_fullname#998]\n+- Project [customer_id#902L, order_customer_id#884L, order_date#885, order_status#887]\n   +- Join LeftOuter, (customer_id#902L = order_customer_id#884L)\n      :- Relation[customer_city#899,customer_email#900,customer_fname#901,customer_id#902L,customer_lname#903,customer_password#904,customer_state#905,customer_street#906,customer_zipcode#907] json\n      +- Relation[order_customer_id#884L,order_date#885,order_id#886L,order_status#887] json\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat,lit\n",
    "\n",
    "customers.join(\n",
    "    orders,\n",
    "    on = customers.customer_id == orders.order_customer_id,\n",
    "    how = 'left_outer'\n",
    "    ). \\\n",
    "    select(customers.customer_id, orders.order_customer_id,orders.order_date, orders.order_status). \\\n",
    "    withColumn('customer_fullname',concat(customers.customer_fname,lit(' '), customers.customer_lname)) .\\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8549c4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-----------------+--------------------+---------------+\n",
      "|customer_id|customer_fullname|order_customer_id|          order_date|   order_status|\n",
      "+-----------+-----------------+-----------------+--------------------+---------------+\n",
      "|          1|Richard Hernandez|                1|2013-12-13 00:00:...|       COMPLETE|\n",
      "|          2|     Mary Barrett|                2|2013-11-30 00:00:...|       COMPLETE|\n",
      "|          2|     Mary Barrett|                2|2013-08-02 00:00:...|        ON_HOLD|\n",
      "|          2|     Mary Barrett|                2|2014-02-18 00:00:...|       COMPLETE|\n",
      "|          2|     Mary Barrett|                2|2013-10-29 00:00:...|PENDING_PAYMENT|\n",
      "+-----------+-----------------+-----------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat,lit\n",
    "\n",
    "customers.join(\n",
    "    orders,\n",
    "    on = customers.customer_id == orders.order_customer_id,\n",
    "    how = 'left_outer'\n",
    "    ). \\\n",
    "    select(customers.customer_id,\n",
    "           concat(customers.customer_fname,lit(' '), customers.customer_lname).alias('customer_fullname'),\n",
    "           orders.order_customer_id,orders.order_date, orders.order_status). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "343d96f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrame in module pyspark.sql.dataframe object:\n",
      "\n",
      "class DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      " |  A distributed collection of data grouped into named columns.\n",
      " |  \n",
      " |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
      " |  and can be created using various functions in :class:`SparkSession`::\n",
      " |  \n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |  Once created, it can be manipulated using the various domain-specific-language\n",
      " |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
      " |  \n",
      " |  To select a column from the :class:`DataFrame`, use the apply method::\n",
      " |  \n",
      " |      ageCol = people.age\n",
      " |  \n",
      " |  A more concrete example::\n",
      " |  \n",
      " |      # To create DataFrame using SparkSession\n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |      department = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |      people.filter(people.age > 30).join(department, people.deptId == department.id) \\\n",
      " |        .groupBy(department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n",
      " |  \n",
      " |  .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrame\n",
      " |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n",
      " |      pyspark.sql.pandas.conversion.PandasConversionMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |      Returns the :class:`Column` denoted by ``name``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.age).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |  \n",
      " |  __getitem__(self, item)\n",
      " |      Returns the column as a :class:`Column`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df['age']).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |      >>> df[ [\"name\", \"age\"]].collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df[ df.age > 3 ].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df[0] > 3].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  __init__(self, jdf, sql_ctx)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  agg(self, *exprs)\n",
      " |      Aggregate on the entire :class:`DataFrame` without groups\n",
      " |      (shorthand for ``df.groupBy().agg()``).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.agg({\"age\": \"max\"}).collect()\n",
      " |      [Row(max(age)=5)]\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.agg(F.min(df.age)).collect()\n",
      " |      [Row(min(age)=2)]\n",
      " |  \n",
      " |  alias(self, alias)\n",
      " |      Returns a new :class:`DataFrame` with an alias set.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      alias : str\n",
      " |          an alias name to be set for the :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df_as1 = df.alias(\"df_as1\")\n",
      " |      >>> df_as2 = df.alias(\"df_as2\")\n",
      " |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      " |      >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\")                 .sort(desc(\"df_as1.name\")).collect()\n",
      " |      [Row(name='Bob', name='Bob', age=5), Row(name='Alice', name='Alice', age=2)]\n",
      " |  \n",
      " |  approxQuantile(self, col, probabilities, relativeError)\n",
      " |      Calculates the approximate quantiles of numerical columns of a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The result of this algorithm has the following deterministic bound:\n",
      " |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      " |      probability `p` up to error `err`, then the algorithm will return\n",
      " |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      " |      close to (p * N). More precisely,\n",
      " |      \n",
      " |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      " |      \n",
      " |      This method implements a variation of the Greenwald-Khanna\n",
      " |      algorithm (with some speed optimizations). The algorithm was first\n",
      " |      present in [[https://doi.org/10.1145/375663.375670\n",
      " |      Space-efficient Online Computation of Quantile Summaries]]\n",
      " |      by Greenwald and Khanna.\n",
      " |      \n",
      " |      Note that null values will be ignored in numerical columns before calculation.\n",
      " |      For columns only containing null values, an empty list is returned.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col: str, tuple or list\n",
      " |          Can be a single column name, or a list of names for multiple columns.\n",
      " |      \n",
      " |          .. versionchanged:: 2.2\n",
      " |             Added support for multiple columns.\n",
      " |      probabilities : list or tuple\n",
      " |          a list of quantile probabilities\n",
      " |          Each number must belong to [0, 1].\n",
      " |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      " |      relativeError : float\n",
      " |          The relative target precision to achieve\n",
      " |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
      " |          could be very expensive. Note that values greater than 1 are\n",
      " |          accepted but give the same result as 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          the approximate quantiles at the given probabilities. If\n",
      " |          the input `col` is a string, the output is a list of floats. If the\n",
      " |          input `col` is a list or tuple of strings, the output is also a\n",
      " |          list, but each element in it is a list of floats, i.e., the output\n",
      " |          is a list of list of floats.\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      " |  \n",
      " |  checkpoint(self, eager=True)\n",
      " |      Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n",
      " |      logical plan of this :class:`DataFrame`, which is especially useful in iterative algorithms\n",
      " |      where the plan may grow exponentially. It will be saved to files inside the checkpoint\n",
      " |      directory set with :meth:`SparkContext.setCheckpointDir`.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eager : bool, optional\n",
      " |          Whether to checkpoint this :class:`DataFrame` immediately\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental.\n",
      " |  \n",
      " |  coalesce(self, numPartitions)\n",
      " |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      " |      \n",
      " |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      " |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      " |      there will not be a shuffle, instead each of the 100 new partitions will\n",
      " |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      " |      it will stay at the current number of partitions.\n",
      " |      \n",
      " |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      " |      this may result in your computation taking place on fewer nodes than\n",
      " |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      " |      you can call repartition(). This will add a shuffle step, but means the\n",
      " |      current upstream partitions will be executed in parallel (per whatever\n",
      " |      the current partitioning is).\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          specify the target number of partitions\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
      " |      1\n",
      " |  \n",
      " |  colRegex(self, colName)\n",
      " |      Selects column based on the column name specified as a regex and returns it\n",
      " |      as :class:`Column`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colName : str\n",
      " |          string, column name specified as a regex.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      " |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      " |      +----+\n",
      " |      |Col2|\n",
      " |      +----+\n",
      " |      |   1|\n",
      " |      |   2|\n",
      " |      |   3|\n",
      " |      +----+\n",
      " |  \n",
      " |  collect(self)\n",
      " |      Returns all the records as a list of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  corr(self, col1, col2, method=None)\n",
      " |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      " |      Currently only supports the Pearson Correlation Coefficient.\n",
      " |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column\n",
      " |      col2 : str\n",
      " |          The name of the second column\n",
      " |      method : str, optional\n",
      " |          The correlation method. Currently only supports \"pearson\"\n",
      " |  \n",
      " |  count(self)\n",
      " |      Returns the number of rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.count()\n",
      " |      2\n",
      " |  \n",
      " |  cov(self, col1, col2)\n",
      " |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      " |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column\n",
      " |      col2 : str\n",
      " |          The name of the second column\n",
      " |  \n",
      " |  createGlobalTempView(self, name)\n",
      " |      Creates a global temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createGlobalTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |  \n",
      " |  createOrReplaceGlobalTempView(self, name)\n",
      " |      Creates or replaces a global temporary view using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |  \n",
      " |  createOrReplaceTempView(self, name)\n",
      " |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createOrReplaceTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |  \n",
      " |  createTempView(self, name)\n",
      " |      Creates a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |  \n",
      " |  crossJoin(self, other)\n",
      " |      Returns the cartesian product with another :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Right side of the cartesian product.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(\"age\", \"name\").collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df2.select(\"name\", \"height\").collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85)]\n",
      " |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n",
      " |      [Row(age=2, name='Alice', height=80), Row(age=2, name='Alice', height=85),\n",
      " |       Row(age=5, name='Bob', height=80), Row(age=5, name='Bob', height=85)]\n",
      " |  \n",
      " |  crosstab(self, col1, col2)\n",
      " |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      " |      table. The number of distinct values for each column should be less than 1e4. At most 1e6\n",
      " |      non-zero pair frequencies will be returned.\n",
      " |      The first column of each row will be the distinct values of `col1` and the column names\n",
      " |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      " |      Pairs that have no occurrences will have zero as their counts.\n",
      " |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column. Distinct items will make the first item of\n",
      " |          each row.\n",
      " |      col2 : str\n",
      " |          The name of the second column. Distinct items will make the column names\n",
      " |          of the :class:`DataFrame`.\n",
      " |  \n",
      " |  cube(self, *cols)\n",
      " |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregations on them.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      | null|   2|    1|\n",
      " |      | null|   5|    1|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |  \n",
      " |  describe(self, *cols)\n",
      " |      Computes basic statistics for numeric and string columns.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      This include count, mean, stddev, min, and max. If no columns are\n",
      " |      given, this function computes statistics for all numerical or string columns.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      Use summary for expanded statistics and control over which statistics to compute.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.describe(['age']).show()\n",
      " |      +-------+------------------+\n",
      " |      |summary|               age|\n",
      " |      +-------+------------------+\n",
      " |      |  count|                 2|\n",
      " |      |   mean|               3.5|\n",
      " |      | stddev|2.1213203435596424|\n",
      " |      |    min|                 2|\n",
      " |      |    max|                 5|\n",
      " |      +-------+------------------+\n",
      " |      >>> df.describe().show()\n",
      " |      +-------+------------------+-----+\n",
      " |      |summary|               age| name|\n",
      " |      +-------+------------------+-----+\n",
      " |      |  count|                 2|    2|\n",
      " |      |   mean|               3.5| null|\n",
      " |      | stddev|2.1213203435596424| null|\n",
      " |      |    min|                 2|Alice|\n",
      " |      |    max|                 5|  Bob|\n",
      " |      +-------+------------------+-----+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.summary\n",
      " |  \n",
      " |  distinct(self)\n",
      " |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.distinct().count()\n",
      " |      2\n",
      " |  \n",
      " |  drop(self, *cols)\n",
      " |      Returns a new :class:`DataFrame` that drops the specified column.\n",
      " |      This is a no-op if schema doesn't contain the given column name(s).\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols: str or :class:`Column`\n",
      " |          a name of the column, or the :class:`Column` to drop\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.drop('age').collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.drop(df.age).collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      " |      [Row(age=5, height=85, name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      " |      [Row(age=5, name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      " |      [Row(name='Bob')]\n",
      " |  \n",
      " |  dropDuplicates(self, subset=None)\n",
      " |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      " |      optionally only considering certain columns.\n",
      " |      \n",
      " |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      " |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      " |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      " |      be and system will accordingly limit the state. In addition, too late data older than\n",
      " |      watermark will be dropped to avoid any possibility of duplicates.\n",
      " |      \n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = sc.parallelize([ \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      " |      >>> df.dropDuplicates().show()\n",
      " |      +-----+---+------+\n",
      " |      | name|age|height|\n",
      " |      +-----+---+------+\n",
      " |      |Alice|  5|    80|\n",
      " |      |Alice| 10|    80|\n",
      " |      +-----+---+------+\n",
      " |      \n",
      " |      >>> df.dropDuplicates(['name', 'height']).show()\n",
      " |      +-----+---+------+\n",
      " |      | name|age|height|\n",
      " |      +-----+---+------+\n",
      " |      |Alice|  5|    80|\n",
      " |      +-----+---+------+\n",
      " |  \n",
      " |  drop_duplicates = dropDuplicates(self, subset=None)\n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  dropna(self, how='any', thresh=None, subset=None)\n",
      " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      how : str, optional\n",
      " |          'any' or 'all'.\n",
      " |          If 'any', drop a row if it contains any nulls.\n",
      " |          If 'all', drop a row only if all its values are null.\n",
      " |      thresh: int, optional\n",
      " |          default None\n",
      " |          If specified, drop rows that have less than `thresh` non-null values.\n",
      " |          This overwrites the `how` parameter.\n",
      " |      subset : str, tuple or list, optional\n",
      " |          optional list of column names to consider.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.drop().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |  \n",
      " |  exceptAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      " |      not in another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT ALL` in SQL.\n",
      " |      As standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame(\n",
      " |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.exceptAll(df2).show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  a|  2|\n",
      " |      |  c|  4|\n",
      " |      +---+---+\n",
      " |  \n",
      " |  explain(self, extended=None, mode=None)\n",
      " |      Prints the (logical and physical) plans to the console for debugging purpose.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      parameters\n",
      " |      ----------\n",
      " |      extended : bool, optional\n",
      " |          default ``False``. If ``False``, prints only the physical plan.\n",
      " |          When this is a string without specifying the ``mode``, it works as the mode is\n",
      " |          specified.\n",
      " |      mode : str, optional\n",
      " |          specifies the expected output format of plans.\n",
      " |      \n",
      " |          * ``simple``: Print only a physical plan.\n",
      " |          * ``extended``: Print both logical and physical plans.\n",
      " |          * ``codegen``: Print a physical plan and generated codes if they are available.\n",
      " |          * ``cost``: Print a logical plan and statistics if they are available.\n",
      " |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n",
      " |      \n",
      " |          .. versionchanged:: 3.0.0\n",
      " |             Added optional argument `mode` to specify the expected output format of plans.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.explain()\n",
      " |      == Physical Plan ==\n",
      " |      *(1) Scan ExistingRDD[age#0,name#1]\n",
      " |      \n",
      " |      >>> df.explain(True)\n",
      " |      == Parsed Logical Plan ==\n",
      " |      ...\n",
      " |      == Analyzed Logical Plan ==\n",
      " |      ...\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...\n",
      " |      == Physical Plan ==\n",
      " |      ...\n",
      " |      \n",
      " |      >>> df.explain(mode=\"formatted\")\n",
      " |      == Physical Plan ==\n",
      " |      * Scan ExistingRDD (1)\n",
      " |      (1) Scan ExistingRDD [codegen id : 1]\n",
      " |      Output [2]: [age#0, name#1]\n",
      " |      ...\n",
      " |      \n",
      " |      >>> df.explain(\"cost\")\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...Statistics...\n",
      " |      ...\n",
      " |  \n",
      " |  fillna(self, value, subset=None)\n",
      " |      Replace null values, alias for ``na.fill()``.\n",
      " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : int, float, string, bool or dict\n",
      " |          Value to replace null values with.\n",
      " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      " |          from column name (string) to replacement value. The replacement value must be\n",
      " |          an int, float, boolean, or string.\n",
      " |      subset : str, tuple or list, optional\n",
      " |          optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.fill(50).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      |  5|    50|  Bob|\n",
      " |      | 50|    50|  Tom|\n",
      " |      | 50|    50| null|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df5.na.fill(False).show()\n",
      " |      +----+-------+-----+\n",
      " |      | age|   name|  spy|\n",
      " |      +----+-------+-----+\n",
      " |      |  10|  Alice|false|\n",
      " |      |   5|    Bob|false|\n",
      " |      |null|Mallory| true|\n",
      " |      +----+-------+-----+\n",
      " |      \n",
      " |      >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      " |      +---+------+-------+\n",
      " |      |age|height|   name|\n",
      " |      +---+------+-------+\n",
      " |      | 10|    80|  Alice|\n",
      " |      |  5|  null|    Bob|\n",
      " |      | 50|  null|    Tom|\n",
      " |      | 50|  null|unknown|\n",
      " |      +---+------+-------+\n",
      " |  \n",
      " |  filter(self, condition)\n",
      " |      Filters rows using the given condition.\n",
      " |      \n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      condition : :class:`Column` or str\n",
      " |          a :class:`Column` of :class:`types.BooleanType`\n",
      " |          or a string of SQL expression.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.age > 3).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(df.age == 2).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      >>> df.filter(\"age > 3\").collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(\"age = 2\").collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  first(self)\n",
      " |      Returns the first row as a :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.first()\n",
      " |      Row(age=2, name='Alice')\n",
      " |  \n",
      " |  foreach(self, f)\n",
      " |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a shorthand for ``df.rdd.foreach()``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def f(person):\n",
      " |      ...     print(person.name)\n",
      " |      >>> df.foreach(f)\n",
      " |  \n",
      " |  foreachPartition(self, f)\n",
      " |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def f(people):\n",
      " |      ...     for person in people:\n",
      " |      ...         print(person.name)\n",
      " |      >>> df.foreachPartition(f)\n",
      " |  \n",
      " |  freqItems(self, cols, support=None)\n",
      " |      Finding frequent items for columns, possibly with false positives. Using the\n",
      " |      frequent element count algorithm described in\n",
      " |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      " |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : list or tuple\n",
      " |          Names of the columns to calculate frequent items for as a list or tuple of\n",
      " |          strings.\n",
      " |      support : float, optional\n",
      " |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      " |          The support must be greater than 1e-4.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |  \n",
      " |  groupBy(self, *cols)\n",
      " |      Groups the :class:`DataFrame` using the specified columns,\n",
      " |      so we can run aggregation on them. See :class:`GroupedData`\n",
      " |      for all the available aggregate functions.\n",
      " |      \n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : list, str or :class:`Column`\n",
      " |          columns to group by.\n",
      " |          Each element should be a column name (string) or an expression (:class:`Column`).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.groupBy().avg().collect()\n",
      " |      [Row(avg(age)=3.5)]\n",
      " |      >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(df.name).avg().collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      " |      [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      " |  \n",
      " |  groupby = groupBy(self, *cols)\n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  head(self, n=None)\n",
      " |      Returns the first ``n`` rows.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, optional\n",
      " |          default 1. Number of rows to return.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      If n is greater than 1, return a list of :class:`Row`.\n",
      " |      If n is 1, return a single Row.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.head()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      >>> df.head(1)\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  hint(self, name, *parameters)\n",
      " |      Specifies some hint on the current :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          A name of the hint.\n",
      " |      parameters : str, list, float or int\n",
      " |          Optional parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\n",
      " |      +----+---+------+\n",
      " |      |name|age|height|\n",
      " |      +----+---+------+\n",
      " |      | Bob|  5|    85|\n",
      " |      +----+---+------+\n",
      " |  \n",
      " |  inputFiles(self)\n",
      " |      Returns a best-effort snapshot of the files that compose this :class:`DataFrame`.\n",
      " |      This method simply asks each constituent BaseRelation for its respective files and\n",
      " |      takes the union of all results. Depending on the source relations, this may not find\n",
      " |      all input files. Duplicates are removed.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
      " |      >>> len(df.inputFiles())\n",
      " |      1\n",
      " |  \n",
      " |  intersect(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows only in\n",
      " |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  intersectAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
      " |      and another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT ALL` in SQL. As standard in SQL, this function\n",
      " |      resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  b|  3|\n",
      " |      +---+---+\n",
      " |  \n",
      " |  isLocal(self)\n",
      " |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      " |      (without any Spark executors).\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  join(self, other, on=None, how=None)\n",
      " |      Joins with another :class:`DataFrame`, using the given join expression.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Right side of the join\n",
      " |      on : str, list or :class:`Column`, optional\n",
      " |          a string for the join column name, a list of column names,\n",
      " |          a join expression (Column), or a list of Columns.\n",
      " |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      " |          the column(s) must exist on both sides, and this performs an equi-join.\n",
      " |      how : str, optional\n",
      " |          default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      " |          ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      " |          ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      " |          ``anti``, ``leftanti`` and ``left_anti``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      The following performs a full outer join between ``df1`` and ``df2``.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import desc\n",
      " |      >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height)                 .sort(desc(\"name\")).collect()\n",
      " |      [Row(name='Bob', height=85), Row(name='Alice', height=None), Row(name=None, height=80)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      " |      \n",
      " |      >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      " |      >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      " |      [Row(name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      " |      [Row(name='Bob', age=5)]\n",
      " |  \n",
      " |  limit(self, num)\n",
      " |      Limits the result count to the number specified.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.limit(1).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.limit(0).collect()\n",
      " |      []\n",
      " |  \n",
      " |  localCheckpoint(self, eager=True)\n",
      " |      Returns a locally checkpointed version of this Dataset. Checkpointing can be used to\n",
      " |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      " |      iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
      " |      stored in the executors using the caching subsystem and therefore they are not reliable.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eager : bool, optional\n",
      " |          Whether to checkpoint this :class:`DataFrame` immediately\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental.\n",
      " |  \n",
      " |  orderBy = sort(self, *cols, **kwargs)\n",
      " |  \n",
      " |  persist(self, storageLevel=StorageLevel(True, True, False, True, 1))\n",
      " |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      " |      operations after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
      " |  \n",
      " |  printSchema(self)\n",
      " |      Prints out the schema in the tree format.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.printSchema()\n",
      " |      root\n",
      " |       |-- age: integer (nullable = true)\n",
      " |       |-- name: string (nullable = true)\n",
      " |      <BLANKLINE>\n",
      " |  \n",
      " |  randomSplit(self, weights, seed=None)\n",
      " |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      weights : list\n",
      " |          list of doubles as weights with which to split the :class:`DataFrame`.\n",
      " |          Weights will be normalized if they don't sum up to 1.0.\n",
      " |      seed : int, optional\n",
      " |          The seed for sampling.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> splits = df4.randomSplit([1.0, 2.0], 24)\n",
      " |      >>> splits[0].count()\n",
      " |      2\n",
      " |      \n",
      " |      >>> splits[1].count()\n",
      " |      2\n",
      " |  \n",
      " |  registerTempTable(self, name)\n",
      " |      Registers this DataFrame as a temporary table using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. deprecated:: 2.0.0\n",
      " |          Use :meth:`DataFrame.createOrReplaceTempView` instead.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.registerTempTable(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |  \n",
      " |  repartition(self, numPartitions, *cols)\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is hash partitioned.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      cols : str or :class:`Column`\n",
      " |          partitioning columns.\n",
      " |      \n",
      " |          .. versionchanged:: 1.6\n",
      " |             Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      " |             optional if partitioning columns are specified.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.repartition(10).rdd.getNumPartitions()\n",
      " |      10\n",
      " |      >>> data = df.union(df).repartition(\"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      >>> data = data.repartition(7, \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> data.rdd.getNumPartitions()\n",
      " |      7\n",
      " |      >>> data = data.repartition(\"name\", \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |  \n",
      " |  repartitionByRange(self, numPartitions, *cols)\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is range partitioned.\n",
      " |      \n",
      " |      At least one partition-by expression must be specified.\n",
      " |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      cols : str or :class:`Column`\n",
      " |          partitioning columns.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Due to performance reasons this method uses sampling to estimate the ranges.\n",
      " |      Hence, the output may not be consistent, since sampling can return different values.\n",
      " |      The sample size can be controlled by the config\n",
      " |      `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      " |      2\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.repartitionByRange(1, \"age\").rdd.getNumPartitions()\n",
      " |      1\n",
      " |      >>> data = df.repartitionByRange(\"age\")\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |  \n",
      " |  replace(self, to_replace, value=<no value>, subset=None)\n",
      " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      " |      aliases of each other.\n",
      " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      " |      or strings. Value can have None. When replacing, the new value will be cast\n",
      " |      to the type of the existing column.\n",
      " |      For numeric replacements all values to be replaced should have unique\n",
      " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      " |      and arbitrary replacement will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      to_replace : bool, int, float, string, list or dict\n",
      " |          Value to be replaced.\n",
      " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      " |          must be a mapping between a value and a replacement.\n",
      " |      value : bool, int, float, string or None, optional\n",
      " |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      " |          list, `value` should be of the same length and type as `to_replace`.\n",
      " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      " |          used as a replacement for each item in `to_replace`.\n",
      " |      subset : list, optional\n",
      " |          optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df4.na.replace(10, 20).show()\n",
      " |      +----+------+-----+\n",
      " |      | age|height| name|\n",
      " |      +----+------+-----+\n",
      " |      |  20|    80|Alice|\n",
      " |      |   5|  null|  Bob|\n",
      " |      |null|  null|  Tom|\n",
      " |      |null|  null| null|\n",
      " |      +----+------+-----+\n",
      " |      \n",
      " |      >>> df4.na.replace('Alice', None).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace({'Alice': None}).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|   A|\n",
      " |      |   5|  null|   B|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |  \n",
      " |  rollup(self, *cols)\n",
      " |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregation on them.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |  \n",
      " |  sameSemantics(self, other)\n",
      " |      Returns `True` when the logical query plans inside both :class:`DataFrame`\\s are equal and\n",
      " |      therefore return same results.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The equality comparison here is simplified by tolerating the cosmetic differences\n",
      " |      such as attribute names.\n",
      " |      \n",
      " |      This API can compare both :class:`DataFrame`\\s very fast but can still return\n",
      " |      `False` on the :class:`DataFrame` that return the same results, for instance, from\n",
      " |      different plans. Such false negative semantic can be useful when caching as an example.\n",
      " |      \n",
      " |      This API is a developer API.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.range(10)\n",
      " |      >>> df2 = spark.range(10)\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\n",
      " |      True\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\n",
      " |      False\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n",
      " |      True\n",
      " |  \n",
      " |  sample(self, withReplacement=None, fraction=None, seed=None)\n",
      " |      Returns a sampled subset of this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      withReplacement : bool, optional\n",
      " |          Sample with replacement or not (default ``False``).\n",
      " |      fraction : float, optional\n",
      " |          Fraction of rows to generate, range [0.0, 1.0].\n",
      " |      seed : int, optional\n",
      " |          Seed for sampling (default a random seed).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |      count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.sample(0.5, 3).count()\n",
      " |      7\n",
      " |      >>> df.sample(fraction=0.5, seed=3).count()\n",
      " |      7\n",
      " |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n",
      " |      1\n",
      " |      >>> df.sample(1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(fraction=1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(False, fraction=1.0).count()\n",
      " |      10\n",
      " |  \n",
      " |  sampleBy(self, col, fractions, seed=None)\n",
      " |      Returns a stratified sample without replacement based on the\n",
      " |      fraction given on each stratum.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col : :class:`Column` or str\n",
      " |          column that defines strata\n",
      " |      \n",
      " |          .. versionchanged:: 3.0\n",
      " |             Added sampling by a column of :class:`Column`\n",
      " |      fractions : dict\n",
      " |          sampling fraction for each stratum. If a stratum is not\n",
      " |          specified, we treat its fraction as zero.\n",
      " |      seed : int, optional\n",
      " |          random seed\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a new :class:`DataFrame` that represents the stratified sample\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      " |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      " |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      " |      +---+-----+\n",
      " |      |key|count|\n",
      " |      +---+-----+\n",
      " |      |  0|    3|\n",
      " |      |  1|    6|\n",
      " |      +---+-----+\n",
      " |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      " |      33\n",
      " |  \n",
      " |  select(self, *cols)\n",
      " |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, :class:`Column`, or list\n",
      " |          column names (string) or expressions (:class:`Column`).\n",
      " |          If one of the column names is '*', that column is expanded to include all columns\n",
      " |          in the current :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select('*').collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.select('name', 'age').collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df.select(df.name, (df.age + 10).alias('age')).collect()\n",
      " |      [Row(name='Alice', age=12), Row(name='Bob', age=15)]\n",
      " |  \n",
      " |  selectExpr(self, *expr)\n",
      " |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a variant of :func:`select` that accepts SQL expressions.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n",
      " |      [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n",
      " |  \n",
      " |  semanticHash(self)\n",
      " |      Returns a hash code of the logical query plan against this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Unlike the standard hash code, the hash is calculated against the query plan\n",
      " |      simplified by tolerating the cosmetic differences such as attribute names.\n",
      " |      \n",
      " |      This API is a developer API.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.range(10).selectExpr(\"id as col0\").semanticHash()  # doctest: +SKIP\n",
      " |      1855039936\n",
      " |      >>> spark.range(10).selectExpr(\"id as col1\").semanticHash()  # doctest: +SKIP\n",
      " |      1855039936\n",
      " |  \n",
      " |  show(self, n=20, truncate=True, vertical=False)\n",
      " |      Prints the first ``n`` rows to the console.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, optional\n",
      " |          Number of rows to show.\n",
      " |      truncate : bool, optional\n",
      " |          If set to ``True``, truncate strings longer than 20 chars by default.\n",
      " |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
      " |          and align cells right.\n",
      " |      vertical : bool, optional\n",
      " |          If set to ``True``, print output rows vertically (one line\n",
      " |          per column value).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df\n",
      " |      DataFrame[age: int, name: string]\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.show(truncate=3)\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  2| Ali|\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |      >>> df.show(vertical=True)\n",
      " |      -RECORD 0-----\n",
      " |       age  | 2\n",
      " |       name | Alice\n",
      " |      -RECORD 1-----\n",
      " |       age  | 5\n",
      " |       name | Bob\n",
      " |  \n",
      " |  sort(self, *cols, **kwargs)\n",
      " |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, list, or :class:`Column`, optional\n",
      " |           list of :class:`Column` or column names to sort by.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      ascending : bool or list, optional\n",
      " |          boolean or list of boolean (default ``True``).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.sort(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.sort(\"age\", ascending=False).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df.sort(asc(\"age\")).collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  sortWithinPartitions(self, *cols, **kwargs)\n",
      " |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, list or :class:`Column`, optional\n",
      " |          list of :class:`Column` or column names to sort by.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      ascending : bool or list, optional\n",
      " |          boolean or list of boolean (default ``True``).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |  \n",
      " |  subtract(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
      " |      but not in another :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  summary(self, *statistics)\n",
      " |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
      " |      - count\n",
      " |      - mean\n",
      " |      - stddev\n",
      " |      - min\n",
      " |      - max\n",
      " |      - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
      " |      \n",
      " |      If no statistics are given, this function computes count, mean, stddev, min,\n",
      " |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.summary().show()\n",
      " |      +-------+------------------+-----+\n",
      " |      |summary|               age| name|\n",
      " |      +-------+------------------+-----+\n",
      " |      |  count|                 2|    2|\n",
      " |      |   mean|               3.5| null|\n",
      " |      | stddev|2.1213203435596424| null|\n",
      " |      |    min|                 2|Alice|\n",
      " |      |    25%|                 2| null|\n",
      " |      |    50%|                 2| null|\n",
      " |      |    75%|                 5| null|\n",
      " |      |    max|                 5|  Bob|\n",
      " |      +-------+------------------+-----+\n",
      " |      \n",
      " |      >>> df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      " |      +-------+---+-----+\n",
      " |      |summary|age| name|\n",
      " |      +-------+---+-----+\n",
      " |      |  count|  2|    2|\n",
      " |      |    min|  2|Alice|\n",
      " |      |    25%|  2| null|\n",
      " |      |    75%|  5| null|\n",
      " |      |    max|  5|  Bob|\n",
      " |      +-------+---+-----+\n",
      " |      \n",
      " |      To do a summary for specific columns first select them:\n",
      " |      \n",
      " |      >>> df.select(\"age\", \"name\").summary(\"count\").show()\n",
      " |      +-------+---+----+\n",
      " |      |summary|age|name|\n",
      " |      +-------+---+----+\n",
      " |      |  count|  2|   2|\n",
      " |      +-------+---+----+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.display\n",
      " |  \n",
      " |  tail(self, num)\n",
      " |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      Running tail requires moving data into the application's driver process, and doing so with\n",
      " |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.tail(1)\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  take(self, num)\n",
      " |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.take(2)\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  toDF(self, *cols)\n",
      " |      Returns a new :class:`DataFrame` that with new specified column names\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str\n",
      " |          new column names\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.toDF('f1', 'f2').collect()\n",
      " |      [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\n",
      " |  \n",
      " |  toJSON(self, use_unicode=True)\n",
      " |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      " |      \n",
      " |      Each row is turned into a JSON document as one element in the returned RDD.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.toJSON().first()\n",
      " |      '{\"age\":2,\"name\":\"Alice\"}'\n",
      " |  \n",
      " |  toLocalIterator(self, prefetchPartitions=False)\n",
      " |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      " |      The iterator will consume as much memory as the largest partition in this\n",
      " |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
      " |      partitions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prefetchPartitions : bool, optional\n",
      " |          If Spark should pre-fetch the next partition  before it is needed.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> list(df.toLocalIterator())\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  transform(self, func)\n",
      " |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          a function that takes and returns a :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
      " |      >>> def cast_all_to_int(input_df):\n",
      " |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
      " |      >>> def sort_columns_asc(input_df):\n",
      " |      ...     return input_df.select(*sorted(input_df.columns))\n",
      " |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
      " |      +-----+---+\n",
      " |      |float|int|\n",
      " |      +-----+---+\n",
      " |      |    1|  1|\n",
      " |      |    2|  2|\n",
      " |      +-----+---+\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  unionAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  unionByName(self, other, allowMissingColumns=False)\n",
      " |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n",
      " |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      The difference between this function and :func:`union` is that this function\n",
      " |      resolves columns by name (not by position):\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      " |      >>> df1.unionByName(df2).show()\n",
      " |      +----+----+----+\n",
      " |      |col0|col1|col2|\n",
      " |      +----+----+----+\n",
      " |      |   1|   2|   3|\n",
      " |      |   6|   4|   5|\n",
      " |      +----+----+----+\n",
      " |      \n",
      " |      When the parameter `allowMissingColumns` is ``True``, the set of column names\n",
      " |      in this and other :class:`DataFrame` can differ; missing columns will be filled with null.\n",
      " |      Further, the missing columns of this :class:`DataFrame` will be added at the end\n",
      " |      in the schema of the union result:\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
      " |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
      " |      +----+----+----+----+\n",
      " |      |col0|col1|col2|col3|\n",
      " |      +----+----+----+----+\n",
      " |      |   1|   2|   3|null|\n",
      " |      |null|   4|   5|   6|\n",
      " |      +----+----+----+----+\n",
      " |      \n",
      " |      .. versionchanged:: 3.1.0\n",
      " |         Added optional argument `allowMissingColumns` to specify whether to allow\n",
      " |         missing columns.\n",
      " |  \n",
      " |  unpersist(self, blocking=False)\n",
      " |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
      " |  \n",
      " |  where = filter(self, condition)\n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withColumn(self, colName, col)\n",
      " |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      " |      existing column that has the same name.\n",
      " |      \n",
      " |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      " |      a column from some other :class:`DataFrame` will raise an error.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colName : str\n",
      " |          string, name of the new column.\n",
      " |      col : :class:`Column`\n",
      " |          a :class:`Column` expression for the new column.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method introduces a projection internally. Therefore, calling it multiple\n",
      " |      times, for instance, via loops in order to add multiple columns can generate big\n",
      " |      plans which can cause performance issues and even `StackOverflowException`.\n",
      " |      To avoid this, use :func:`select` with the multiple columns at once.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.withColumn('age2', df.age + 2).collect()\n",
      " |      [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
      " |  \n",
      " |  withColumnRenamed(self, existing, new)\n",
      " |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
      " |      This is a no-op if schema doesn't contain the given column name.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      existing : str\n",
      " |          string, name of the existing column to rename.\n",
      " |      new : str\n",
      " |          string, new name of the column.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.withColumnRenamed('age', 'age2').collect()\n",
      " |      [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n",
      " |  \n",
      " |  withWatermark(self, eventTime, delayThreshold)\n",
      " |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      " |      in time before which we assume no more late data is going to arrive.\n",
      " |      \n",
      " |      Spark will use this watermark for several purposes:\n",
      " |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      " |          when using output modes that do not allow updates.\n",
      " |      \n",
      " |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      " |      \n",
      " |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      " |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      " |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      " |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      " |      process records that arrive more than `delayThreshold` late.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eventTime : str\n",
      " |          the name of the column that contains the event time of the row.\n",
      " |      delayThreshold : str\n",
      " |          the minimum delay to wait to data to arrive late, relative to the\n",
      " |          latest record that has been processed in the form of an interval\n",
      " |          (e.g. \"1 minute\" or \"5 hours\").\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import timestamp_seconds\n",
      " |      >>> sdf.select(\n",
      " |      ...    'name',\n",
      " |      ...    timestamp_seconds(sdf.time).alias('time')).withWatermark('time', '10 minutes')\n",
      " |      DataFrame[name: string, time: timestamp]\n",
      " |  \n",
      " |  writeTo(self, table)\n",
      " |      Create a write configuration builder for v2 sources.\n",
      " |      \n",
      " |      This builder is used to configure and execute write operations.\n",
      " |      \n",
      " |      For example, to append or create or replace existing tables.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n",
      " |      >>> df.writeTo(                              # doctest: +SKIP\n",
      " |      ...     \"catalog.db.table\"\n",
      " |      ... ).partitionedBy(\"col\").createOrReplace()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  columns\n",
      " |      Returns all column names as a list.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.columns\n",
      " |      ['age', 'name']\n",
      " |  \n",
      " |  dtypes\n",
      " |      Returns all column names and their data types as a list.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'int'), ('name', 'string')]\n",
      " |  \n",
      " |  isStreaming\n",
      " |      Returns ``True`` if this :class:`Dataset` contains one or more sources that continuously\n",
      " |      return data as it arrives. A :class:`Dataset` that reads data from a streaming source\n",
      " |      must be executed as a :class:`StreamingQuery` using the :func:`start` method in\n",
      " |      :class:`DataStreamWriter`.  Methods that return a single answer, (e.g., :func:`count` or\n",
      " |      :func:`collect`) will throw an :class:`AnalysisException` when there is a streaming\n",
      " |      source present.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |  \n",
      " |  na\n",
      " |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  rdd\n",
      " |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  schema\n",
      " |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.schema\n",
      " |      StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n",
      " |  \n",
      " |  stat\n",
      " |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  storageLevel\n",
      " |      Get the :class:`DataFrame`'s current storage level.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.storageLevel\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> df.cache().storageLevel\n",
      " |      StorageLevel(True, True, False, True, 1)\n",
      " |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
      " |      StorageLevel(True, False, False, False, 2)\n",
      " |  \n",
      " |  write\n",
      " |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrameWriter`\n",
      " |  \n",
      " |  writeStream\n",
      " |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataStreamWriter`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |  \n",
      " |  mapInPandas(self, func, schema)\n",
      " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      " |      function that takes and outputs a pandas DataFrame, and returns the result as a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The function should take an iterator of `pandas.DataFrame`\\s and return\n",
      " |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n",
      " |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n",
      " |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n",
      " |      Each `pandas.DataFrame` size can be controlled by\n",
      " |      `spark.sql.execution.arrow.maxRecordsPerBatch`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n",
      " |          outputs an iterator of `pandas.DataFrame`\\s.\n",
      " |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      " |          the return type of the `func` in PySpark. The value can be either a\n",
      " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import pandas_udf\n",
      " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      " |      >>> def filter_func(iterator):\n",
      " |      ...     for pdf in iterator:\n",
      " |      ...         yield pdf[pdf.id == 1]\n",
      " |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
      " |      +---+---+\n",
      " |      | id|age|\n",
      " |      +---+---+\n",
      " |      |  1| 21|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.pandas_udf\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n",
      " |  \n",
      " |  toPandas(self)\n",
      " |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      " |      \n",
      " |      This is only available if Pandas is installed and available.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting Pandas's :class:`DataFrame` is\n",
      " |      expected to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.toPandas()  # doctest: +SKIP\n",
      " |         age   name\n",
      " |      0    2  Alice\n",
      " |      1    5    Bob\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2da40dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_order_details = customers.join(\n",
    "    orders,\n",
    "    on=customers['customer_id'] == orders['order_customer_id'],\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "136efa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eb411dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_order_details.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "43b62131",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_order_details_left = customers.join(\n",
    "    orders,\n",
    "    on=customers['customer_id'] == orders['order_customer_id'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a54a386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68913"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_order_details_left.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0a42298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_order_details_left = customers.join(\n",
    "    orders,\n",
    "    on= customers['customer_id']==orders['order_customer_id'],\n",
    "    how='left_outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d474ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68913"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_order_details_left.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "12b6c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_order_details_right = orders.join(\n",
    "    customers,\n",
    "    on = orders['order_customer_id'] == customers['customer_id'],\n",
    "    how='right'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "86afc58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68913"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_order_details_right.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "49570f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_order_details_right = orders.join(\n",
    "    customers,\n",
    "    on = orders['order_customer_id'] == customers['customer_id'],\n",
    "    how='right_outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3054877e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68913"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_order_details_right.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1c5fb9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+--------------------+--------+---------------+\n",
      "|customer_id|customer_email|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------+--------------+-----------------+--------------------+--------+---------------+\n",
      "|          1|     XXXXXXXXX|                1|2013-12-13 00:00:...|   22945|       COMPLETE|\n",
      "|          2|     XXXXXXXXX|                2|2013-11-30 00:00:...|   67863|       COMPLETE|\n",
      "|          2|     XXXXXXXXX|                2|2013-08-02 00:00:...|   57963|        ON_HOLD|\n",
      "|          2|     XXXXXXXXX|                2|2014-02-18 00:00:...|   33865|       COMPLETE|\n",
      "|          2|     XXXXXXXXX|                2|2013-10-29 00:00:...|   15192|PENDING_PAYMENT|\n",
      "+-----------+--------------+-----------------+--------------------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_order_details_left. \\\n",
    "    select(customers.customer_id, customers.customer_email, orders[\"*\"]). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "76dc2a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "feaeba20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12435"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d70bfe78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_order_details.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2855ba12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68913"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_order_details_left.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "484ba05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_order_details_left. \\\n",
    "    filter(orders.order_id.isNull()). \\\n",
    "    select(customers.customer_id, customers.customer_email, orders['*']). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fa005764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+----------+--------+------------+\n",
      "|customer_id|customer_email|order_customer_id|order_date|order_id|order_status|\n",
      "+-----------+--------------+-----------------+----------+--------+------------+\n",
      "|        219|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|        339|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|        469|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       1187|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       1481|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       1808|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       2073|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       2096|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       2450|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       4555|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       4927|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       6072|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       6613|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       7011|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       7552|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       8243|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       8343|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       8575|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       8778|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       8882|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       9060|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       9315|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|      10060|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|      10330|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|      10439|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|      10913|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|      10958|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|      12175|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|      12190|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|      12392|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "+-----------+--------------+-----------------+----------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_order_details_left. \\\n",
    "    filter(orders.order_id.isNull()). \\\n",
    "    select(customers.customer_id, customers.customer_email, orders['*']). \\\n",
    "    show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "73d60c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+----------+--------+------------+\n",
      "|customer_id|customer_email|order_customer_id|order_date|order_id|order_status|\n",
      "+-----------+--------------+-----------------+----------+--------+------------+\n",
      "|        219|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|        339|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|        469|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       1187|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       1481|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       1808|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       2073|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       2096|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       2450|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       4555|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       4927|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       6072|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       6613|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       7011|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       7552|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       8243|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       8343|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       8575|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       8778|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "|       8882|     XXXXXXXXX|             null|      null|    null|        null|\n",
      "+-----------+--------------+-----------------+----------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers.alias('c'). \\\n",
    "    join(\n",
    "        orders.alias('o'),\n",
    "        on= customers.customer_id == orders.order_customer_id,\n",
    "        how = 'left'\n",
    "    ). \\\n",
    "    filter('o.order_id IS NULL'). \\\n",
    "    selectExpr('c.customer_id', 'c.customer_email', 'o.*'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bc32321f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers.alias('c'). \\\n",
    "    join(\n",
    "        orders.alias('o'),\n",
    "        on= customers.customer_id == orders.order_customer_id,\n",
    "        how = 'left'\n",
    "    ). \\\n",
    "    filter('o.order_id IS NULL'). \\\n",
    "    selectExpr('c.customer_id', 'c.customer_email', 'o.*'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "daff48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.json(\"/public/retail_db_json/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d9c86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = spark.read.json(\"/public/retail_db_json/order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e2f34f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filtered = orders.filter(\"order_date LIKE '2013%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "036e396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_order_details_left = customers.alias('c'). \\\n",
    "    join(\n",
    "        orders_filtered.alias('o'),\n",
    "        on = customers['customer_id'] == orders_filtered['order_customer_id'],\n",
    "        how='left_outer'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "620f0b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30662"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "10fb90d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31746"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_order_details_left.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6cdc8fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- customer_fname: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_lname: string (nullable = true)\n",
      " |-- customer_password: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- customer_street: string (nullable = true)\n",
      " |-- customer_zipcode: string (nullable = true)\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_order_details_left.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9048d807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|order_count|\n",
      "+-----------+-----------+\n",
      "|         38|          1|\n",
      "|         83|          1|\n",
      "|         46|          1|\n",
      "|          5|          1|\n",
      "|         55|          1|\n",
      "+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_order_details_left. \\\n",
    "    groupBy(\"customer_id\"). \\\n",
    "    agg( count(lit(1)).alias('order_count')). \\\n",
    "    orderBy(col('order_count').asc()). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "201f9869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12435"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_order_details_left. \\\n",
    "    groupBy(\"customer_id\"). \\\n",
    "    agg( count(lit(1)).alias('order_count')). \\\n",
    "    orderBy(col('order_count').asc()). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bc5c9d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+\n",
      "|customer_id|order_count|order_count2|\n",
      "+-----------+-----------+------------+\n",
      "|         83|          1|           1|\n",
      "|          5|          1|           1|\n",
      "|         26|          1|           1|\n",
      "|         28|          1|           0|\n",
      "|         29|          1|           1|\n",
      "|         30|          1|           1|\n",
      "|         38|          1|           1|\n",
      "|         46|          1|           1|\n",
      "|         55|          1|           0|\n",
      "|         57|          1|           1|\n",
      "|         81|          1|           1|\n",
      "|         84|          1|           1|\n",
      "|         91|          1|           0|\n",
      "|        104|          1|           1|\n",
      "|        115|          1|           1|\n",
      "|        128|          1|           1|\n",
      "|        130|          1|           1|\n",
      "|        136|          1|           1|\n",
      "|        140|          1|           1|\n",
      "|        151|          1|           1|\n",
      "+-----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,min,max,when\n",
    "\n",
    "customer_order_details_left. \\\n",
    "    groupBy(\"customer_id\"). \\\n",
    "    agg( \n",
    "        count(lit(1)).alias('order_count'),\n",
    "        sum((when(col('order_id').isNotNull(),1).otherwise(0))).alias('order_count2')\n",
    "    ). \\\n",
    "    orderBy(col('order_count').asc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bbf76440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+\n",
      "|customer_id|order_count|order_count2|\n",
      "+-----------+-----------+------------+\n",
      "|         28|          1|           0|\n",
      "|         55|          1|           0|\n",
      "|         91|          1|           0|\n",
      "|        186|          1|           0|\n",
      "|        200|          1|           0|\n",
      "|        219|          1|           0|\n",
      "|        301|          1|           0|\n",
      "|        360|          1|           0|\n",
      "|        399|          1|           0|\n",
      "|        456|          1|           0|\n",
      "|        476|          1|           0|\n",
      "|        519|          1|           0|\n",
      "|        540|          1|           0|\n",
      "|        574|          1|           0|\n",
      "|        588|          1|           0|\n",
      "|        631|          1|           0|\n",
      "|        652|          1|           0|\n",
      "|        657|          1|           0|\n",
      "|        701|          1|           0|\n",
      "|        769|          1|           0|\n",
      "+-----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_order_details_left. \\\n",
    "    filter(orders.order_id.isNull()). \\\n",
    "    groupBy(\"customer_id\"). \\\n",
    "    agg( \n",
    "        count(lit(1)).alias('order_count'),\n",
    "        sum((when(col('order_id').isNotNull(),1).otherwise(0))).alias('order_count2')\n",
    "    ). \\\n",
    "    orderBy(col('order_count').asc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8061c9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1084"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_order_details_left. \\\n",
    "    filter(orders.order_id.isNull()). \\\n",
    "    groupBy(\"customer_id\"). \\\n",
    "    agg( \n",
    "        count(lit(1)).alias('order_count'),\n",
    "        sum((when(col('order_id').isNotNull(),1).otherwise(0))).alias('order_count2')\n",
    "    ). \\\n",
    "    orderBy(col('order_count').asc()). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2f00654e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1084"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "31746-30662"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b624160b",
   "metadata": {},
   "source": [
    "## Air Traffic tables join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "83a35c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 17 Joining Data Sets'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b80b5e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5adf8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtrafficPath = \"/public/airlines_all/airlines-part/flightmonth=200801\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5d99d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark. \\\n",
    "    read. \\\n",
    "    parquet(airtrafficPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8bc0e6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+------+----+----------+\n",
      "|Year|Month|DayOfMonth|Origin|Dest|CRSDepTime|\n",
      "+----+-----+----------+------+----+----------+\n",
      "|2008|    1|        16|   BGR| CVG|      1735|\n",
      "|2008|    1|        17|   SYR| CVG|      1701|\n",
      "|2008|    1|        17|   SAV| BOS|      1225|\n",
      "|2008|    1|        17|   CVG| GRR|      1530|\n",
      "|2008|    1|        17|   STL| CVG|      1205|\n",
      "|2008|    1|        18|   STL| JFK|      1150|\n",
      "|2008|    1|        18|   MCI| CVG|      1009|\n",
      "|2008|    1|        19|   TUL| CVG|       835|\n",
      "|2008|    1|        20|   JFK| PHL|      1935|\n",
      "|2008|    1|        20|   RDU| CVG|       830|\n",
      "|2008|    1|        21|   CVG| DTW|      1640|\n",
      "|2008|    1|        21|   MSY| LGA|      1204|\n",
      "|2008|    1|        21|   JFK| PHL|      1935|\n",
      "|2008|    1|        21|   DCA| JFK|      1830|\n",
      "|2008|    1|        21|   HSV| DCA|       700|\n",
      "|2008|    1|        22|   ORD| CVG|      1910|\n",
      "|2008|    1|        22|   CVG| JFK|      1320|\n",
      "|2008|    1|        23|   LGA| SAV|       908|\n",
      "|2008|    1|        23|   CLT| CVG|      1252|\n",
      "|2008|    1|        23|   GSP| LGA|       635|\n",
      "+----+-----+----------+------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select(\n",
    "        \"Year\", \"Month\", \"DayOfMonth\",\n",
    "        \"Origin\", \"Dest\", \"CRSDepTime\"\n",
    "    ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5ac11b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605659"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7db2be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodesPath = \"/public/airlines_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9c268f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValidAirportCodes(airportCodesPath):\n",
    "    airportCodes = spark. \\\n",
    "        read. \\\n",
    "        option(\"sep\",\"\\t\"). \\\n",
    "        option(\"header\",True). \\\n",
    "        option(\"inferSchema\", True). \\\n",
    "        csv(airportCodesPath). \\\n",
    "        filter(\"!(State='Hawaii' AND IATA='Big') AND Country='USA'\")\n",
    "    return airportCodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3cef97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodes = getValidAirportCodes(airportCodesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "73152763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8efdab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-------------+-----+-------+----+----------+\n",
      "|Year|Month|DayOfMonth|         City|State|Country|IATA|CRSDepTime|\n",
      "+----+-----+----------+-------------+-----+-------+----+----------+\n",
      "|2008|    1|        16|       Bangor|   ME|    USA| BGR|      1735|\n",
      "|2008|    1|        17|     Syracuse|   NY|    USA| SYR|      1701|\n",
      "|2008|    1|        17|     Savannah|   GA|    USA| SAV|      1225|\n",
      "|2008|    1|        17|   Cincinnati|   OH|    USA| CVG|      1530|\n",
      "|2008|    1|        17|    St. Louis|   MO|    USA| STL|      1205|\n",
      "|2008|    1|        18|    St. Louis|   MO|    USA| STL|      1150|\n",
      "|2008|    1|        18|  Kansas City|   MO|    USA| MCI|      1009|\n",
      "|2008|    1|        19|        Tulsa|   OK|    USA| TUL|       835|\n",
      "|2008|    1|        20|     New York|   NY|    USA| JFK|      1935|\n",
      "|2008|    1|        20|      Raleigh|   NC|    USA| RDU|       830|\n",
      "|2008|    1|        21|   Cincinnati|   OH|    USA| CVG|      1640|\n",
      "|2008|    1|        21|  New Orleans|   LA|    USA| MSY|      1204|\n",
      "|2008|    1|        21|     New York|   NY|    USA| JFK|      1935|\n",
      "|2008|    1|        21|Washington DC| null|    USA| DCA|      1830|\n",
      "|2008|    1|        21|   Huntsville|   AL|    USA| HSV|       700|\n",
      "|2008|    1|        22|      Chicago|   IL|    USA| ORD|      1910|\n",
      "|2008|    1|        22|   Cincinnati|   OH|    USA| CVG|      1320|\n",
      "|2008|    1|        23|     New York|   NY|    USA| LGA|       908|\n",
      "|2008|    1|        23|    Charlotte|   NC|    USA| CLT|      1252|\n",
      "|2008|    1|        23|   Greenville|   SC|    USA| GSP|       635|\n",
      "+----+-----+----------+-------------+-----+-------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA']\n",
    "    ). \\\n",
    "    select(airtraffic['Year'], airtraffic['Month'], airtraffic['DayOfMonth'],airportCodes['*'],airtraffic['CRSDepTime']). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cafd58ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600074"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA']\n",
    "    ). \\\n",
    "    select(airtraffic['Year'], airtraffic['Month'], airtraffic['DayOfMonth'],airportCodes['*'],airtraffic['CRSDepTime']). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c0c22ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|Origin|FlightCount|\n",
      "+------+-----------+\n",
      "|   BGR|        208|\n",
      "|   SYR|       1048|\n",
      "|   CVG|       8659|\n",
      "|   STL|       5329|\n",
      "|   JFK|      10023|\n",
      "|   MSY|       3453|\n",
      "|   DCA|       7304|\n",
      "|   HSV|        901|\n",
      "|   ORD|      29936|\n",
      "|   CLT|      10752|\n",
      "|   GSP|        995|\n",
      "|   BOS|       9717|\n",
      "|   COS|       1445|\n",
      "|   BNA|       4935|\n",
      "|   ATL|      33897|\n",
      "|   SJC|       4976|\n",
      "|   GJT|        372|\n",
      "|   AZO|        359|\n",
      "|   ELP|       1818|\n",
      "|   PDX|       4898|\n",
      "+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on = airtraffic['Origin']==airportCodes['IATA']\n",
    "    ). \\\n",
    "    groupBy(airtraffic['Origin']). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    select(airtraffic['Origin'], col('FlightCount')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b113ed43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|Origin|FlightCount|\n",
      "+------+-----------+\n",
      "|   ATL|      33897|\n",
      "|   ORD|      29936|\n",
      "|   DFW|      23861|\n",
      "|   DEN|      19477|\n",
      "|   LAX|      18945|\n",
      "|   PHX|      17695|\n",
      "|   IAH|      15531|\n",
      "|   LAS|      15292|\n",
      "|   DTW|      14357|\n",
      "|   EWR|      12467|\n",
      "|   SLC|      12401|\n",
      "|   MSP|      11800|\n",
      "|   SFO|      11573|\n",
      "|   MCO|      11070|\n",
      "|   CLT|      10752|\n",
      "|   LGA|      10300|\n",
      "|   JFK|      10023|\n",
      "|   BOS|       9717|\n",
      "|   BWI|       8883|\n",
      "|   CVG|       8659|\n",
      "+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on = airtraffic['Origin']==airportCodes['IATA']\n",
    "    ). \\\n",
    "    groupBy(airtraffic['Origin']). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    select(airtraffic['Origin'], col('FlightCount')). \\\n",
    "    orderBy(col('FlightCount').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d8cb474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 207 Get Flight Count Per US State using Spark Data Frame APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b79a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 17 Joining Data Sets'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac96ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b8d64e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtrafficPath = \"/public/airlines_all/airlines-part/flightmonth=200801\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2850f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtrafficPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad6e6608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+------+----+----------+\n",
      "|Year|Month|DayOfMonth|Origin|Dest|CRSDepTime|\n",
      "+----+-----+----------+------+----+----------+\n",
      "|2008|    1|        16|   BGR| CVG|      1735|\n",
      "|2008|    1|        17|   SYR| CVG|      1701|\n",
      "|2008|    1|        17|   SAV| BOS|      1225|\n",
      "|2008|    1|        17|   CVG| GRR|      1530|\n",
      "|2008|    1|        17|   STL| CVG|      1205|\n",
      "+----+-----+----------+------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select(\n",
    "        \"Year\", \"Month\", \"DayOfMonth\",\n",
    "        \"Origin\",\"Dest\", \"CRSDepTime\"\n",
    "    ). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "706d4f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605659"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53fcd5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodesPath = \"/public/airlines_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "463b37bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValidAirportCodes(airportCodesPath):\n",
    "    airportCodes = spark. \\\n",
    "        read. \\\n",
    "        option(\"sep\",\"\\t\"). \\\n",
    "        option(\"header\",True). \\\n",
    "        option(\"inferSchema\", True). \\\n",
    "        csv(airportCodesPath). \\\n",
    "        filter(\"!(State='Hawaii' AND IATA='Big') AND Country='USA'\")\n",
    "    return airportCodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b013da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodes = getValidAirportCodes(airportCodesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4473ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c68cc197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,lit,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccdb8094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|State|FlightCount|\n",
      "+-----+-----------+\n",
      "|   NY|      28414|\n",
      "|   MO|      11808|\n",
      "|   NC|      17942|\n",
      "|   IL|      39812|\n",
      "|   SC|       3525|\n",
      "|   TN|      13549|\n",
      "|   VA|       4093|\n",
      "|   MI|      17824|\n",
      "|   ID|       2497|\n",
      "|   OR|       6221|\n",
      "|   SD|        844|\n",
      "|   AZ|      20768|\n",
      "|   NE|       2547|\n",
      "|   NM|       3509|\n",
      "|   MN|      12357|\n",
      "|   MD|       8883|\n",
      "|   IA|       2315|\n",
      "|   MS|       2005|\n",
      "|   NJ|      12498|\n",
      "|   CT|       2729|\n",
      "+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA'],\n",
    "        how='inner'\n",
    "    ). \\\n",
    "    groupBy( airportCodes['State']). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c42a1349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|State|FlightCount|\n",
      "+-----+-----------+\n",
      "|   CA|      72853|\n",
      "|   TX|      63930|\n",
      "|   FL|      41042|\n",
      "|   IL|      39812|\n",
      "|   GA|      35527|\n",
      "|   NY|      28414|\n",
      "|   CO|      23288|\n",
      "|   AZ|      20768|\n",
      "|   OH|      19209|\n",
      "|   NC|      17942|\n",
      "|   MI|      17824|\n",
      "|   NV|      17763|\n",
      "| null|      14090|\n",
      "|   TN|      13549|\n",
      "|   PA|      13491|\n",
      "|   UT|      12709|\n",
      "|   NJ|      12498|\n",
      "|   MN|      12357|\n",
      "|   MO|      11808|\n",
      "|   WA|      10210|\n",
      "+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA'],\n",
    "        how='inner'\n",
    "    ). \\\n",
    "    groupBy( airportCodes['State']). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    orderBy(col('FlightCount').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8834e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 208 Solution Get Dormant US Airports using Spark DF APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90fec92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 17 Joining Data Sets'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "629aade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae4c99b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtrafficPath = \"/public/airlines_all/airlines-part/flightmonth=200801\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d56a0845",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtrafficPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3bd5756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+------+----+----------+\n",
      "|Year|Month|DayOfMonth|Origin|Dest|CRSDepTime|\n",
      "+----+-----+----------+------+----+----------+\n",
      "|2008|    1|        16|   BGR| CVG|      1735|\n",
      "|2008|    1|        17|   SYR| CVG|      1701|\n",
      "|2008|    1|        17|   SAV| BOS|      1225|\n",
      "|2008|    1|        17|   CVG| GRR|      1530|\n",
      "|2008|    1|        17|   STL| CVG|      1205|\n",
      "+----+-----+----------+------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select(\n",
    "        \"Year\", \"Month\", \"DayOfMonth\",\n",
    "        \"Origin\",\"Dest\", \"CRSDepTime\"\n",
    "    ). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3ada3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodesPath = \"/public/airlines_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f95c6ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValidAirportCodes(airportCodesPath):\n",
    "    airportCodes = spark. \\\n",
    "        read. \\\n",
    "        option(\"sep\",\"\\t\"). \\\n",
    "        option(\"header\",True). \\\n",
    "        option(\"inferSchema\", True). \\\n",
    "        csv(airportCodesPath). \\\n",
    "        filter(\"!(State='Hawaii' AND IATA='Big') AND Country='USA'\")\n",
    "    return airportCodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b2d5c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodes = getValidAirportCodes(airportCodesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9766496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b412c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65a6a7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-------+----+------+\n",
      "|          City|State|Country|IATA|Origin|\n",
      "+--------------+-----+-------+----+------+\n",
      "|      Aberdeen|   SD|    USA| ABR|  null|\n",
      "|       Alamosa|   CO|    USA| ALS|  null|\n",
      "|      Alliance|   NE|    USA| AIA|  null|\n",
      "|        Alpena|   MI|    USA| APN|  null|\n",
      "|       Altoona|   PA|    USA| AOO|  null|\n",
      "|        Athens|   GA|    USA| AHN|  null|\n",
      "|       Augusta|   ME|    USA| AUG|  null|\n",
      "|    Bar Harbor|   ME|    USA| BHB|  null|\n",
      "|       Beckley|   WV|    USA| BKW|  null|\n",
      "|       Bedford|   MA|    USA| BED|  null|\n",
      "|       Bemidji|   MN|    USA| BJI|  null|\n",
      "|       Bettles|   AK|    USA| BTT|  null|\n",
      "|   Bloomington|   IN|    USA| BMG|  null|\n",
      "|     Bluefield|   WV|    USA| BLF|  null|\n",
      "|     Brookings|   SD|    USA| BKX|  null|\n",
      "|    Burlington|   IA|    USA| BRL|  null|\n",
      "|    Burlington|   MA|    USA| BBF|  null|\n",
      "|Cape Girardeau|   MO|    USA| CGI|  null|\n",
      "|      Carlsbad|   NM|    USA| CNM|  null|\n",
      "|      Cheyenne|   WY|    USA| CYS|  null|\n",
      "+--------------+-----+-------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCodes. \\\n",
    "    join(\n",
    "        airtraffic,\n",
    "        on= airportCodes['IATA']==airtraffic['Origin'],\n",
    "        how = 'left'\n",
    "    ). \\\n",
    "    filter(\"Origin IS NULL\"). \\\n",
    "    select(airportCodes['*'], col(\"Origin\")). \\\n",
    "    show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "508d7444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-------+----+------+\n",
      "|          City|State|Country|IATA|Origin|\n",
      "+--------------+-----+-------+----+------+\n",
      "|      Aberdeen|   SD|    USA| ABR|  null|\n",
      "|       Alamosa|   CO|    USA| ALS|  null|\n",
      "|      Alliance|   NE|    USA| AIA|  null|\n",
      "|        Alpena|   MI|    USA| APN|  null|\n",
      "|       Altoona|   PA|    USA| AOO|  null|\n",
      "|        Athens|   GA|    USA| AHN|  null|\n",
      "|       Augusta|   ME|    USA| AUG|  null|\n",
      "|    Bar Harbor|   ME|    USA| BHB|  null|\n",
      "|       Beckley|   WV|    USA| BKW|  null|\n",
      "|       Bedford|   MA|    USA| BED|  null|\n",
      "|       Bemidji|   MN|    USA| BJI|  null|\n",
      "|       Bettles|   AK|    USA| BTT|  null|\n",
      "|   Bloomington|   IN|    USA| BMG|  null|\n",
      "|     Bluefield|   WV|    USA| BLF|  null|\n",
      "|     Brookings|   SD|    USA| BKX|  null|\n",
      "|    Burlington|   IA|    USA| BRL|  null|\n",
      "|    Burlington|   MA|    USA| BBF|  null|\n",
      "|Cape Girardeau|   MO|    USA| CGI|  null|\n",
      "|      Carlsbad|   NM|    USA| CNM|  null|\n",
      "|      Cheyenne|   WY|    USA| CYS|  null|\n",
      "+--------------+-----+-------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportCodes. \\\n",
    "    join(\n",
    "        airtraffic,\n",
    "        on= airportCodes['IATA']==airtraffic['Origin'],\n",
    "        how = 'left'\n",
    "    ). \\\n",
    "    filter(airtraffic['Origin'].isNull()). \\\n",
    "    select(airportCodes['*'], col(\"Origin\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00da515a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes. \\\n",
    "    join(\n",
    "        airtraffic,\n",
    "        on= airportCodes['IATA']==airtraffic['Origin'],\n",
    "        how = 'left'\n",
    "    ). \\\n",
    "    filter(airtraffic['Origin'].isNull()). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "312b9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 209 Solution - Get Origins without master data using Spark DF APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fde8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 17 Joining Data Sets'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8657c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c24258bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtrafficPath = \"/public/airlines_all/airlines-part/flightmonth=200801\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4888f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtrafficPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b3007cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+------+----+----------+\n",
      "|Year|Month|DayOfMonth|Origin|Dest|CRSDepTime|\n",
      "+----+-----+----------+------+----+----------+\n",
      "|2008|    1|        16|   BGR| CVG|      1735|\n",
      "|2008|    1|        17|   SYR| CVG|      1701|\n",
      "|2008|    1|        17|   SAV| BOS|      1225|\n",
      "|2008|    1|        17|   CVG| GRR|      1530|\n",
      "|2008|    1|        17|   STL| CVG|      1205|\n",
      "+----+-----+----------+------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select(\n",
    "        \"Year\", \"Month\", \"DayOfMonth\",\n",
    "        \"Origin\",\"Dest\", \"CRSDepTime\"\n",
    "    ). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3339dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodesPath = \"/public/airlines_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a1d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValidAirportCodes(airportCodesPath):\n",
    "    airportCodes = spark. \\\n",
    "        read. \\\n",
    "        option(\"sep\",\"\\t\"). \\\n",
    "        option(\"header\",True). \\\n",
    "        option(\"inferSchema\", True). \\\n",
    "        csv(airportCodesPath). \\\n",
    "        filter(\"!(State='Hawaii' AND IATA='Big') AND Country='USA'\")\n",
    "    return airportCodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "340d932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodes = getValidAirportCodes(airportCodesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ad0fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bba906e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+-------+----+\n",
      "|Origin|City|State|Country|IATA|\n",
      "+------+----+-----+-------+----+\n",
      "|   HDN|null| null|   null|null|\n",
      "|   HDN|null| null|   null|null|\n",
      "|   SJU|null| null|   null|null|\n",
      "|   SJU|null| null|   null|null|\n",
      "|   ITO|null| null|   null|null|\n",
      "+------+----+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA'],\n",
    "        how = 'left'\n",
    "    ).\\\n",
    "    filter(\"IATA IS NULL\"). \\\n",
    "    select(airtraffic['Origin'],airportCodes['*']). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c48a352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+-------+----+\n",
      "|Origin|City|State|Country|IATA|\n",
      "+------+----+-----+-------+----+\n",
      "|   HDN|null| null|   null|null|\n",
      "|   SJU|null| null|   null|null|\n",
      "|   ITO|null| null|   null|null|\n",
      "|   STT|null| null|   null|null|\n",
      "|   CEC|null| null|   null|null|\n",
      "+------+----+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA'],\n",
    "        how = 'left'\n",
    "    ).\\\n",
    "    filter(\"IATA IS NULL\"). \\\n",
    "    select(airtraffic['Origin'],airportCodes['*']). \\\n",
    "    distinct(). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e10a095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+-------+----+\n",
      "|Origin|City|State|Country|IATA|\n",
      "+------+----+-----+-------+----+\n",
      "|   HDN|null| null|   null|null|\n",
      "|   SJU|null| null|   null|null|\n",
      "|   ITO|null| null|   null|null|\n",
      "|   STT|null| null|   null|null|\n",
      "|   CEC|null| null|   null|null|\n",
      "+------+----+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA'],\n",
    "        how = 'left'\n",
    "    ).\\\n",
    "    filter(airportCodes['IATA'].isNull()). \\\n",
    "    select(airtraffic['Origin'],airportCodes['*']). \\\n",
    "    distinct(). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41697a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA'],\n",
    "        how = 'left'\n",
    "    ).\\\n",
    "    filter(airportCodes['IATA'].isNull()). \\\n",
    "    select(airtraffic['Origin'],airportCodes['*']). \\\n",
    "    distinct(). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe68061a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+-------+----+\n",
      "|Origin|City|State|Country|IATA|\n",
      "+------+----+-----+-------+----+\n",
      "|   HDN|null| null|   null|null|\n",
      "|   SJU|null| null|   null|null|\n",
      "|   ITO|null| null|   null|null|\n",
      "|   STT|null| null|   null|null|\n",
      "|   CEC|null| null|   null|null|\n",
      "|   CDC|null| null|   null|null|\n",
      "|   PSG|null| null|   null|null|\n",
      "|   ADK|null| null|   null|null|\n",
      "|   KOA|null| null|   null|null|\n",
      "|   OTZ|null| null|   null|null|\n",
      "|   BQN|null| null|   null|null|\n",
      "|   STX|null| null|   null|null|\n",
      "|   PMD|null| null|   null|null|\n",
      "|   PSE|null| null|   null|null|\n",
      "|   SCC|null| null|   null|null|\n",
      "|   SLE|null| null|   null|null|\n",
      "+------+----+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select(\"Origin\"). \\\n",
    "    distinct(). \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA'],\n",
    "        how = 'left'\n",
    "    ). \\\n",
    "    filter(\"IATA IS NULL\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35c66620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select(\"Origin\"). \\\n",
    "    distinct(). \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA'],\n",
    "        how = 'left'\n",
    "    ). \\\n",
    "    filter(\"IATA IS NULL\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc1abb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 210 Soulution Problem 6 -Get count of Flights without master data using Spark DF APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8de852e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 17 Joining Data Sets'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "594dcfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f648acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtrafficPath = \"/public/airlines_all/airlines-part/flightmonth=200801\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "400e3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtrafficPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "324e6c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+------+----+----------+\n",
      "|Year|Month|DayOfMonth|Origin|Dest|CRSDepTime|\n",
      "+----+-----+----------+------+----+----------+\n",
      "|2008|    1|        16|   BGR| CVG|      1735|\n",
      "|2008|    1|        17|   SYR| CVG|      1701|\n",
      "|2008|    1|        17|   SAV| BOS|      1225|\n",
      "|2008|    1|        17|   CVG| GRR|      1530|\n",
      "|2008|    1|        17|   STL| CVG|      1205|\n",
      "+----+-----+----------+------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select(\n",
    "        \"Year\", \"Month\", \"DayOfMonth\",\n",
    "        \"Origin\",\"Dest\", \"CRSDepTime\"\n",
    "    ). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b665f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodesPath = \"/public/airlines_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b06313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValidAirportCodes(airportCodesPath):\n",
    "    airportCodes = spark. \\\n",
    "        read. \\\n",
    "        option(\"sep\",\"\\t\"). \\\n",
    "        option(\"header\",True). \\\n",
    "        option(\"inferSchema\", True). \\\n",
    "        csv(airportCodesPath). \\\n",
    "        filter(\"!(State='Hawaii' AND IATA='Big')\")\n",
    "    return airportCodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92916523",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodes = getValidAirportCodes(airportCodesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a44ed8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "015e5009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+------+----+----------+----+-----+-------+----+\n",
      "|Year|Month|DayOfMonth|Origin|Dest|CRSDepTime|City|State|Country|IATA|\n",
      "+----+-----+----------+------+----+----------+----+-----+-------+----+\n",
      "|2008|    1|         8|   HDN| DEN|      1403|null| null|   null|null|\n",
      "|2008|    1|        26|   HDN| DEN|      1533|null| null|   null|null|\n",
      "|2008|    1|        26|   SJU| CLT|      1520|null| null|   null|null|\n",
      "|2008|    1|        19|   SJU| ATL|       945|null| null|   null|null|\n",
      "|2008|    1|        31|   ITO| HNL|      1010|null| null|   null|null|\n",
      "|2008|    1|        26|   ITO| HNL|      1725|null| null|   null|null|\n",
      "|2008|    1|        24|   KOA| LAX|      2355|null| null|   null|null|\n",
      "|2008|    1|        22|   SJU| EWR|      1845|null| null|   null|null|\n",
      "|2008|    1|        21|   SJU| MIA|      1815|null| null|   null|null|\n",
      "|2008|    1|        26|   STT| MIA|       845|null| null|   null|null|\n",
      "|2008|    1|        11|   SJU| MIA|      1235|null| null|   null|null|\n",
      "|2008|    1|        31|   SJU| DFW|      1720|null| null|   null|null|\n",
      "|2008|    1|         5|   SJU| MIA|      2110|null| null|   null|null|\n",
      "|2008|    1|        30|   ITO| HNL|       915|null| null|   null|null|\n",
      "|2008|    1|         1|   KOA| HNL|      1245|null| null|   null|null|\n",
      "|2008|    1|         1|   KOA| HNL|      1030|null| null|   null|null|\n",
      "|2008|    1|        28|   ITO| HNL|      1153|null| null|   null|null|\n",
      "|2008|    1|         3|   OTZ| ANC|      1502|null| null|   null|null|\n",
      "|2008|    1|         8|   OTZ| OME|       829|null| null|   null|null|\n",
      "|2008|    1|        20|   SJU| JFK|      1245|null| null|   null|null|\n",
      "+----+-----+----------+------+----+----------+----+-----+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA'],\n",
    "        how='left'\n",
    "    ). \\\n",
    "    filter(\"IATA IS NULL\"). \\\n",
    "    select(airtraffic['Year'], airtraffic['Month'],airtraffic['DayOfMonth'],\n",
    "           airtraffic['Origin'], airtraffic['Dest'], airtraffic['CRSDepTime'],\n",
    "           airportCodes['*']\n",
    "    ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7518e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, lit, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6d32f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|Origin|FlightCount|\n",
      "+------+-----------+\n",
      "|   HDN|        429|\n",
      "|   SJU|       1997|\n",
      "|   ITO|        786|\n",
      "|   STT|        311|\n",
      "|   CEC|         88|\n",
      "|   CDC|         48|\n",
      "|   PSG|         62|\n",
      "|   ADK|          9|\n",
      "|   KOA|       1316|\n",
      "|   OTZ|         92|\n",
      "|   BQN|        124|\n",
      "|   STX|         40|\n",
      "|   PMD|         57|\n",
      "|   PSE|        110|\n",
      "|   SCC|         62|\n",
      "|   SLE|         54|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA'],\n",
    "        how='left'\n",
    "    ). \\\n",
    "    filter(\"IATA IS NULL\"). \\\n",
    "    groupBy(airtraffic['Origin']). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0af134b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|Origin|FlightCount|\n",
      "+------+-----------+\n",
      "|   SJU|       1997|\n",
      "|   KOA|       1316|\n",
      "|   ITO|        786|\n",
      "|   HDN|        429|\n",
      "|   STT|        311|\n",
      "|   BQN|        124|\n",
      "|   PSE|        110|\n",
      "|   OTZ|         92|\n",
      "|   CEC|         88|\n",
      "|   SCC|         62|\n",
      "|   PSG|         62|\n",
      "|   PMD|         57|\n",
      "|   SLE|         54|\n",
      "|   CDC|         48|\n",
      "|   STX|         40|\n",
      "|   ADK|          9|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA'],\n",
    "        how='left'\n",
    "    ). \\\n",
    "    filter(\"IATA IS NULL\"). \\\n",
    "    groupBy(airtraffic['Origin']). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    orderBy(col('FlightCount').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75aeb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 211 Solution Problem 5 Get count of Flights per Airport without master data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4059a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 17 Joining Data Sets'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "160037f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69fc9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtrafficPath = \"/public/airlines_all/airlines-part/flightmonth=200801\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0479f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtrafficPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "168dd489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+------+----+----------+\n",
      "|Year|Month|DayOfMonth|Origin|Dest|CRSDepTime|\n",
      "+----+-----+----------+------+----+----------+\n",
      "|2008|    1|        16|   BGR| CVG|      1735|\n",
      "|2008|    1|        17|   SYR| CVG|      1701|\n",
      "|2008|    1|        17|   SAV| BOS|      1225|\n",
      "|2008|    1|        17|   CVG| GRR|      1530|\n",
      "|2008|    1|        17|   STL| CVG|      1205|\n",
      "+----+-----+----------+------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select(\n",
    "        \"Year\", \"Month\", \"DayOfMonth\",\n",
    "        \"Origin\",\"Dest\", \"CRSDepTime\"\n",
    "    ). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e969d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodesPath = \"/public/airlines_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e4fca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValidAirportCodes(airportCodesPath):\n",
    "    airportCodes = spark. \\\n",
    "        read. \\\n",
    "        option(\"sep\",\"\\t\"). \\\n",
    "        option(\"header\",True). \\\n",
    "        option(\"inferSchema\", True). \\\n",
    "        csv(airportCodesPath). \\\n",
    "        filter(\"!(State='Hawaii' AND IATA='Big')\")\n",
    "    return airportCodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a948890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportCodes = getValidAirportCodes(airportCodesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "759c2931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportCodes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "355238b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+------+----+----------+----+-----+-------+----+\n",
      "|Year|Month|DayOfMonth|Origin|Dest|CRSDepTime|City|State|Country|IATA|\n",
      "+----+-----+----------+------+----+----------+----+-----+-------+----+\n",
      "|2008|    1|         8|   HDN| DEN|      1403|null| null|   null|null|\n",
      "|2008|    1|        26|   HDN| DEN|      1533|null| null|   null|null|\n",
      "|2008|    1|        26|   SJU| CLT|      1520|null| null|   null|null|\n",
      "|2008|    1|        19|   SJU| ATL|       945|null| null|   null|null|\n",
      "|2008|    1|        31|   ITO| HNL|      1010|null| null|   null|null|\n",
      "|2008|    1|        26|   ITO| HNL|      1725|null| null|   null|null|\n",
      "|2008|    1|        24|   KOA| LAX|      2355|null| null|   null|null|\n",
      "|2008|    1|        22|   SJU| EWR|      1845|null| null|   null|null|\n",
      "|2008|    1|        21|   SJU| MIA|      1815|null| null|   null|null|\n",
      "|2008|    1|        26|   STT| MIA|       845|null| null|   null|null|\n",
      "|2008|    1|        11|   SJU| MIA|      1235|null| null|   null|null|\n",
      "|2008|    1|        31|   SJU| DFW|      1720|null| null|   null|null|\n",
      "|2008|    1|         5|   SJU| MIA|      2110|null| null|   null|null|\n",
      "|2008|    1|        30|   ITO| HNL|       915|null| null|   null|null|\n",
      "|2008|    1|         1|   KOA| HNL|      1245|null| null|   null|null|\n",
      "|2008|    1|         1|   KOA| HNL|      1030|null| null|   null|null|\n",
      "|2008|    1|        28|   ITO| HNL|      1153|null| null|   null|null|\n",
      "|2008|    1|         3|   OTZ| ANC|      1502|null| null|   null|null|\n",
      "|2008|    1|         8|   OTZ| OME|       829|null| null|   null|null|\n",
      "|2008|    1|        20|   SJU| JFK|      1245|null| null|   null|null|\n",
      "+----+-----+----------+------+----+----------+----+-----+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA'],\n",
    "        how='left'\n",
    "    ). \\\n",
    "    filter(\"IATA IS NULL\"). \\\n",
    "    select(airtraffic['Year'], airtraffic['Month'],airtraffic['DayOfMonth'],\n",
    "           airtraffic['Origin'], airtraffic['Dest'], airtraffic['CRSDepTime'],\n",
    "           airportCodes['*']\n",
    "    ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c12b872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5585"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    join(\n",
    "        airportCodes,\n",
    "        on= airtraffic['Origin']==airportCodes['IATA'],\n",
    "        how='left'\n",
    "    ). \\\n",
    "    filter(\"IATA IS NULL\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3af8c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 212 Solution Problem 7 - Get Daily Revenue using Spark DF APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7fd272e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 17 Joining Data Sets'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0985abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e07ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.json(\"/public/retail_db_json/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "062bfa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = spark.read.json(\"/public/retail_db_json/order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb5f6808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5e9cf53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- order_item_order_id: long (nullable = true)\n",
      " |-- order_item_product_id: long (nullable = true)\n",
      " |-- order_item_product_price: double (nullable = true)\n",
      " |-- order_item_quantity: long (nullable = true)\n",
      " |-- order_item_subtotal: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f754984b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e990eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filtered = orders.filter(\"order_status IN ('COMPLETE','CLOSED')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "91fa7e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30455"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b00cbda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172198"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_items.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ab396e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "60e6f999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|          order_date| revenue|\n",
      "+--------------------+--------+\n",
      "|2013-07-25 00:00:...|31547.23|\n",
      "|2013-07-26 00:00:...|54713.23|\n",
      "|2013-07-27 00:00:...|48411.48|\n",
      "|2013-07-28 00:00:...|35672.03|\n",
      "|2013-07-29 00:00:...| 54579.7|\n",
      "|2013-07-30 00:00:...|49329.29|\n",
      "|2013-07-31 00:00:...|59212.49|\n",
      "|2013-08-01 00:00:...|49160.08|\n",
      "|2013-08-02 00:00:...|50688.58|\n",
      "|2013-08-03 00:00:...|43416.74|\n",
      "|2013-08-04 00:00:...|35093.01|\n",
      "|2013-08-05 00:00:...|34025.27|\n",
      "|2013-08-06 00:00:...|57843.89|\n",
      "|2013-08-07 00:00:...|45525.59|\n",
      "|2013-08-08 00:00:...|33549.47|\n",
      "|2013-08-09 00:00:...|29225.16|\n",
      "|2013-08-10 00:00:...|46435.04|\n",
      "|2013-08-11 00:00:...| 31155.5|\n",
      "|2013-08-12 00:00:...|59014.74|\n",
      "|2013-08-13 00:00:...|17956.88|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_filtered. \\\n",
    "    join(\n",
    "        order_items,\n",
    "        on= orders_filtered.order_id==order_items.order_item_order_id,\n",
    "        how ='inner'\n",
    "    ). \\\n",
    "    groupBy(orders_filtered[\"order_date\"]). \\\n",
    "    agg(\n",
    "        round(sum(order_items[\"order_item_subtotal\"]),2).alias(\"revenue\")\n",
    "    ). \\\n",
    "    orderBy(orders_filtered[\"order_date\"]). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "44b1f113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_filtered. \\\n",
    "    join(\n",
    "        order_items,\n",
    "        on= orders_filtered.order_id==order_items.order_item_order_id,\n",
    "        how ='inner'\n",
    "    ). \\\n",
    "    groupBy(orders_filtered[\"order_date\"]). \\\n",
    "    agg(\n",
    "        round(sum(order_items[\"order_item_subtotal\"]),2).alias(\"revenue\")\n",
    "    ). \\\n",
    "    orderBy(orders_filtered[\"order_date\"]). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7a791a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 213 - Solution Problem 8 - Get Daily Revenue Rolled up till Yearly using Spari DF APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1de40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 17 Joining Data Sets'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef888ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c621d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.json(\"/public/retail_db_json/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "558afcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = spark.read.json(\"/public/retail_db_json/order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c668d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffbf1108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- order_item_order_id: long (nullable = true)\n",
      " |-- order_item_product_id: long (nullable = true)\n",
      " |-- order_item_product_price: double (nullable = true)\n",
      " |-- order_item_quantity: long (nullable = true)\n",
      " |-- order_item_subtotal: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ccf97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0f7eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filtered = orders.filter(\"order_status IN ('COMPLETE','CLOSED')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c08407f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30455"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d35d5f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172198"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_items.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6e0af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26af87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_join = orders_filtered. \\\n",
    "    join(order_items, orders_filtered['order_id']==order_items['order_item_order_id'],'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e142a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct, sum, round, date_format, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1ca1657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+--------+\n",
      "|order_year|order_month|          order_date| revenue|\n",
      "+----------+-----------+--------------------+--------+\n",
      "|      2013|     201307|2013-07-27 00:00:...|48411.48|\n",
      "|      2013|     201307|2013-07-28 00:00:...|35672.03|\n",
      "|      2013|     201307|2013-07-29 00:00:...| 54579.7|\n",
      "|      2013|     201307|2013-07-30 00:00:...|49329.29|\n",
      "|      2013|     201308|2013-08-01 00:00:...|49160.08|\n",
      "|      2013|     201308|2013-08-02 00:00:...|50688.58|\n",
      "|      2013|     201308|2013-08-04 00:00:...|35093.01|\n",
      "|      2013|     201308|2013-08-06 00:00:...|57843.89|\n",
      "|      2013|     201308|2013-08-09 00:00:...|29225.16|\n",
      "|      2013|     201308|2013-08-11 00:00:...| 31155.5|\n",
      "|      2013|     201308|2013-08-15 00:00:...|49566.68|\n",
      "|      2013|     201308|2013-08-17 00:00:...|63226.83|\n",
      "|      2013|     201308|2013-08-22 00:00:...|38190.02|\n",
      "|      2013|     201308|2013-08-24 00:00:...|52650.15|\n",
      "|      2013|     201308|2013-08-26 00:00:...| 38548.4|\n",
      "|      2013|     201308|2013-08-27 00:00:...|41010.78|\n",
      "|      2013|     201308|2013-08-31 00:00:...|33955.11|\n",
      "|      2013|     201309|2013-09-01 00:00:...|42237.77|\n",
      "|      2013|     201309|2013-09-02 00:00:...|44463.56|\n",
      "|      2013|     201309|2013-09-05 00:00:...|59942.43|\n",
      "+----------+-----------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "revenue = orders_join. \\\n",
    "    rollup(\n",
    "        year('order_date').alias('order_year'),\n",
    "        date_format(col('order_date'),'yyyyMM').alias('order_month'),\n",
    "        'order_date'\n",
    "    ). \\\n",
    "    agg(\n",
    "        round(sum(order_items['order_item_subtotal']),2).alias('revenue')\n",
    "    ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcdc8e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-----------+--------+\n",
      "|order_year|order_month|          order_date|order_count| revenue|\n",
      "+----------+-----------+--------------------+-----------+--------+\n",
      "|      2014|     201407|2014-07-24 00:00:...|         81|50885.19|\n",
      "|      2014|     201407|2014-07-23 00:00:...|         58|38795.23|\n",
      "|      2014|     201407|2014-07-22 00:00:...|         60|36717.24|\n",
      "|      2014|     201407|2014-07-21 00:00:...|         83| 51427.7|\n",
      "|      2014|     201407|2014-07-20 00:00:...|        105|60047.45|\n",
      "|      2014|     201407|2014-07-19 00:00:...|         71|38420.99|\n",
      "|      2014|     201407|2014-07-18 00:00:...|         72| 43856.6|\n",
      "|      2014|     201407|2014-07-17 00:00:...|         64|36384.77|\n",
      "|      2014|     201407|2014-07-16 00:00:...|         67|43011.92|\n",
      "|      2014|     201407|2014-07-15 00:00:...|        102|53480.23|\n",
      "|      2014|     201407|2014-07-14 00:00:...|         58|29937.52|\n",
      "|      2014|     201407|2014-07-13 00:00:...|         74|40410.99|\n",
      "|      2014|     201407|2014-07-12 00:00:...|         71|38449.77|\n",
      "|      2014|     201407|2014-07-11 00:00:...|         50|29596.32|\n",
      "|      2014|     201407|2014-07-10 00:00:...|         74|47826.02|\n",
      "|      2014|     201407|2014-07-09 00:00:...|         63|36929.91|\n",
      "|      2014|     201407|2014-07-08 00:00:...|         83|50434.81|\n",
      "|      2014|     201407|2014-07-07 00:00:...|         61|35441.49|\n",
      "|      2014|     201407|2014-07-06 00:00:...|         29|16451.76|\n",
      "|      2014|     201407|2014-07-05 00:00:...|         84|47214.68|\n",
      "+----------+-----------+--------------------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "revenue = orders_join. \\\n",
    "    rollup(\n",
    "        year('order_date').alias('order_year'),\n",
    "        date_format(col('order_date'),'yyyyMM').alias('order_month'),\n",
    "        'order_date'\n",
    "    ). \\\n",
    "    agg(\n",
    "        countDistinct('order_id').alias('order_count'),\n",
    "        round(sum(order_items['order_item_subtotal']),2).alias('revenue')\n",
    "    ). \\\n",
    "    orderBy(col('order_year').desc(), col('order_month').desc(),col('order_date').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af267145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-----------+-------------+\n",
      "|order_year|order_month|          order_date|order_count|      revenue|\n",
      "+----------+-----------+--------------------+-----------+-------------+\n",
      "|      null|       null|                null|      25266|1.501298248E7|\n",
      "|      2013|       null|                null|      11266|    6686892.0|\n",
      "|      2013|     201307|                null|        564|    333465.45|\n",
      "|      2013|     201307|2013-07-25 00:00:...|         51|     31547.23|\n",
      "|      2013|     201307|2013-07-26 00:00:...|         99|     54713.23|\n",
      "|      2013|     201307|2013-07-27 00:00:...|         80|     48411.48|\n",
      "|      2013|     201307|2013-07-28 00:00:...|         67|     35672.03|\n",
      "|      2013|     201307|2013-07-29 00:00:...|         90|      54579.7|\n",
      "|      2013|     201307|2013-07-30 00:00:...|         90|     49329.29|\n",
      "|      2013|     201307|2013-07-31 00:00:...|         87|     59212.49|\n",
      "|      2013|     201308|                null|       2073|    1221828.9|\n",
      "|      2013|     201308|2013-08-01 00:00:...|         82|     49160.08|\n",
      "|      2013|     201308|2013-08-02 00:00:...|         90|     50688.58|\n",
      "|      2013|     201308|2013-08-03 00:00:...|         72|     43416.74|\n",
      "|      2013|     201308|2013-08-04 00:00:...|         63|     35093.01|\n",
      "|      2013|     201308|2013-08-05 00:00:...|         62|     34025.27|\n",
      "|      2013|     201308|2013-08-06 00:00:...|         99|     57843.89|\n",
      "|      2013|     201308|2013-08-07 00:00:...|         77|     45525.59|\n",
      "|      2013|     201308|2013-08-08 00:00:...|         57|     33549.47|\n",
      "|      2013|     201308|2013-08-09 00:00:...|         43|     29225.16|\n",
      "+----------+-----------+--------------------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "revenue = orders_join. \\\n",
    "    rollup(\n",
    "        year('order_date').alias('order_year'),\n",
    "        date_format(col('order_date'),'yyyyMM').alias('order_month'),\n",
    "        'order_date'\n",
    "    ). \\\n",
    "    agg(\n",
    "        countDistinct('order_id').alias('order_count'),\n",
    "        round(sum(order_items['order_item_subtotal']),2).alias('revenue')\n",
    "    ). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42fb8ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-----------+---------+\n",
      "|order_year|order_month|          order_date|order_count|  revenue|\n",
      "+----------+-----------+--------------------+-----------+---------+\n",
      "|      2013|     201307|2013-07-25 00:00:...|         51| 31547.23|\n",
      "|      2013|     201307|2013-07-26 00:00:...|         99| 54713.23|\n",
      "|      2013|     201307|2013-07-27 00:00:...|         80| 48411.48|\n",
      "|      2013|     201307|2013-07-28 00:00:...|         67| 35672.03|\n",
      "|      2013|     201307|2013-07-29 00:00:...|         90|  54579.7|\n",
      "|      2013|     201307|2013-07-30 00:00:...|         90| 49329.29|\n",
      "|      2013|     201307|2013-07-31 00:00:...|         87| 59212.49|\n",
      "|      2013|     201307|                null|        564|333465.45|\n",
      "|      2013|     201308|2013-08-01 00:00:...|         82| 49160.08|\n",
      "|      2013|     201308|2013-08-02 00:00:...|         90| 50688.58|\n",
      "|      2013|     201308|2013-08-03 00:00:...|         72| 43416.74|\n",
      "|      2013|     201308|2013-08-04 00:00:...|         63| 35093.01|\n",
      "|      2013|     201308|2013-08-05 00:00:...|         62| 34025.27|\n",
      "|      2013|     201308|2013-08-06 00:00:...|         99| 57843.89|\n",
      "|      2013|     201308|2013-08-07 00:00:...|         77| 45525.59|\n",
      "|      2013|     201308|2013-08-08 00:00:...|         57| 33549.47|\n",
      "|      2013|     201308|2013-08-09 00:00:...|         43| 29225.16|\n",
      "|      2013|     201308|2013-08-10 00:00:...|         83| 46435.04|\n",
      "|      2013|     201308|2013-08-11 00:00:...|         58|  31155.5|\n",
      "|      2013|     201308|2013-08-12 00:00:...|         93| 59014.74|\n",
      "+----------+-----------+--------------------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "revenue = orders_join. \\\n",
    "    rollup(\n",
    "        year('order_date').alias('order_year'),\n",
    "        date_format(col('order_date'),'yyyyMM').alias('order_month'),\n",
    "        'order_date'\n",
    "    ). \\\n",
    "    agg(\n",
    "        countDistinct('order_id').alias('order_count'),\n",
    "        round(sum(order_items['order_item_subtotal']),2).alias('revenue')\n",
    "    ). \\\n",
    "    orderBy(\n",
    "        col('order_year').asc_nulls_last(),\n",
    "        col('order_month').asc_nulls_last(),\n",
    "        col('order_date').asc_nulls_last()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff69375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
