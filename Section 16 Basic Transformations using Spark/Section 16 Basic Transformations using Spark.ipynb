{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c256fd61",
   "metadata": {},
   "source": [
    "## Section 16 Basic Transformations using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7a3a2",
   "metadata": {},
   "source": [
    "### 184, 185 Basic Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b552d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse/'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config(\"spark.driver.memory\", '4g'). \\\n",
    "        config('spark.executor.memory', '4g'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Basic Transformations'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()\n",
    "\n",
    "#         config('spark.executor.cores', '6'). \\\n",
    "#from pyspark.sql import SparkSession \n",
    "\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "#spark.conf.set('spark.executor.cores', '12')\n",
    "#spark.conf.set(\"spark.driver.memory\", '2g')\n",
    "#spark.conf.set('spark.executor.memory', '2g')\n",
    "#spark.conf.set(\"spark.python.worker.memory\", '24g')\n",
    "#spark.conf.set(\"spark.sql.shuffle.partitions\", '12')\n",
    "#sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b959997c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 hdfs supergroup   14654075 2021-01-28 11:28 /public/airlines_all/airlines-part/flightmonth=200801/part-00252-5cde1303-4ebf-4a12-8fad-f5d9f9c9124a.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /public/airlines_all/airlines-part/flightmonth=200801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4c14bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = '/public/airlines_all/airlines-part/flightmonth=200801'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9223ea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21dc351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      " |-- IsArrDelayed: string (nullable = true)\n",
      " |-- IsDepDelayed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948e2de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+\n",
      "|Year|Month|DayOfMonth|\n",
      "+----+-----+----------+\n",
      "|2008|1    |1         |\n",
      "|2008|1    |2         |\n",
      "|2008|1    |3         |\n",
      "|2008|1    |4         |\n",
      "|2008|1    |5         |\n",
      "|2008|1    |6         |\n",
      "|2008|1    |7         |\n",
      "|2008|1    |8         |\n",
      "|2008|1    |9         |\n",
      "|2008|1    |10        |\n",
      "|2008|1    |11        |\n",
      "|2008|1    |12        |\n",
      "|2008|1    |13        |\n",
      "|2008|1    |14        |\n",
      "|2008|1    |15        |\n",
      "|2008|1    |16        |\n",
      "|2008|1    |17        |\n",
      "|2008|1    |18        |\n",
      "|2008|1    |19        |\n",
      "|2008|1    |20        |\n",
      "|2008|1    |21        |\n",
      "|2008|1    |22        |\n",
      "|2008|1    |23        |\n",
      "|2008|1    |24        |\n",
      "|2008|1    |25        |\n",
      "|2008|1    |26        |\n",
      "|2008|1    |27        |\n",
      "|2008|1    |28        |\n",
      "|2008|1    |29        |\n",
      "|2008|1    |30        |\n",
      "|2008|1    |31        |\n",
      "+----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.select('Year','Month', 'DayOfMonth').distinct().sort('DayOfMonth').show(31,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ba3beaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605659"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2066c031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.select('Year','Month', 'DayOfMonth').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45192ea1",
   "metadata": {},
   "source": [
    "### Basic filtering of Data or rows using where "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0e01499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297956"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter(\"IsArrDelayed = 'YES'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d15e59ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210965"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter(\"IsArrDelayed = 'YES' and IsDepDelayed='YES'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3130d7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297956"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter(airtraffic[\"IsArrDelayed\"]==\"YES\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfea7c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210965"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter((airtraffic[\"IsArrDelayed\"]==\"YES\") & (airtraffic[\"IsDepDelayed\"]==\"YES\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c3a1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9634eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297956"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter(col(\"IsArrDelayed\")==\"YES\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a35ab535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210965"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter((col(\"IsArrDelayed\")==\"YES\") & (col(\"IsDepDelayed\")==\"YES\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43c94579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+------+----+------------+------------+\n",
      "|Year|Month|DayOfMonth|Origin|Dest|IsArrDelayed|IsDepDelayed|\n",
      "+----+-----+----------+------+----+------------+------------+\n",
      "|2008|    1|        17|   SYR| CVG|         YES|         YES|\n",
      "|2008|    1|        18|   MCI| CVG|         YES|         YES|\n",
      "|2008|    1|        21|   DCA| JFK|         YES|         YES|\n",
      "|2008|    1|        22|   ORD| CVG|         YES|         YES|\n",
      "|2008|    1|         1|   ATL| BNA|         YES|         YES|\n",
      "+----+-----+----------+------+----+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select(\"Year\", \"Month\", \"DayOfMonth\",\"Origin\",\"Dest\",\"IsArrDelayed\", \"IsDepDelayed\"). \\\n",
    "    filter((col(\"IsArrDelayed\")==\"YES\") & (col(\"IsDepDelayed\")==\"YES\")). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "080e5ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Cancelled|\n",
      "+---------+\n",
      "|        1|\n",
      "|        0|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.select('Cancelled').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76e67fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17293"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter(col('Cancelled')==1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "288e9ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17293"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter('Cancelled==1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aecefa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17293"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter(airtraffic['Cancelled']==1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9aa86696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17293"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter(airtraffic.Cancelled==1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f184d0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11573"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter(\"Origin == 'SFO'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad664e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11573"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter(airtraffic[\"Origin\"] == 'SFO').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34ea85d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|IsDepDelayed|\n",
      "+------------+\n",
      "|         YES|\n",
      "|          NO|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.select('IsDepDelayed').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ed60aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Cancelled|\n",
      "+---------+\n",
      "|        0|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.filter(\"IsDepDelayed = 'NO'\").select('Cancelled').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f627fec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340461"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter(\"IsDepDelayed = 'NO'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f93a8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340461"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter(airtraffic[\"IsDepDelayed\"] == 'NO').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3be85b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340461"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter(col(\"IsDepDelayed\") == 'NO').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb4f2151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340461"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.filter(airtraffic.IsDepDelayed == 'NO').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce410665",
   "metadata": {},
   "source": [
    "### 187 Filtering Example using dates on Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "573ff167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse/'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config(\"spark.driver.memory\", '4g'). \\\n",
    "        config('spark.executor.memory', '4g'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Basic Transformations'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b135a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed6b2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = '/public/airlines_all/airlines-part/flightmonth=200801'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0e119f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64ce1272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      " |-- IsArrDelayed: string (nullable = true)\n",
      " |-- IsDepDelayed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba9bb8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+\n",
      "|Year|Month|DayOfMonth|\n",
      "+----+-----+----------+\n",
      "|2008|    1|         1|\n",
      "|2008|    1|         2|\n",
      "|2008|    1|         3|\n",
      "|2008|    1|         4|\n",
      "|2008|    1|         5|\n",
      "|2008|    1|         6|\n",
      "|2008|    1|         7|\n",
      "|2008|    1|         8|\n",
      "|2008|    1|         9|\n",
      "|2008|    1|        10|\n",
      "|2008|    1|        11|\n",
      "|2008|    1|        12|\n",
      "|2008|    1|        13|\n",
      "|2008|    1|        14|\n",
      "|2008|    1|        15|\n",
      "|2008|    1|        16|\n",
      "|2008|    1|        17|\n",
      "|2008|    1|        18|\n",
      "|2008|    1|        19|\n",
      "|2008|    1|        20|\n",
      "|2008|    1|        21|\n",
      "|2008|    1|        22|\n",
      "|2008|    1|        23|\n",
      "|2008|    1|        24|\n",
      "|2008|    1|        25|\n",
      "|2008|    1|        26|\n",
      "|2008|    1|        27|\n",
      "|2008|    1|        28|\n",
      "|2008|    1|        29|\n",
      "|2008|    1|        30|\n",
      "|2008|    1|        31|\n",
      "+----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.select('Year','Month','DayOfMonth').distinct().sort('DayOfMonth').show(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd9522ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.select('Year','Month','DayOfMonth').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb414640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605659"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20502853",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "531f8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l,'dummy STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53a7bfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8bdccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b64bf8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2024-02-12|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0972d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "642145cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|current_date|day_name|\n",
      "+------------+--------+\n",
      "|  2024-02-12|     Mon|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date().alias(\"current_date\"), date_format(current_date(),'EE').alias('day_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "67c0f817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|current_date|day_name|\n",
      "+------------+--------+\n",
      "|  2024-02-12|  Monday|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date().alias(\"current_date\"), date_format(current_date(),'EEEE').alias('day_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67c5dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+\n",
      "|Year|Month|DayOfMonth|\n",
      "+----+-----+----------+\n",
      "|2008|    1|        16|\n",
      "|2008|    1|        17|\n",
      "|2008|    1|        17|\n",
      "|2008|    1|        17|\n",
      "|2008|    1|        17|\n",
      "|2008|    1|        18|\n",
      "|2008|    1|        18|\n",
      "|2008|    1|        19|\n",
      "|2008|    1|        20|\n",
      "|2008|    1|        20|\n",
      "|2008|    1|        21|\n",
      "|2008|    1|        21|\n",
      "|2008|    1|        21|\n",
      "|2008|    1|        21|\n",
      "|2008|    1|        21|\n",
      "|2008|    1|        22|\n",
      "|2008|    1|        22|\n",
      "|2008|    1|        23|\n",
      "|2008|    1|        23|\n",
      "|2008|    1|        23|\n",
      "+----+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.select('Year','Month','DayOfMonth').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6437c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lpad, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6a0013c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|FlightDate|\n",
      "+----------+\n",
      "|  20080116|\n",
      "|  20080117|\n",
      "|  20080117|\n",
      "|  20080117|\n",
      "|  20080117|\n",
      "|  20080118|\n",
      "|  20080118|\n",
      "|  20080119|\n",
      "|  20080120|\n",
      "|  20080120|\n",
      "|  20080121|\n",
      "|  20080121|\n",
      "|  20080121|\n",
      "|  20080121|\n",
      "|  20080121|\n",
      "|  20080122|\n",
      "|  20080122|\n",
      "|  20080123|\n",
      "|  20080123|\n",
      "|  20080123|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.select(\n",
    "    concat(\n",
    "        col(\"year\"),\n",
    "        lpad(col(\"Month\"),2,\"0\"),\n",
    "        lpad(col(\"DayOfMonth\"),2,\"0\")\n",
    "    ).alias('FlightDate')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a45c346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "61ceabc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|to_date(FlightDate, yyyyMMdd)|\n",
      "+-----------------------------+\n",
      "|                   2008-01-16|\n",
      "|                   2008-01-17|\n",
      "|                   2008-01-17|\n",
      "|                   2008-01-17|\n",
      "|                   2008-01-17|\n",
      "|                   2008-01-18|\n",
      "|                   2008-01-18|\n",
      "|                   2008-01-19|\n",
      "|                   2008-01-20|\n",
      "|                   2008-01-20|\n",
      "|                   2008-01-21|\n",
      "|                   2008-01-21|\n",
      "|                   2008-01-21|\n",
      "|                   2008-01-21|\n",
      "|                   2008-01-21|\n",
      "|                   2008-01-22|\n",
      "|                   2008-01-22|\n",
      "|                   2008-01-23|\n",
      "|                   2008-01-23|\n",
      "|                   2008-01-23|\n",
      "+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.select(\n",
    "        concat(\n",
    "            col(\"year\"),\n",
    "            lpad(col(\"Month\"),2,\"0\"),\n",
    "            lpad(col(\"DayOfMonth\"),2,\"0\")\n",
    "        ).alias('FlightDate')\n",
    "    ).\\\n",
    "    select(to_date(col('FlightDate'), 'yyyyMMdd')). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7e00ab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|FlightDate|\n",
      "+----------+\n",
      "|2008-01-16|\n",
      "|2008-01-17|\n",
      "|2008-01-17|\n",
      "|2008-01-17|\n",
      "|2008-01-17|\n",
      "|2008-01-18|\n",
      "|2008-01-18|\n",
      "|2008-01-19|\n",
      "|2008-01-20|\n",
      "|2008-01-20|\n",
      "|2008-01-21|\n",
      "|2008-01-21|\n",
      "|2008-01-21|\n",
      "|2008-01-21|\n",
      "|2008-01-21|\n",
      "|2008-01-22|\n",
      "|2008-01-22|\n",
      "|2008-01-23|\n",
      "|2008-01-23|\n",
      "|2008-01-23|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.select(\n",
    "        concat(\n",
    "            col(\"year\"),\n",
    "            lpad(col(\"Month\"),2,\"0\"),\n",
    "            lpad(col(\"DayOfMonth\"),2,\"0\")\n",
    "        ).alias('FlightDate')\n",
    "    ).\\\n",
    "    select(to_date(col('FlightDate'), 'yyyyMMdd').alias('FlightDate')). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "24853b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|FlightDate|\n",
      "+----------+\n",
      "|2008-01-16|\n",
      "|2008-01-17|\n",
      "|2008-01-17|\n",
      "|2008-01-17|\n",
      "|2008-01-17|\n",
      "|2008-01-18|\n",
      "|2008-01-18|\n",
      "|2008-01-19|\n",
      "|2008-01-20|\n",
      "|2008-01-20|\n",
      "|2008-01-21|\n",
      "|2008-01-21|\n",
      "|2008-01-21|\n",
      "|2008-01-21|\n",
      "|2008-01-21|\n",
      "|2008-01-22|\n",
      "|2008-01-22|\n",
      "|2008-01-23|\n",
      "|2008-01-23|\n",
      "|2008-01-23|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.select(\n",
    "        concat(\n",
    "            col(\"year\"),\n",
    "            lpad(col(\"Month\"),2,\"0\"),\n",
    "            lpad(col(\"DayOfMonth\"),2,\"0\")\n",
    "        ).alias('FlightDate')\n",
    "    ).\\\n",
    "    selectExpr(\"to_date(FlightDate, 'yyyyMMdd') AS FlightDate\"). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e5c51c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|FlightDate|\n",
      "+----------+\n",
      "| Wednesday|\n",
      "|  Thursday|\n",
      "|  Thursday|\n",
      "|  Thursday|\n",
      "|  Thursday|\n",
      "|    Friday|\n",
      "|    Friday|\n",
      "|  Saturday|\n",
      "|    Sunday|\n",
      "|    Sunday|\n",
      "|    Monday|\n",
      "|    Monday|\n",
      "|    Monday|\n",
      "|    Monday|\n",
      "|    Monday|\n",
      "|   Tuesday|\n",
      "|   Tuesday|\n",
      "| Wednesday|\n",
      "| Wednesday|\n",
      "| Wednesday|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.select(\n",
    "        concat(\n",
    "            col(\"year\"),\n",
    "            lpad(col(\"Month\"),2,\"0\"),\n",
    "            lpad(col(\"DayOfMonth\"),2,\"0\")\n",
    "        ).alias('FlightDate')\n",
    "    ).\\\n",
    "    selectExpr(\"date_format(to_date(FlightDate, 'yyyyMMdd'),'EEEE') AS FlightDate\"). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f2f8ae6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76395"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    withColumn(\"FlightDate\", \n",
    "        concat(\n",
    "            col(\"year\"),\n",
    "            lpad(col(\"Month\"),2,\"0\"),\n",
    "            lpad(col(\"DayOfMonth\"),2,\"0\")\n",
    "            )\n",
    "        ). \\\n",
    "    filter(\"\"\"\n",
    "        date_format(to_date(FlightDate, 'yyyyMMdd'),'EEEE') = 'Sunday'\n",
    "        \"\"\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "07cdcee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34708"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    withColumn(\"FlightDate\", \n",
    "        concat(\n",
    "            col(\"year\"),\n",
    "            lpad(col(\"Month\"),2,\"0\"),\n",
    "            lpad(col(\"DayOfMonth\"),2,\"0\")\n",
    "            )\n",
    "        ). \\\n",
    "    filter(\"\"\"\n",
    "        IsDepDelayed = 'YES' AND \n",
    "        date_format(to_date(FlightDate, 'yyyyMMdd'),'EEEE') = 'Sunday'\n",
    "        \"\"\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7ba00397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,concat, lpad, \\\n",
    "                                    date_format, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a5cde5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34708"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    withColumn(\"FlightDate\", \n",
    "        concat(\n",
    "            col(\"year\"),\n",
    "            lpad(col(\"Month\"),2,\"0\"),\n",
    "            lpad(col(\"DayOfMonth\"),2,\"0\")\n",
    "            )\n",
    "        ). \\\n",
    "    filter((col(\"IsDepDelayed\") == 'YES') & \n",
    "        (date_format(\n",
    "            to_date((\"FlightDate\"), 'yyyyMMdd'),'EEEE')\n",
    "             == 'Sunday')\n",
    "        ). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a09f64",
   "metadata": {},
   "source": [
    "### 188 Boolean Operators while filtering from Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93afb36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse/'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config(\"spark.driver.memory\", '4g'). \\\n",
    "        config('spark.executor.memory', '4g'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Basic Transformations'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889eab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = '/public/airlines_all/airlines-part/flightmonth=200801'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc06dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99b3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = '/public/airlines_all/airlines-part/flightmonth=200801'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629a0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a6573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|IsDepDelayed|IsArrDelayed|Cancelled|\n",
      "+------------+------------+---------+\n",
      "|          NO|          NO|        0|\n",
      "|         YES|         YES|        1|\n",
      "|          NO|         YES|        0|\n",
      "|         YES|          NO|        0|\n",
      "|         YES|         YES|        0|\n",
      "+------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('IsDepDelayed','IsArrDelayed','Cancelled'). \\\n",
    "    distinct(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "208c2d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('IsDepDelayed','IsArrDelayed','Cancelled'). \\\n",
    "    distinct(). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd2aab67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54233"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter(\"IsDepDelayed='YES' AND IsArrDelayed='NO' AND Cancelled=0\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85a3dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffb55d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54233"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter((col(\"IsDepDelayed\")=='YES') &\n",
    "           (col(\"IsArrDelayed\")=='NO') & \n",
    "           (col(\"Cancelled\")==0)\n",
    "          ). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b56682c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54233"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter((airtraffic.IsDepDelayed=='YES') &\n",
    "           (airtraffic.IsArrDelayed=='NO') & \n",
    "           (airtraffic.Cancelled==0)\n",
    "          ). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "566b7ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|IsDepDelayed|IsArrDelayed|Cancelled|\n",
      "+------------+------------+---------+\n",
      "|          NO|          NO|        0|\n",
      "|         YES|         YES|        1|\n",
      "|          NO|         YES|        0|\n",
      "|         YES|          NO|        0|\n",
      "|         YES|         YES|        0|\n",
      "+------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('IsDepDelayed','IsArrDelayed','Cancelled'). \\\n",
    "    distinct(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4713b92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20705"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter(\"IsDepDelayed='NO' AND ArrDelay>=15\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b38d60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20705"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter(\"IsDepDelayed='NO' AND ArrDelay>=15 AND Cancelled=0\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd4a2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a737375b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20705"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter((airtraffic.IsDepDelayed=='NO') &  (airtraffic.ArrDelay>=15)). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a41f7f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20705"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter((airtraffic[\"IsDepDelayed\"]=='NO') &  (airtraffic[\"ArrDelay\"]>=15)). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b37df1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lpad, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcd3de62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+----------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|IsArrDelayed|IsDepDelayed|FlightDate|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+----------+\n",
      "|2008|    1|        16|        3|   1725|      1735|   1959|      2021|           OH|     5367| N716CA|              154|           166|    146|     -22|     -10|   BGR| CVG|     906|     1|      7|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|  20080116|\n",
      "|2008|    1|        17|        4|   1717|      1701|   1915|      1855|           OH|     4977| N967CA|              118|           114|    101|      20|      16|   SYR| CVG|     527|     2|     15|        0|            null|       0|          16|           0|       4|            0|                0|         YES|         YES|  20080117|\n",
      "|2008|    1|        17|        4|   1220|      1225|   1440|      1504|           OH|     5352| N709CA|              140|           159|    117|     -24|      -5|   SAV| BOS|     901|     8|     15|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|  20080117|\n",
      "|2008|    1|        17|        4|   1530|      1530|   1645|      1637|           OH|     5426| N779CA|               75|            67|     45|       8|       0|   CVG| GRR|     268|     5|     25|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|         YES|          NO|  20080117|\n",
      "|2008|    1|        17|        4|   1203|      1205|   1429|      1429|           OH|     5441| N809CA|               86|            84|     58|       0|      -2|   STL| CVG|     307|     3|     25|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|  20080117|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    withColumn(\"FlightDate\",\n",
    "        concat(\n",
    "            col(\"Year\"),\n",
    "            lpad(col(\"Month\"),2,\"0\"),\n",
    "            lpad(col(\"DayOfMonth\"),2,\"0\")\n",
    "        )\n",
    "    ). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c18d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e674438",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l,'dummy STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5c560fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa5ba2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2024-02-12|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "df.select(current_date()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd9a1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2b546cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|current_date()|day_name|\n",
      "+--------------+--------+\n",
      "|    2024-02-12|     Mon|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date(),date_format(current_date(),'EE').alias('day_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f1aa4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|current_date()|day_name|\n",
      "+--------------+--------+\n",
      "|    2024-02-12|  Monday|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date(),date_format(current_date(),'EEEE').alias('day_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97abba7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57873"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat, lpad, to_date, date_format, current_date\n",
    "\n",
    "airtraffic. \\\n",
    "    withColumn(\"FlightDate\",\n",
    "        concat(\n",
    "            col(\"Year\"),\n",
    "            lpad(col(\"Month\"),2,\"0\"),\n",
    "            lpad(col(\"DayOfMonth\"),2,\"0\")\n",
    "        )\n",
    "    ). \\\n",
    "    filter(\"\"\"\n",
    "        IsDepDelayed = 'YES' AND Cancelled=0 AND\n",
    "        (date_format(to_date(FlightDate,'yyyyMMdd'),'EEEE')='Saturday'\n",
    "            OR date_format(to_date(FlightDate,'yyyyMMdd'),'EEEE')='Sunday'\n",
    "            )\n",
    "    \"\"\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39713266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57873"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    withColumn(\"FlightDate\",\n",
    "        concat(\n",
    "            col(\"Year\"),\n",
    "            lpad(col(\"Month\"),2,\"0\"),\n",
    "            lpad(col(\"DayOfMonth\"),2,\"0\")\n",
    "        )\n",
    "    ). \\\n",
    "    filter(\n",
    "        (col(\"IsDepDelayed\") == 'YES') & \n",
    "        (col(\"Cancelled\")==0) &\n",
    "        ((date_format(to_date(col(\"FlightDate\"),'yyyyMMdd'),'EEEE')=='Saturday')\n",
    "            | (date_format(to_date(col(\"FlightDate\"),'yyyyMMdd'),'EEEE')=='Sunday')\n",
    "            )\n",
    "    ). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "deadb88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57873"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    withColumn(\"FlightDate\",\n",
    "        concat(\n",
    "            col(\"Year\"),\n",
    "            lpad(col(\"Month\"),2,\"0\"),\n",
    "            lpad(col(\"DayOfMonth\"),2,\"0\")\n",
    "        )\n",
    "    ). \\\n",
    "    filter(\n",
    "        (airtraffic.IsDepDelayed == 'YES') & \n",
    "        (airtraffic.Cancelled==0) &\n",
    "        ((date_format(to_date(col(\"FlightDate\"),'yyyyMMdd'),'EEEE')=='Saturday')\n",
    "            | (date_format(to_date(col(\"FlightDate\"),'yyyyMMdd'),'EEEE')=='Sunday')\n",
    "            )\n",
    "    ). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d80bab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 189 Using IN Operator or isin Function while filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d8318ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse/'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config(\"spark.driver.memory\", '4g'). \\\n",
    "        config('spark.executor.memory', '4g'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Basic Transformations'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d5c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = '/public/airlines_all/airlines-part/flightmonth=200801'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f655d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22610a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fec937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|IsDepDelayed|IsArrDelayed|Cancelled|\n",
      "+------------+------------+---------+\n",
      "|          NO|          NO|        0|\n",
      "|         YES|         YES|        1|\n",
      "|          NO|         YES|        0|\n",
      "|         YES|          NO|        0|\n",
      "|         YES|         YES|        0|\n",
      "+------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('IsDepDelayed','IsArrDelayed','Cancelled'). \\\n",
    "    distinct(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4fb0380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118212"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter(\"Origin IN ('ORD','DFW', 'ATL','LAX','SFO')\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78e68eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605659"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5589dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08d36b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62457db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l,\"dummy STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b94ef5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e543e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = col('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03dde6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method isin in module pyspark.sql.column:\n",
      "\n",
      "isin(*cols) method of pyspark.sql.column.Column instance\n",
      "    A boolean expression that is evaluated to true if the value of this\n",
      "    expression is contained by the evaluated values of the arguments.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n",
      "    [Row(age=5, name='Bob')]\n",
      "    >>> df[df.age.isin([1, 2, 3])].collect()\n",
      "    [Row(age=2, name='Alice')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(c.isin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc9fedec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118212"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter(col(\"Origin\").isin('ORD','DFW', 'ATL','LAX','SFO')). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6635e3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118212"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter(airtraffic.Origin.isin('ORD','DFW', 'ATL','LAX','SFO')). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5e6bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,date_format, to_date, concat,lpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0d04f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57873"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    withColumn(\"FlightDate\",\n",
    "        concat(\n",
    "            col(\"Year\"),\n",
    "            lpad(col(\"Month\"),2,\"0\"),\n",
    "            lpad(col(\"DayOfMonth\"),2,\"0\")\n",
    "        )\n",
    "    ). \\\n",
    "    filter(\n",
    "        \"\"\"\n",
    "            IsDepDelayed='YES' AND Cancelled=0 AND\n",
    "            date_format(to_date(FlightDate,'yyyyMMdd'),'EEEE') IN ('Saturday','Sunday')\n",
    "        \"\"\"\n",
    "    ). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7d0a649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57873"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    withColumn(\"FlightDate\",\n",
    "        concat(\n",
    "            col(\"Year\"),\n",
    "            lpad(col(\"Month\"),2,\"0\"),\n",
    "            lpad(col(\"DayOfMonth\"),2,\"0\")\n",
    "        )\n",
    "    ). \\\n",
    "    filter((col(\"IsDepDelayed\")=='YES') &\n",
    "           (col(\"Cancelled\")==0) &\n",
    "           (date_format(to_date(col(\"FlightDate\"),'yyyyMMdd'),'EEEE').\n",
    "            isin('Saturday','Sunday'))\n",
    "    ). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a37b6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 190 Using LIKE operator or Like function while filtering from Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d09d2ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse/'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config(\"spark.driver.memory\", '4g'). \\\n",
    "        config('spark.executor.memory', '4g'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Basic Transformations'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e66ad5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = '/public/airlines_all/airlines-part/flightmonth=200801'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a1a90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54b79914",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f812e4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|IsDepDelayed|IsArrDelayed|Cancelled|\n",
      "+------------+------------+---------+\n",
      "|          NO|          NO|        0|\n",
      "|         YES|         YES|        1|\n",
      "|          NO|         YES|        0|\n",
      "|         YES|          NO|        0|\n",
      "|         YES|         YES|        0|\n",
      "+------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('IsDepDelayed','IsArrDelayed','Cancelled'). \\\n",
    "    distinct(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a617633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, 10,\n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, None,\n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, '',\n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, 10,\n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d1520a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees, \n",
    "        schema=\"\"\"employee_id INT, first_name STRING, \n",
    "        last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n",
    "        phone_number STRING, ssn STRING\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf56dd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad1307f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|   phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states|+1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"first_name LIKE 'Sco%'\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "507229b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|   phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states|+1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"UPPER(first_name) LIKE 'SCO%'\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3cd3457a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|   phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states|+1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(col('first_name').like('Sco%')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e7b0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = col('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "216385e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method _ in module pyspark.sql.column:\n",
      "\n",
      "_(other) method of pyspark.sql.column.Column instance\n",
      "    SQL like expression. Returns a boolean :class:`Column` based on a SQL LIKE match.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    other : str\n",
      "        a SQL LIKE pattern\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    pyspark.sql.Column.rlike\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.filter(df.name.like('Al%')).collect()\n",
      "    [Row(age=2, name='Alice')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(c.like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db8f88c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|   phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states|+1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "employeesDF. \\\n",
    "    filter(upper(col('first_name')).like('SCO%')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f657b28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|   phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states|+1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"UPPER(first_name) LIKE '%OT%'\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69405846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|   phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states|+1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(upper(col(\"first_name\")).like(\"%OT%\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b35eac6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"UPPER(first_name) NOT LIKE '%OT%'\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c95a8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|   phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states|+1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+-----+-------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(upper(col(\"first_name\")).like('%OT%')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab2aee59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(~(upper(col(\"first_name\")).like('%OT%'))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb0b94fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|        India|+91 234 567 8901|456 78 9123|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|    AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"phone_number NOT LIKE '%44%'\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04cd325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter((col(\"phone_number\").like('%44%'))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "381e9280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|        India|+91 234 567 8901|456 78 9123|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|    AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(~(col(\"phone_number\").like('%44%'))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0b573c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 191 Using Between Operator while filtering from Spark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c58042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse/'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config(\"spark.driver.memory\", '4g'). \\\n",
    "        config('spark.executor.memory', '4g'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Basic Transformations'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0527a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = '/public/airlines_all/airlines-part/flightmonth=200801'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4b7b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3be9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d62fb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|IsDepDelayed|IsArrDelayed|Cancelled|\n",
      "+------------+------------+---------+\n",
      "|          NO|          NO|        0|\n",
      "|         YES|         YES|        1|\n",
      "|          NO|         YES|        0|\n",
      "|         YES|          NO|        0|\n",
      "|         YES|         YES|        0|\n",
      "+------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('IsDepDelayed','IsArrDelayed','Cancelled'). \\\n",
    "    distinct(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43cfb8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.appUIAddress', 'http://g02.itversity.com:46093'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.sql.repl.eagerEval.enabled', 'true'),\n",
       " ('spark.eventLog.dir', 'hdfs:///spark-logs'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1707552082651_2452'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '10'),\n",
       " ('spark.app.startTime', '1707811962469'),\n",
       " ('spark.app.name', 'itv011204 | Python - Basic Transformations'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://m02.itversity.com:19088/proxy/application_1707552082651_2452'),\n",
       " ('spark.shuffle.io.connectionTimeout', '6000'),\n",
       " ('spark.driver.port', '32903'),\n",
       " ('spark.app.id', 'application_1707552082651_2452'),\n",
       " ('spark.yarn.historyServer.address', 'm02.itversity.com:18080'),\n",
       " ('spark.yarn.jars', ''),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///spark-logs'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.history.fs.update.interval', '10s'),\n",
       " ('spark.driver.extraJavaOptions', '-Dderby.system.home=/tmp/derby/'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'm02.itversity.com'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.sql.warehouse.dir', '/user/itv011204/warehouse/'),\n",
       " ('spark.executor.extraLibraryPath', '/opt/hadoop/lib/native'),\n",
       " ('spark.history.ui.port', '18080'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.driver.host', 'g02.itversity.com'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.dynamicAllocation.minExecutors', '2'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.history.fs.cleaner.enabled', 'true'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip:/opt/spark-3.1.2-bin-hadoop3.2/python<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.ui.port', '0'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa8335d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+----------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|IsArrDelayed|IsDepDelayed|FlightDate|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+----------+\n",
      "|2008|    1|        16|        3|   1725|      1735|   1959|      2021|           OH|     5367| N716CA|              154|           166|    146|     -22|     -10|   BGR| CVG|     906|     1|      7|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|  20080116|\n",
      "|2008|    1|        17|        4|   1717|      1701|   1915|      1855|           OH|     4977| N967CA|              118|           114|    101|      20|      16|   SYR| CVG|     527|     2|     15|        0|            null|       0|          16|           0|       4|            0|                0|         YES|         YES|  20080117|\n",
      "|2008|    1|        17|        4|   1220|      1225|   1440|      1504|           OH|     5352| N709CA|              140|           159|    117|     -24|      -5|   SAV| BOS|     901|     8|     15|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|  20080117|\n",
      "|2008|    1|        17|        4|   1530|      1530|   1645|      1637|           OH|     5426| N779CA|               75|            67|     45|       8|       0|   CVG| GRR|     268|     5|     25|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|         YES|          NO|  20080117|\n",
      "|2008|    1|        17|        4|   1203|      1205|   1429|      1429|           OH|     5441| N809CA|               86|            84|     58|       0|      -2|   STL| CVG|     307|     3|     25|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|          NO|          NO|  20080117|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+------------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lpad, concat, col\n",
    "\n",
    "airtraffic. \\\n",
    "    withColumn(\"FlightDate\",\n",
    "        concat(col(\"Year\"),\n",
    "              lpad(col(\"Month\"),2,\"0\"),\n",
    "              lpad(col(\"DayOfMonth\"),2,\"0\"))\n",
    "              ). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6c17f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86180"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    withColumn(\"FlightDate\",\n",
    "        concat(col(\"Year\"),\n",
    "              lpad(col(\"Month\"),2,\"0\"),\n",
    "              lpad(col(\"DayOfMonth\"),2,\"0\"))\n",
    "              ). \\\n",
    "    filter(\"\"\"\n",
    "        IsDepDelayed = 'YES' AND\n",
    "        Cancelled = 0 AND\n",
    "        FlightDate BETWEEN 20080101 AND 20080109\n",
    "        \"\"\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21407a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86180"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    withColumn(\"FlightDate\",\n",
    "        concat(col(\"Year\"),\n",
    "              lpad(col(\"Month\"),2,\"0\"),\n",
    "              lpad(col(\"DayOfMonth\"),2,\"0\"))\n",
    "              ). \\\n",
    "    filter(\"\"\"\n",
    "        IsDepDelayed = 'YES' AND\n",
    "        Cancelled = 0 AND\n",
    "        FlightDate >= 20080101 AND \n",
    "        FlightDate <= 20080109\n",
    "        \"\"\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aff29412",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = col('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ef0b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method between in module pyspark.sql.column:\n",
      "\n",
      "between(lowerBound, upperBound) method of pyspark.sql.column.Column instance\n",
      "    A boolean expression that is evaluated to true if the value of this\n",
      "    expression is between the given columns.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.select(df.name, df.age.between(2, 4)).show()\n",
      "    +-----+---------------------------+\n",
      "    | name|((age >= 2) AND (age <= 4))|\n",
      "    +-----+---------------------------+\n",
      "    |Alice|                       true|\n",
      "    |  Bob|                      false|\n",
      "    +-----+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(c.between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc3dd134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86180"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    withColumn(\"FlightDate\",\n",
    "        concat(col(\"Year\"),\n",
    "              lpad(col(\"Month\"),2,\"0\"),\n",
    "              lpad(col(\"DayOfMonth\"),2,\"0\"))\n",
    "              ). \\\n",
    "    filter(\n",
    "        (col(\"IsDepDelayed\")=='YES') &\n",
    "        (col(\"Cancelled\")==0) &\n",
    "        (col(\"FlightDate\").between(20080101, 20080109))\n",
    "    ). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dfb9701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86180"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    withColumn(\"FlightDate\",\n",
    "        concat(col(\"Year\"),\n",
    "              lpad(col(\"Month\"),2,\"0\"),\n",
    "              lpad(col(\"DayOfMonth\"),2,\"0\"))\n",
    "              ). \\\n",
    "    filter(\n",
    "        (col(\"IsDepDelayed\")=='YES') &\n",
    "        (col(\"Cancelled\")==0) &\n",
    "        (col(\"FlightDate\")>=20080101) &\n",
    "        (col(\"FlightDate\")<=20080109)\n",
    "    ). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02afd077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105319"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter((col(\"ArrDelay\").between(15,60))). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "044f4e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105319"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter(\"ArrDelay BETWEEN 15 AND 60\"). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bdb4724",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 192 Dealing with NULLs while Filtering from Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "49051d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse/'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.network.timeout','6000'). \\\n",
    "        config('spark.executor.heartbeatInterval','20s'). \\\n",
    "        config(\"spark.driver.memory\", '4g'). \\\n",
    "        config('spark.executor.memory', '4g'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Basic Transformations'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4fce8bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = '/public/airlines_all/airlines-part/flightmonth=200801'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3370ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ab69a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "73694353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|IsDepDelayed|IsArrDelayed|Cancelled|\n",
      "+------------+------------+---------+\n",
      "|          NO|          NO|        0|\n",
      "|         YES|         YES|        1|\n",
      "|          NO|         YES|        0|\n",
      "|         YES|          NO|        0|\n",
      "|         YES|         YES|        0|\n",
      "+------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('IsDepDelayed','IsArrDelayed','Cancelled'). \\\n",
    "    distinct(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b3cdc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, 10,\n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, None,\n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, '',\n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, 10,\n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d72fb57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b035d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "51f52d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-----------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-----------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|      India|+91 234 567 8901|456 78 9123|\n",
      "+-----------+----------+---------+------+-----+-----------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"bonus IS NULL\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "634aea9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"bonus IS NOT NULL\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9069a9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"bonus = ''\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d91aa5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states| +1 123 456 7890|123 45 6789|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|    AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"bonus <> ''\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bbe8bbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states| +1 123 456 7890|123 45 6789|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|    AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"bonus != ''\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3383a63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states| +1 123 456 7890|123 45 6789|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|    AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"bonus IS NOT NULL AND bonus != ''\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9a056fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"bonus IS NULL OR BONUS = ''\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7f305f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states| +1 123 456 7890|123 45 6789|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|    AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"!(bonus IS NULL OR BONUS = '')\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf076079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "64f6e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=col('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e03e4226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNotNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "True if the current expression is NOT null.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from pyspark.sql import Row\n",
       ">>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
       ">>> df.filter(df.height.isNotNull()).collect()\n",
       "[Row(name='Tom', height=80)]\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/column.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c.isNotNull?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b1ab877c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-----------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-----------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|      India|+91 234 567 8901|456 78 9123|\n",
      "+-----------+----------+---------+------+-----+-----------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(col(\"bonus\").isNull()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ecca2712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(col(\"bonus\")==''). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4d28e717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(col(\"bonus\").isNotNull()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "17db47e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states| +1 123 456 7890|123 45 6789|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|    AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter((col(\"bonus\").isNotNull()) & (col(\"bonus\")!='') ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "758bc14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(~((col(\"bonus\").isNotNull()) & (col(\"bonus\")!='')) ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f35af462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-----------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-----------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|      India|+91 234 567 8901|456 78 9123|\n",
      "+-----------+----------+---------+------+-----+-----------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"bonus IS NULL\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "783aef69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"bonus = ''\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "591b6632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"bonus IS NULL OR bonus=''\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "84b2bf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"nullif(bonus,'') IS NULL\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5290886f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states| +1 123 456 7890|123 45 6789|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|    AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(\"nullif(bonus,'') IS NOT NULL\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "794f7d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(col(\"bonus\").cast('int').isNull()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "54909a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|  nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10|united states| +1 123 456 7890|123 45 6789|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|    AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+-------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    filter(~(col(\"bonus\").cast('int').isNull())). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "507d293e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.sql.repl.eagerEval.enabled', 'true'),\n",
       " ('spark.eventLog.dir', 'hdfs:///spark-logs'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '10'),\n",
       " ('spark.app.name', 'itv011204 | Python - Basic Transformations'),\n",
       " ('spark.shuffle.io.connectionTimeout', '6000'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://m02.itversity.com:19088/proxy/application_1707552082651_2466'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1707552082651_2466'),\n",
       " ('spark.yarn.historyServer.address', 'm02.itversity.com:18080'),\n",
       " ('spark.driver.appUIAddress', 'http://g02.itversity.com:33795'),\n",
       " ('spark.yarn.jars', ''),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///spark-logs'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.history.fs.update.interval', '10s'),\n",
       " ('spark.driver.extraJavaOptions', '-Dderby.system.home=/tmp/derby/'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'm02.itversity.com'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.app.id', 'application_1707552082651_2466'),\n",
       " ('spark.sql.warehouse.dir', '/user/itv011204/warehouse/'),\n",
       " ('spark.executor.extraLibraryPath', '/opt/hadoop/lib/native'),\n",
       " ('spark.history.ui.port', '18080'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.driver.host', 'g02.itversity.com'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.dynamicAllocation.minExecutors', '2'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.startTime', '1707813974322'),\n",
       " ('spark.history.fs.cleaner.enabled', 'true'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip:/opt/spark-3.1.2-bin-hadoop3.2/python<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.ui.port', '0'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.driver.port', '33865'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "33e55774",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 193 Total Aggregations on Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7072ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## count, sum, avg, min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d60c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse/'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config(\"spark.driver.memory\", '4g'). \\\n",
    "        config('spark.executor.memory', '4g'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Basic Transformations'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()\n",
    "\n",
    "#        config('spark.network.timeout','6000'). \\\n",
    "#        config('spark.executor.heartbeatInterval','20s'). \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8854fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = '/public/airlines_all/airlines-part/flightmonth=200801'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59953a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a11670",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d128701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|IsDepDelayed|IsArrDelayed|Cancelled|\n",
      "+------------+------------+---------+\n",
      "|          NO|          NO|        0|\n",
      "|         YES|         YES|        1|\n",
      "|          NO|         YES|        0|\n",
      "|         YES|          NO|        0|\n",
      "|         YES|         YES|        0|\n",
      "+------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('IsDepDelayed','IsArrDelayed','Cancelled'). \\\n",
    "    distinct(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c46a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605659"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d05bbb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(airtraffic.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a43bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cddca21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| count|\n",
      "+------+\n",
      "|605659|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select(count(\"*\").alias(\"count\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "707026a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| count|\n",
      "+------+\n",
      "|605659|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "airtraffic. \\\n",
    "    select(count(lit(1)).alias(\"count\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f8ba901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+------------------+\n",
      "|summary|  Year| Month|        DayOfMonth|\n",
      "+-------+------+------+------------------+\n",
      "|  count|605659|605659|            605659|\n",
      "|   mean|2008.0|   1.0|15.908469947610785|\n",
      "| stddev|   0.0|   0.0| 8.994294747375292|\n",
      "|    min|  2008|     1|                 1|\n",
      "|    max|  2008|     1|                31|\n",
      "+-------+------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('Year','Month','DayOfMonth'). \\\n",
    "    describe(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3f3a836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+------------------+\n",
      "|summary|  Year| Month|        DayOfMonth|\n",
      "+-------+------+------+------------------+\n",
      "|  count|605659|605659|            605659|\n",
      "|   mean|2008.0|   1.0|15.908469947610785|\n",
      "| stddev|   0.0|   0.0| 8.994294747375292|\n",
      "|    min|  2008|     1|                 1|\n",
      "|    25%|  2008|     1|                 8|\n",
      "|    50%|  2008|     1|                16|\n",
      "|    75%|  2008|     1|                24|\n",
      "|    max|  2008|     1|                31|\n",
      "+-------+------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('Year','Month','DayOfMonth'). \\\n",
    "    summary(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce087038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('Year', 'Month', 'DayOfMonth'). \\\n",
    "    distinct(). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f2858b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9eb4cef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|countDistinct|\n",
      "+-------------+\n",
      "|           31|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select(countDistinct('Year', 'Month', 'DayOfMonth').alias('countDistinct')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc409b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lpad, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "507a03da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|countDistinct|\n",
      "+-------------+\n",
      "|           31|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select(countDistinct(\n",
    "        concat('Year',\n",
    "              lpad('Month',2,'0'),\n",
    "              lpad('DayOfMonth',2,'0')\n",
    "              )).alias('countDistinct')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "565fc20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, 10,\n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, None,\n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, '',\n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, 10,\n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8569761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1822836d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e34d079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, col, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23c2d8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_bonus|\n",
      "+-----------+\n",
      "|      250.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(((sum(coalesce(col('bonus').cast('int'),lit(0)) * col('salary')))/lit(100)).alias('total_bonus')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18020e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_bonus|\n",
      "+-----------+\n",
      "|      250.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    selectExpr('sum((coalesce(cast(bonus AS INT),0) * salary)/100) AS total_bonus'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5fd21b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = spark.read.json(\"/public/retail_db_json/order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78940b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|            1|                  1|                  957|                  299.98|                  1|             299.98|\n",
      "|            2|                  2|                 1073|                  199.99|                  1|             199.99|\n",
      "|            3|                  2|                  502|                    50.0|                  5|              250.0|\n",
      "|            4|                  2|                  403|                  129.99|                  1|             129.99|\n",
      "|            5|                  4|                  897|                   24.99|                  2|              49.98|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8821371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter order_id 2\n"
     ]
    }
   ],
   "source": [
    "order_id = input(\"Enter order_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a32a9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36167517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|order_revenue|\n",
      "+-------------+\n",
      "|       579.98|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items. \\\n",
    "    filter(f'order_item_order_id=={int(order_id)}'). \\\n",
    "    select(sum(\"order_item_subtotal\").alias('order_revenue')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2611d78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|            2|                  2|                 1073|                  199.99|                  1|             199.99|\n",
      "|            3|                  2|                  502|                    50.0|                  5|              250.0|\n",
      "|            4|                  2|                  403|                  129.99|                  1|             129.99|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items. \\\n",
    "    filter(f'order_item_order_id=={int(order_id)}'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ab90f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|            2|                  2|                 1073|                  199.99|                  1|             199.99|\n",
      "|            3|                  2|                  502|                    50.0|                  5|              250.0|\n",
      "|            4|                  2|                  403|                  129.99|                  1|             129.99|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items. \\\n",
    "    filter(col(\"order_item_order_id\")==lit(int(order_id))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3790ed0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|order_revenue|\n",
      "+-------------+\n",
      "|       579.98|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items. \\\n",
    "    filter(col(\"order_item_order_id\")==lit(int(order_id))). \\\n",
    "    select(sum(col('order_item_subtotal')).alias('order_revenue')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020c1b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 194 Aggregate data using groupBy from Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1622a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse/'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.network.timeout','6000'). \\\n",
    "        config('spark.executor.heartbeatInterval','20s'). \\\n",
    "        config(\"spark.driver.memory\", '4g'). \\\n",
    "        config('spark.executor.memory', '4g'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Basic Transformations'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3285afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = '/public/airlines_all/airlines-part/flightmonth=200801'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1b45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f019c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c979800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|IsDepDelayed|IsArrDelayed|Cancelled|\n",
      "+------------+------------+---------+\n",
      "|          NO|          NO|        0|\n",
      "|         YES|         YES|        1|\n",
      "|          NO|         YES|        0|\n",
      "|         YES|          NO|        0|\n",
      "|         YES|         YES|        0|\n",
      "+------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('IsDepDelayed','IsArrDelayed','Cancelled'). \\\n",
    "    distinct(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90af861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "326c9cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|FlightDate|count|\n",
      "+----------+-----+\n",
      "|  20080101|19175|\n",
      "|  20080102|20953|\n",
      "|  20080103|20937|\n",
      "|  20080104|20929|\n",
      "|  20080105|18066|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            'Year',\n",
    "            lpad('Month',2,'0'),\n",
    "            lpad('DayOfMonth',2,'0')\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    count(). \\\n",
    "    sort('FlightDate'). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e257b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, concat, lpad, lit, sum, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9f162b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FlightDate|FlightCount|\n",
      "+----------+-----------+\n",
      "|  20080120|      18653|\n",
      "|  20080130|      19766|\n",
      "|  20080115|      19503|\n",
      "|  20080118|      20347|\n",
      "|  20080122|      19504|\n",
      "+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            'Year',\n",
    "            lpad('Month',2,'0'),\n",
    "            lpad('DayOfMonth',2,'0')\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e5749a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+------------------+\n",
      "|FlightDate|FlightCount|TotalDepDelay|   AverageDepDelay|\n",
      "+----------+-----------+-------------+------------------+\n",
      "|  20080120|      18406|     117460.0| 6.381614690861675|\n",
      "|  20080130|      19072|     129345.0| 6.781931627516778|\n",
      "|  20080115|      19204|      75096.0|3.9104353259737556|\n",
      "|  20080118|      20117|     223738.0|11.121837252075359|\n",
      "|  20080122|      18716|     303796.0| 16.23188715537508|\n",
      "|  20080104|      20160|     277373.0|13.758581349206349|\n",
      "|  20080125|      19787|     229850.0|11.616212664880983|\n",
      "|  20080102|      20442|     452979.0|22.159230995010272|\n",
      "|  20080105|      17610|     306068.0|17.380352072685973|\n",
      "|  20080111|      19825|     190918.0|  9.63016393442623|\n",
      "|  20080109|      19443|      89595.0| 4.608085172041352|\n",
      "|  20080127|      18265|     365491.0|20.010457158499865|\n",
      "|  20080101|      18623|     354108.0| 19.01455189819041|\n",
      "|  20080128|      19493|     220046.0|11.288462525008978|\n",
      "|  20080119|      15373|     155488.0|10.114356339035972|\n",
      "|  20080106|      19210|     323214.0| 16.82529932326913|\n",
      "|  20080123|      19239|     190807.0| 9.917719216175477|\n",
      "|  20080117|      19401|     341271.0|17.590381939075307|\n",
      "|  20080116|      19232|      61021.0| 3.172888935108153|\n",
      "|  20080112|      16346|      24876.0|1.5218402055548759|\n",
      "+----------+-----------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('Cancelled=0'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            'Year',\n",
    "            lpad('Month',2,'0'),\n",
    "            lpad('DayOfMonth',2,'0')\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(\n",
    "        count(lit(1)).alias('FlightCount'),\n",
    "        sum('DepDelay').alias('TotalDepDelay'),\n",
    "        avg('DepDelay').alias('AverageDepDelay')\n",
    "    ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03a0c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e5d88f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+---------------+\n",
      "|FlightDate|FlightCount|TotalDepDelay|AverageDepDelay|\n",
      "+----------+-----------+-------------+---------------+\n",
      "|  20080120|      18406|     117460.0|           6.38|\n",
      "|  20080130|      19072|     129345.0|           6.78|\n",
      "|  20080115|      19204|      75096.0|           3.91|\n",
      "|  20080118|      20117|     223738.0|          11.12|\n",
      "|  20080122|      18716|     303796.0|          16.23|\n",
      "|  20080104|      20160|     277373.0|          13.76|\n",
      "|  20080125|      19787|     229850.0|          11.62|\n",
      "|  20080102|      20442|     452979.0|          22.16|\n",
      "|  20080105|      17610|     306068.0|          17.38|\n",
      "|  20080111|      19825|     190918.0|           9.63|\n",
      "|  20080109|      19443|      89595.0|           4.61|\n",
      "|  20080127|      18265|     365491.0|          20.01|\n",
      "|  20080101|      18623|     354108.0|          19.01|\n",
      "|  20080128|      19493|     220046.0|          11.29|\n",
      "|  20080119|      15373|     155488.0|          10.11|\n",
      "|  20080106|      19210|     323214.0|          16.83|\n",
      "|  20080123|      19239|     190807.0|           9.92|\n",
      "|  20080117|      19401|     341271.0|          17.59|\n",
      "|  20080116|      19232|      61021.0|           3.17|\n",
      "|  20080112|      16346|      24876.0|           1.52|\n",
      "+----------+-----------+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('Cancelled=0'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            'Year',\n",
    "            lpad('Month',2,'0'),\n",
    "            lpad('DayOfMonth',2,'0')\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(\n",
    "        count(lit(1)).alias('FlightCount'),\n",
    "        sum('DepDelay').alias('TotalDepDelay'),\n",
    "        round(avg('DepDelay'),2).alias('AverageDepDelay')\n",
    "    ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a706584b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+---------------+\n",
      "|FlightDate|FlightCount|TotalDepDelay|AverageDepDelay|\n",
      "+----------+-----------+-------------+---------------+\n",
      "|  20080101|      18623|     354108.0|          19.01|\n",
      "|  20080102|      20442|     452979.0|          22.16|\n",
      "|  20080103|      20462|     329690.0|          16.11|\n",
      "|  20080104|      20160|     277373.0|          13.76|\n",
      "|  20080105|      17610|     306068.0|          17.38|\n",
      "|  20080106|      19210|     323214.0|          16.83|\n",
      "|  20080107|      19762|     238431.0|          12.07|\n",
      "|  20080108|      19140|     200670.0|          10.48|\n",
      "|  20080109|      19443|      89595.0|           4.61|\n",
      "|  20080110|      19956|     148603.0|           7.45|\n",
      "|  20080111|      19825|     190918.0|           9.63|\n",
      "|  20080112|      16346|      24876.0|           1.52|\n",
      "|  20080113|      18587|     101753.0|           5.47|\n",
      "|  20080114|      19267|      98261.0|            5.1|\n",
      "|  20080115|      19204|      75096.0|           3.91|\n",
      "|  20080116|      19232|      61021.0|           3.17|\n",
      "|  20080117|      19401|     341271.0|          17.59|\n",
      "|  20080118|      20117|     223738.0|          11.12|\n",
      "|  20080119|      15373|     155488.0|          10.11|\n",
      "|  20080120|      18406|     117460.0|           6.38|\n",
      "+----------+-----------+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('Cancelled=0'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            'Year',\n",
    "            lpad('Month',2,'0'),\n",
    "            lpad('DayOfMonth',2,'0')\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(\n",
    "        count(lit(1)).alias('FlightCount'),\n",
    "        sum('DepDelay').alias('TotalDepDelay'),\n",
    "        round(avg('DepDelay'),2).alias('AverageDepDelay')\n",
    "    ). \\\n",
    "    sort('FlightDate'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e533579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+---------------+\n",
      "|FlightDate|FlightCount|TotalDepDelay|AverageDepDelay|\n",
      "+----------+-----------+-------------+---------------+\n",
      "|  20080131|      19179|     396280.0|          20.66|\n",
      "|  20080130|      19072|     129345.0|           6.78|\n",
      "|  20080129|      18596|     184855.0|           9.94|\n",
      "|  20080128|      19493|     220046.0|          11.29|\n",
      "|  20080127|      18265|     365491.0|          20.01|\n",
      "|  20080126|      15860|      92129.0|           5.81|\n",
      "|  20080125|      19787|     229850.0|          11.62|\n",
      "|  20080124|      19935|     158134.0|           7.93|\n",
      "|  20080123|      19239|     190807.0|           9.92|\n",
      "|  20080122|      18716|     303796.0|          16.23|\n",
      "|  20080121|      19658|     370196.0|          18.83|\n",
      "|  20080120|      18406|     117460.0|           6.38|\n",
      "|  20080119|      15373|     155488.0|          10.11|\n",
      "|  20080118|      20117|     223738.0|          11.12|\n",
      "|  20080117|      19401|     341271.0|          17.59|\n",
      "|  20080116|      19232|      61021.0|           3.17|\n",
      "|  20080115|      19204|      75096.0|           3.91|\n",
      "|  20080114|      19267|      98261.0|            5.1|\n",
      "|  20080113|      18587|     101753.0|           5.47|\n",
      "|  20080112|      16346|      24876.0|           1.52|\n",
      "+----------+-----------+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('Cancelled=0'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            'Year',\n",
    "            lpad('Month',2,'0'),\n",
    "            lpad('DayOfMonth',2,'0')\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(\n",
    "        count(lit(1)).alias('FlightCount'),\n",
    "        sum('DepDelay').alias('TotalDepDelay'),\n",
    "        round(avg('DepDelay'),2).alias('AverageDepDelay')\n",
    "    ). \\\n",
    "    sort(col('FlightDate').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbffdd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_path = '/public/retail_db_json/order_items'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de2d84ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = spark.read.json(order_items_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e2b05b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- order_item_order_id: long (nullable = true)\n",
      " |-- order_item_product_id: long (nullable = true)\n",
      " |-- order_item_product_price: double (nullable = true)\n",
      " |-- order_item_quantity: long (nullable = true)\n",
      " |-- order_item_subtotal: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb2a1217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172198"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_items.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c270357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+\n",
      "|order_item_order_id|sum(order_item_subtotal)|\n",
      "+-------------------+------------------------+\n",
      "|                 29|                 1109.85|\n",
      "|                474|       774.8199999999999|\n",
      "|                964|       739.8800000000001|\n",
      "|               1677|       649.9200000000001|\n",
      "|               1806|                  789.94|\n",
      "|               1950|      1015.8700000000001|\n",
      "|               2214|                  449.96|\n",
      "|               2250|                  889.94|\n",
      "|               2453|       999.9300000000001|\n",
      "|               2509|                  889.94|\n",
      "|               2529|                   59.99|\n",
      "|               2927|       999.9100000000001|\n",
      "|               3091|      469.93000000000006|\n",
      "|               3764|                   95.98|\n",
      "|               4590|                  949.83|\n",
      "|               4894|                  899.94|\n",
      "|               5385|                  629.86|\n",
      "|               5409|       699.9200000000001|\n",
      "|               6721|                  139.99|\n",
      "|               7225|                  774.86|\n",
      "+-------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items. \\\n",
    "    groupBy(\"order_item_order_id\"). \\\n",
    "    sum(\"order_item_subtotal\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fd57936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|order_item_order_id| revenue_per_order|\n",
      "+-------------------+------------------+\n",
      "|                 29|           1109.85|\n",
      "|                474| 774.8199999999999|\n",
      "|                964| 739.8800000000001|\n",
      "|               1677| 649.9200000000001|\n",
      "|               1806|            789.94|\n",
      "|               1950|1015.8700000000001|\n",
      "|               2214|            449.96|\n",
      "|               2250|            889.94|\n",
      "|               2453| 999.9300000000001|\n",
      "|               2509|            889.94|\n",
      "|               2529|             59.99|\n",
      "|               2927| 999.9100000000001|\n",
      "|               3091|469.93000000000006|\n",
      "|               3764|             95.98|\n",
      "|               4590|            949.83|\n",
      "|               4894|            899.94|\n",
      "|               5385|            629.86|\n",
      "|               5409| 699.9200000000001|\n",
      "|               6721|            139.99|\n",
      "|               7225|            774.86|\n",
      "+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items. \\\n",
    "    groupBy(\"order_item_order_id\"). \\\n",
    "    agg(sum(\"order_item_subtotal\").alias(\"revenue_per_order\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2c444f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|order_item_order_id|revenue_per_order|\n",
      "+-------------------+-----------------+\n",
      "|                 29|          1109.85|\n",
      "|                474|           774.82|\n",
      "|                964|           739.88|\n",
      "|               1677|           649.92|\n",
      "|               1806|           789.94|\n",
      "|               1950|          1015.87|\n",
      "|               2214|           449.96|\n",
      "|               2250|           889.94|\n",
      "|               2453|           999.93|\n",
      "|               2509|           889.94|\n",
      "|               2529|            59.99|\n",
      "|               2927|           999.91|\n",
      "|               3091|           469.93|\n",
      "|               3764|            95.98|\n",
      "|               4590|           949.83|\n",
      "|               4894|           899.94|\n",
      "|               5385|           629.86|\n",
      "|               5409|           699.92|\n",
      "|               6721|           139.99|\n",
      "|               7225|           774.86|\n",
      "+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items. \\\n",
    "    groupBy(\"order_item_order_id\"). \\\n",
    "    agg(round(sum(\"order_item_subtotal\"),2).alias(\"revenue_per_order\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8374efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min,max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5d990c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-----------------------+-----------------------+\n",
      "|order_item_order_id|revenue_per_order|order_item_subtotal_min|order_item_subtotal_max|\n",
      "+-------------------+-----------------+-----------------------+-----------------------+\n",
      "|                 29|          1109.85|                 129.99|                 399.98|\n",
      "|                474|           774.82|                  24.99|                 199.99|\n",
      "|                964|           739.88|                 129.99|                 299.98|\n",
      "|               1677|           649.92|                   50.0|                  250.0|\n",
      "|               1806|           789.94|                  150.0|                 399.98|\n",
      "|               1950|          1015.87|                  87.96|                 399.98|\n",
      "|               2214|           449.96|                   50.0|                 399.96|\n",
      "|               2250|           889.94|                  59.99|                 399.98|\n",
      "|               2453|           999.93|                  100.0|                 299.98|\n",
      "|               2509|           889.94|                  59.99|                 399.98|\n",
      "|               2529|            59.99|                  59.99|                  59.99|\n",
      "|               2927|           999.91|                 119.98|                 299.98|\n",
      "|               3091|           469.93|                  49.98|                 299.98|\n",
      "|               3764|            95.98|                  95.98|                  95.98|\n",
      "|               4590|           949.83|                  99.99|                 399.98|\n",
      "|               4894|           899.94|                  99.99|                 299.98|\n",
      "|               5385|           629.86|                   70.0|                  249.9|\n",
      "|               5409|           699.92|                  99.99|                 299.98|\n",
      "|               6721|           139.99|                  39.99|                  100.0|\n",
      "|               7225|           774.86|                  74.97|                 299.98|\n",
      "+-------------------+-----------------+-----------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items. \\\n",
    "    groupBy('order_item_order_id'). \\\n",
    "    agg(\n",
    "        round(sum('order_item_subtotal'),2).alias('revenue_per_order'),\n",
    "        min('order_item_subtotal').alias('order_item_subtotal_min'),\n",
    "        max('order_item_subtotal').alias('order_item_subtotal_max')\n",
    "    ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85b84216",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 195 Aggregate data using rollup on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b3fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af78740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse/'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config(\"spark.driver.memory\", '6g'). \\\n",
    "        config('spark.executor.memory', '6g'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Basic Transformations'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()\n",
    "\n",
    "#        config('spark.executor.heartbeatInterval','20s'). \\\n",
    "#         config('spark.network.timeout','6000'). \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69ec2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = '/public/airlines_all/airlines-part/flightmonth=200801'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6bf391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf5eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e24e3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|IsDepDelayed|IsArrDelayed|Cancelled|\n",
      "+------------+------------+---------+\n",
      "|          NO|          NO|        0|\n",
      "|         YES|         YES|        1|\n",
      "|          NO|         YES|        0|\n",
      "|         YES|          NO|        0|\n",
      "|         YES|         YES|        0|\n",
      "+------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('IsDepDelayed','IsArrDelayed','Cancelled'). \\\n",
    "    distinct(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3218c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_path = '/public/retail_db_json/order_items'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "046aab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = spark.read.json(order_items_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a51b200c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172198"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_items.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a5b74e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- order_item_order_id: long (nullable = true)\n",
      " |-- order_item_product_id: long (nullable = true)\n",
      " |-- order_item_product_price: double (nullable = true)\n",
      " |-- order_item_quantity: long (nullable = true)\n",
      " |-- order_item_subtotal: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a5752f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.json('/public/retail_db_json/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc429bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a82e7e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eae5df35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          order_date|count|\n",
      "+--------------------+-----+\n",
      "|2013-08-13 00:00:...|   73|\n",
      "|2013-10-12 00:00:...|  162|\n",
      "|2013-11-15 00:00:...|  135|\n",
      "|2014-03-19 00:00:...|  130|\n",
      "|2014-04-26 00:00:...|  251|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy('order_date'). \\\n",
    "    count(). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03d6b206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|          order_date|order_count|\n",
      "+--------------------+-----------+\n",
      "|2013-08-13 00:00:...|         73|\n",
      "|2013-10-12 00:00:...|        162|\n",
      "|2013-11-15 00:00:...|        135|\n",
      "|2014-03-19 00:00:...|        130|\n",
      "|2014-04-26 00:00:...|        251|\n",
      "|2013-09-16 00:00:...|        121|\n",
      "|2013-09-20 00:00:...|        139|\n",
      "|2013-12-31 00:00:...|        266|\n",
      "|2013-09-06 00:00:...|        276|\n",
      "|2014-06-15 00:00:...|        128|\n",
      "|2013-12-24 00:00:...|        170|\n",
      "|2014-01-07 00:00:...|        163|\n",
      "|2014-06-07 00:00:...|        191|\n",
      "|2013-10-14 00:00:...|        139|\n",
      "|2013-11-11 00:00:...|        246|\n",
      "|2014-01-27 00:00:...|        163|\n",
      "|2014-01-29 00:00:...|        158|\n",
      "|2014-02-14 00:00:...|        174|\n",
      "|2014-04-15 00:00:...|        180|\n",
      "|2014-04-22 00:00:...|        144|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, lit\n",
    "\n",
    "orders. \\\n",
    "    groupBy('order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b953470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy('order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9acaa9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0morders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
       "the specified columns, so we can run aggregation on them.\n",
       "\n",
       ".. versionadded:: 1.4.0\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
       "+-----+----+-----+\n",
       "| name| age|count|\n",
       "+-----+----+-----+\n",
       "| null|null|    2|\n",
       "|Alice|null|    1|\n",
       "|Alice|   2|    1|\n",
       "|  Bob|null|    1|\n",
       "|  Bob|   5|    1|\n",
       "+-----+----+-----+\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orders.rollup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e605ced5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|          order_date|order_count|\n",
      "+--------------------+-----------+\n",
      "|                null|      68883|\n",
      "|2013-07-25 00:00:...|        143|\n",
      "|2013-07-26 00:00:...|        269|\n",
      "|2013-07-27 00:00:...|        202|\n",
      "|2013-07-28 00:00:...|        187|\n",
      "|2013-07-29 00:00:...|        253|\n",
      "|2013-07-30 00:00:...|        227|\n",
      "|2013-07-31 00:00:...|        252|\n",
      "|2013-08-01 00:00:...|        246|\n",
      "|2013-08-02 00:00:...|        224|\n",
      "|2013-08-03 00:00:...|        183|\n",
      "|2013-08-04 00:00:...|        187|\n",
      "|2013-08-05 00:00:...|        153|\n",
      "|2013-08-06 00:00:...|        258|\n",
      "|2013-08-07 00:00:...|        203|\n",
      "|2013-08-08 00:00:...|        154|\n",
      "|2013-08-09 00:00:...|        125|\n",
      "|2013-08-10 00:00:...|        270|\n",
      "|2013-08-11 00:00:...|        154|\n",
      "|2013-08-12 00:00:...|        255|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    rollup('order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_date'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "889f86a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    rollup('order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_date'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccac7c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37f581a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----+\n",
      "|order_month|          order_date|count|\n",
      "+-----------+--------------------+-----+\n",
      "|     201308|2013-08-18 00:00:...|  199|\n",
      "|     201402|2014-02-02 00:00:...|  192|\n",
      "|     201406|2014-06-14 00:00:...|  255|\n",
      "|     201308|2013-08-19 00:00:...|   93|\n",
      "|     201403|2014-03-22 00:00:...|  223|\n",
      "+-----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy(date_format('order_date','yyyyMM').alias('order_month'), 'order_date'). \\\n",
    "    count(). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3be41788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac81fbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+\n",
      "|order_month|          order_date|order_count|\n",
      "+-----------+--------------------+-----------+\n",
      "|     201308|2013-08-18 00:00:...|        199|\n",
      "|     201402|2014-02-02 00:00:...|        192|\n",
      "|     201406|2014-06-14 00:00:...|        255|\n",
      "|     201308|2013-08-19 00:00:...|         93|\n",
      "|     201403|2014-03-22 00:00:...|        223|\n",
      "+-----------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy(date_format('order_date','yyyyMM').alias('order_month'), 'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58322740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy(date_format('order_date','yyyyMM').alias('order_month'), 'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40782fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+\n",
      "|order_month|          order_date|order_count|\n",
      "+-----------+--------------------+-----------+\n",
      "|       null|                null|      68883|\n",
      "|     201307|                null|       1533|\n",
      "|     201307|2013-07-25 00:00:...|        143|\n",
      "|     201307|2013-07-26 00:00:...|        269|\n",
      "|     201307|2013-07-27 00:00:...|        202|\n",
      "|     201307|2013-07-28 00:00:...|        187|\n",
      "|     201307|2013-07-29 00:00:...|        253|\n",
      "|     201307|2013-07-30 00:00:...|        227|\n",
      "|     201307|2013-07-31 00:00:...|        252|\n",
      "|     201308|                null|       5680|\n",
      "|     201308|2013-08-01 00:00:...|        246|\n",
      "|     201308|2013-08-02 00:00:...|        224|\n",
      "|     201308|2013-08-03 00:00:...|        183|\n",
      "|     201308|2013-08-04 00:00:...|        187|\n",
      "|     201308|2013-08-05 00:00:...|        153|\n",
      "|     201308|2013-08-06 00:00:...|        258|\n",
      "|     201308|2013-08-07 00:00:...|        203|\n",
      "|     201308|2013-08-08 00:00:...|        154|\n",
      "|     201308|2013-08-09 00:00:...|        125|\n",
      "|     201308|2013-08-10 00:00:...|        270|\n",
      "|     201308|2013-08-11 00:00:...|        154|\n",
      "|     201308|2013-08-12 00:00:...|        255|\n",
      "|     201308|2013-08-13 00:00:...|         73|\n",
      "|     201308|2013-08-14 00:00:...|        193|\n",
      "|     201308|2013-08-15 00:00:...|        200|\n",
      "+-----------+--------------------+-----------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    rollup(\n",
    "        date_format('order_date','yyyyMM').alias('order_month'),\n",
    "        'order_date'\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_month','order_date'). \\\n",
    "    show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8ab60ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    rollup(\n",
    "        date_format('order_date','yyyyMM').alias('order_month'),\n",
    "        'order_date'\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_month','order_date'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d51e127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6085fe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-----------+\n",
      "|order_year|order_month|          order_date|order_count|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "|      2013|     201307|2013-07-25 00:00:...|        143|\n",
      "|      2013|     201307|2013-07-26 00:00:...|        269|\n",
      "|      2013|     201307|2013-07-27 00:00:...|        202|\n",
      "|      2013|     201307|2013-07-28 00:00:...|        187|\n",
      "|      2013|     201307|2013-07-29 00:00:...|        253|\n",
      "|      2013|     201307|2013-07-30 00:00:...|        227|\n",
      "|      2013|     201307|2013-07-31 00:00:...|        252|\n",
      "|      2013|     201308|2013-08-01 00:00:...|        246|\n",
      "|      2013|     201308|2013-08-02 00:00:...|        224|\n",
      "|      2013|     201308|2013-08-03 00:00:...|        183|\n",
      "|      2013|     201308|2013-08-04 00:00:...|        187|\n",
      "|      2013|     201308|2013-08-05 00:00:...|        153|\n",
      "|      2013|     201308|2013-08-06 00:00:...|        258|\n",
      "|      2013|     201308|2013-08-07 00:00:...|        203|\n",
      "|      2013|     201308|2013-08-08 00:00:...|        154|\n",
      "|      2013|     201308|2013-08-09 00:00:...|        125|\n",
      "|      2013|     201308|2013-08-10 00:00:...|        270|\n",
      "|      2013|     201308|2013-08-11 00:00:...|        154|\n",
      "|      2013|     201308|2013-08-12 00:00:...|        255|\n",
      "|      2013|     201308|2013-08-13 00:00:...|         73|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy(\n",
    "        year('order_date').alias('order_year'),\n",
    "        date_format('order_date','yyyyMM').alias('order_month'),\n",
    "        'order_date'\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3ee44d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy(\n",
    "        year('order_date').alias('order_year'),\n",
    "        date_format('order_date','yyyyMM').alias('order_month'),\n",
    "        'order_date'\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7c54d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-----------+\n",
      "|order_year|order_month|          order_date|order_count|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "|      null|       null|                null|      68883|\n",
      "|      2013|       null|                null|      30662|\n",
      "|      2013|     201307|                null|       1533|\n",
      "|      2013|     201307|2013-07-25 00:00:...|        143|\n",
      "|      2013|     201307|2013-07-26 00:00:...|        269|\n",
      "|      2013|     201307|2013-07-27 00:00:...|        202|\n",
      "|      2013|     201307|2013-07-28 00:00:...|        187|\n",
      "|      2013|     201307|2013-07-29 00:00:...|        253|\n",
      "|      2013|     201307|2013-07-30 00:00:...|        227|\n",
      "|      2013|     201307|2013-07-31 00:00:...|        252|\n",
      "|      2013|     201308|                null|       5680|\n",
      "|      2013|     201308|2013-08-01 00:00:...|        246|\n",
      "|      2013|     201308|2013-08-02 00:00:...|        224|\n",
      "|      2013|     201308|2013-08-03 00:00:...|        183|\n",
      "|      2013|     201308|2013-08-04 00:00:...|        187|\n",
      "|      2013|     201308|2013-08-05 00:00:...|        153|\n",
      "|      2013|     201308|2013-08-06 00:00:...|        258|\n",
      "|      2013|     201308|2013-08-07 00:00:...|        203|\n",
      "|      2013|     201308|2013-08-08 00:00:...|        154|\n",
      "|      2013|     201308|2013-08-09 00:00:...|        125|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    rollup(\n",
    "        year('order_date').alias('order_year'),\n",
    "        date_format('order_date','yyyyMM').alias('order_month'),\n",
    "        'order_date'\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c32f5964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    rollup(\n",
    "        year('order_date').alias('order_year'),\n",
    "        date_format('order_date','yyyyMM').alias('order_month'),\n",
    "        'order_date'\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c611cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-----------+\n",
      "|order_year|order_month|          order_date|order_count|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "|      2014|     201401|                null|       5908|\n",
      "|      2014|     201401|2014-01-01 00:00:...|        135|\n",
      "|      2014|     201401|2014-01-02 00:00:...|        111|\n",
      "|      2014|     201401|2014-01-03 00:00:...|        250|\n",
      "|      2014|     201401|2014-01-04 00:00:...|        129|\n",
      "|      2014|     201401|2014-01-05 00:00:...|        266|\n",
      "|      2014|     201401|2014-01-06 00:00:...|        155|\n",
      "|      2014|     201401|2014-01-07 00:00:...|        163|\n",
      "|      2014|     201401|2014-01-08 00:00:...|        122|\n",
      "|      2014|     201401|2014-01-09 00:00:...|        207|\n",
      "|      2014|     201401|2014-01-10 00:00:...|        241|\n",
      "|      2014|     201401|2014-01-11 00:00:...|        281|\n",
      "|      2014|     201401|2014-01-12 00:00:...|        215|\n",
      "|      2014|     201401|2014-01-13 00:00:...|        179|\n",
      "|      2014|     201401|2014-01-14 00:00:...|        209|\n",
      "|      2014|     201401|2014-01-15 00:00:...|        243|\n",
      "|      2014|     201401|2014-01-16 00:00:...|        194|\n",
      "|      2014|     201401|2014-01-17 00:00:...|        149|\n",
      "|      2014|     201401|2014-01-18 00:00:...|        139|\n",
      "|      2014|     201401|2014-01-19 00:00:...|        217|\n",
      "|      2014|     201401|2014-01-20 00:00:...|        203|\n",
      "|      2014|     201401|2014-01-21 00:00:...|        259|\n",
      "|      2014|     201401|2014-01-22 00:00:...|        209|\n",
      "|      2014|     201401|2014-01-23 00:00:...|        220|\n",
      "|      2014|     201401|2014-01-24 00:00:...|        159|\n",
      "|      2014|     201401|2014-01-25 00:00:...|        104|\n",
      "|      2014|     201401|2014-01-26 00:00:...|        154|\n",
      "|      2014|     201401|2014-01-27 00:00:...|        163|\n",
      "|      2014|     201401|2014-01-28 00:00:...|        197|\n",
      "|      2014|     201401|2014-01-29 00:00:...|        158|\n",
      "|      2014|     201401|2014-01-30 00:00:...|        254|\n",
      "|      2014|     201401|2014-01-31 00:00:...|        223|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    rollup(\n",
    "        year('order_date').alias('order_year'),\n",
    "        date_format('order_date','yyyyMM').alias('order_month'),\n",
    "        'order_date'\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    filter(\"order_month=201401\").\\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    show(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "911f5cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    rollup(\n",
    "        year('order_date').alias('order_year'),\n",
    "        date_format('order_date','yyyyMM').alias('order_month'),\n",
    "        'order_date'\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    filter(\"order_month=201401\").\\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29717d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 196 Aggregate data using cube on Spark DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c24b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse/'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config(\"spark.driver.memory\", '6g'). \\\n",
    "        config('spark.executor.memory', '6g'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Basic Transformations'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()\n",
    "\n",
    "#        config('spark.executor.heartbeatInterval','20s'). \\\n",
    "#         config('spark.network.timeout','6000'). \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "236b3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = '/public/airlines_all/airlines-part/flightmonth=200801'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5d80f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8234103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a60ad898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|IsDepDelayed|IsArrDelayed|Cancelled|\n",
      "+------------+------------+---------+\n",
      "|          NO|          NO|        0|\n",
      "|         YES|         YES|        1|\n",
      "|          NO|         YES|        0|\n",
      "|         YES|          NO|        0|\n",
      "|         YES|         YES|        0|\n",
      "+------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('IsDepDelayed','IsArrDelayed','Cancelled'). \\\n",
    "    distinct(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "668048f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.json('/public/retail_db_json/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c6f6858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43e46e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dec3161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e6f128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|          order_date|order_count|\n",
      "+--------------------+-----------+\n",
      "|2013-08-13 00:00:...|         73|\n",
      "|2013-10-12 00:00:...|        162|\n",
      "|2013-11-15 00:00:...|        135|\n",
      "|2014-03-19 00:00:...|        130|\n",
      "|2014-04-26 00:00:...|        251|\n",
      "|2013-09-16 00:00:...|        121|\n",
      "|2013-09-20 00:00:...|        139|\n",
      "|2013-12-31 00:00:...|        266|\n",
      "|2013-09-06 00:00:...|        276|\n",
      "|2014-06-15 00:00:...|        128|\n",
      "|2013-12-24 00:00:...|        170|\n",
      "|2014-01-07 00:00:...|        163|\n",
      "|2014-06-07 00:00:...|        191|\n",
      "|2013-10-14 00:00:...|        139|\n",
      "|2013-11-11 00:00:...|        246|\n",
      "|2014-01-27 00:00:...|        163|\n",
      "|2014-01-29 00:00:...|        158|\n",
      "|2014-02-14 00:00:...|        174|\n",
      "|2014-04-15 00:00:...|        180|\n",
      "|2014-04-22 00:00:...|        144|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy('order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31261226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy('order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f5e054d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0morders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcube\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
       "the specified columns, so we can run aggregations on them.\n",
       "\n",
       ".. versionadded:: 1.4.0\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
       "+-----+----+-----+\n",
       "| name| age|count|\n",
       "+-----+----+-----+\n",
       "| null|null|    2|\n",
       "| null|   2|    1|\n",
       "| null|   5|    1|\n",
       "|Alice|null|    1|\n",
       "|Alice|   2|    1|\n",
       "|  Bob|null|    1|\n",
       "|  Bob|   5|    1|\n",
       "+-----+----+-----+\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orders.cube?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f006a106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|          order_date|order_count|\n",
      "+--------------------+-----------+\n",
      "|                null|      68883|\n",
      "|2013-07-25 00:00:...|        143|\n",
      "|2013-07-26 00:00:...|        269|\n",
      "|2013-07-27 00:00:...|        202|\n",
      "|2013-07-28 00:00:...|        187|\n",
      "|2013-07-29 00:00:...|        253|\n",
      "|2013-07-30 00:00:...|        227|\n",
      "|2013-07-31 00:00:...|        252|\n",
      "|2013-08-01 00:00:...|        246|\n",
      "|2013-08-02 00:00:...|        224|\n",
      "|2013-08-03 00:00:...|        183|\n",
      "|2013-08-04 00:00:...|        187|\n",
      "|2013-08-05 00:00:...|        153|\n",
      "|2013-08-06 00:00:...|        258|\n",
      "|2013-08-07 00:00:...|        203|\n",
      "|2013-08-08 00:00:...|        154|\n",
      "|2013-08-09 00:00:...|        125|\n",
      "|2013-08-10 00:00:...|        270|\n",
      "|2013-08-11 00:00:...|        154|\n",
      "|2013-08-12 00:00:...|        255|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube('order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_date'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31747b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube('order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_date'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75615856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, count, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d9af87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+\n",
      "|order_month|          order_date|order_count|\n",
      "+-----------+--------------------+-----------+\n",
      "|     201308|2013-08-18 00:00:...|        199|\n",
      "|     201402|2014-02-02 00:00:...|        192|\n",
      "|     201406|2014-06-14 00:00:...|        255|\n",
      "|     201308|2013-08-19 00:00:...|         93|\n",
      "|     201403|2014-03-22 00:00:...|        223|\n",
      "|     201405|2014-05-06 00:00:...|        265|\n",
      "|     201311|2013-11-23 00:00:...|        251|\n",
      "|     201404|2014-04-19 00:00:...|        116|\n",
      "|     201401|2014-01-20 00:00:...|        203|\n",
      "|     201401|2014-01-27 00:00:...|        163|\n",
      "|     201402|2014-02-18 00:00:...|        219|\n",
      "|     201406|2014-06-18 00:00:...|        179|\n",
      "|     201405|2014-05-13 00:00:...|        201|\n",
      "|     201406|2014-06-17 00:00:...|        142|\n",
      "|     201308|2013-08-16 00:00:...|        131|\n",
      "|     201311|2013-11-08 00:00:...|        170|\n",
      "|     201311|2013-11-19 00:00:...|        188|\n",
      "|     201311|2013-11-25 00:00:...|        133|\n",
      "|     201308|2013-08-04 00:00:...|        187|\n",
      "|     201310|2013-10-18 00:00:...|        129|\n",
      "+-----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy(date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1fb2e321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+\n",
      "|order_month|          order_date|order_count|\n",
      "+-----------+--------------------+-----------+\n",
      "|       null|                null|      68883|\n",
      "|       null|2013-07-25 00:00:...|        143|\n",
      "|       null|2013-07-26 00:00:...|        269|\n",
      "|       null|2013-07-27 00:00:...|        202|\n",
      "|       null|2013-07-28 00:00:...|        187|\n",
      "|       null|2013-07-29 00:00:...|        253|\n",
      "|       null|2013-07-30 00:00:...|        227|\n",
      "|       null|2013-07-31 00:00:...|        252|\n",
      "|       null|2013-08-01 00:00:...|        246|\n",
      "|       null|2013-08-02 00:00:...|        224|\n",
      "|       null|2013-08-03 00:00:...|        183|\n",
      "|       null|2013-08-04 00:00:...|        187|\n",
      "|       null|2013-08-05 00:00:...|        153|\n",
      "|       null|2013-08-06 00:00:...|        258|\n",
      "|       null|2013-08-07 00:00:...|        203|\n",
      "|       null|2013-08-08 00:00:...|        154|\n",
      "|       null|2013-08-09 00:00:...|        125|\n",
      "|       null|2013-08-10 00:00:...|        270|\n",
      "|       null|2013-08-11 00:00:...|        154|\n",
      "|       null|2013-08-12 00:00:...|        255|\n",
      "+-----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_month','order_date'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae8cab12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_month','order_date'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad8470e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+\n",
      "|order_month|          order_date|order_count|\n",
      "+-----------+--------------------+-----------+\n",
      "|     201307|                null|       1533|\n",
      "|     201307|2013-07-25 00:00:...|        143|\n",
      "|     201307|2013-07-26 00:00:...|        269|\n",
      "|     201307|2013-07-27 00:00:...|        202|\n",
      "|     201307|2013-07-28 00:00:...|        187|\n",
      "|     201307|2013-07-29 00:00:...|        253|\n",
      "|     201307|2013-07-30 00:00:...|        227|\n",
      "|     201307|2013-07-31 00:00:...|        252|\n",
      "|     201308|                null|       5680|\n",
      "|     201308|2013-08-01 00:00:...|        246|\n",
      "|     201308|2013-08-02 00:00:...|        224|\n",
      "|     201308|2013-08-03 00:00:...|        183|\n",
      "|     201308|2013-08-04 00:00:...|        187|\n",
      "|     201308|2013-08-05 00:00:...|        153|\n",
      "|     201308|2013-08-06 00:00:...|        258|\n",
      "|     201308|2013-08-07 00:00:...|        203|\n",
      "|     201308|2013-08-08 00:00:...|        154|\n",
      "|     201308|2013-08-09 00:00:...|        125|\n",
      "|     201308|2013-08-10 00:00:...|        270|\n",
      "|     201308|2013-08-11 00:00:...|        154|\n",
      "+-----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    filter('order_month IS NOT NULL'). \\\n",
    "    orderBy('order_month','order_date'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea12ce1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    filter('order_month IS NOT NULL'). \\\n",
    "    orderBy('order_month','order_date'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "39b80c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, count, lit, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b49075e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-----------+\n",
      "|order_year|order_month|          order_date|order_count|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "|      2013|     201309|2013-09-13 00:00:...|        103|\n",
      "|      2014|     201404|2014-04-08 00:00:...|        233|\n",
      "|      2013|     201312|2013-12-06 00:00:...|        256|\n",
      "|      2013|     201310|2013-10-13 00:00:...|        277|\n",
      "|      2013|     201309|2013-09-01 00:00:...|        119|\n",
      "|      2013|     201310|2013-10-06 00:00:...|        249|\n",
      "|      2014|     201401|2014-01-14 00:00:...|        209|\n",
      "|      2014|     201401|2014-01-16 00:00:...|        194|\n",
      "|      2014|     201403|2014-03-18 00:00:...|        252|\n",
      "|      2014|     201403|2014-03-31 00:00:...|        263|\n",
      "|      2014|     201404|2014-04-28 00:00:...|        125|\n",
      "|      2013|     201309|2013-09-23 00:00:...|        177|\n",
      "|      2013|     201312|2013-12-23 00:00:...|        154|\n",
      "|      2014|     201402|2014-02-09 00:00:...|        240|\n",
      "|      2014|     201405|2014-05-11 00:00:...|        227|\n",
      "|      2013|     201308|2013-08-13 00:00:...|         73|\n",
      "|      2014|     201403|2014-03-14 00:00:...|        201|\n",
      "|      2014|     201404|2014-04-20 00:00:...|        195|\n",
      "|      2014|     201401|2014-01-04 00:00:...|        129|\n",
      "|      2014|     201406|2014-06-02 00:00:...|        228|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f3c9a1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    groupBy(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "73ca36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-----------+\n",
      "|order_year|order_month|          order_date|order_count|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "|      null|       null|                null|      68883|\n",
      "|      2013|       null|                null|      30662|\n",
      "|      2013|     201307|                null|       1533|\n",
      "|      2013|     201307|2013-07-25 00:00:...|        143|\n",
      "|      2013|     201307|2013-07-26 00:00:...|        269|\n",
      "|      2013|     201307|2013-07-27 00:00:...|        202|\n",
      "|      2013|     201307|2013-07-28 00:00:...|        187|\n",
      "|      2013|     201307|2013-07-29 00:00:...|        253|\n",
      "|      2013|     201307|2013-07-30 00:00:...|        227|\n",
      "|      2013|     201307|2013-07-31 00:00:...|        252|\n",
      "|      2013|     201308|                null|       5680|\n",
      "|      2013|     201308|2013-08-01 00:00:...|        246|\n",
      "|      2013|     201308|2013-08-02 00:00:...|        224|\n",
      "|      2013|     201308|2013-08-03 00:00:...|        183|\n",
      "|      2013|     201308|2013-08-04 00:00:...|        187|\n",
      "|      2013|     201308|2013-08-05 00:00:...|        153|\n",
      "|      2013|     201308|2013-08-06 00:00:...|        258|\n",
      "|      2013|     201308|2013-08-07 00:00:...|        203|\n",
      "|      2013|     201308|2013-08-08 00:00:...|        154|\n",
      "|      2013|     201308|2013-08-09 00:00:...|        125|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    rollup(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0fab98e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    rollup(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f6e314e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-----------+\n",
      "|order_year|order_month|          order_date|order_count|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "|      null|       null|                null|      68883|\n",
      "|      null|       null|2013-07-25 00:00:...|        143|\n",
      "|      null|       null|2013-07-26 00:00:...|        269|\n",
      "|      null|       null|2013-07-27 00:00:...|        202|\n",
      "|      null|       null|2013-07-28 00:00:...|        187|\n",
      "|      null|       null|2013-07-29 00:00:...|        253|\n",
      "|      null|       null|2013-07-30 00:00:...|        227|\n",
      "|      null|       null|2013-07-31 00:00:...|        252|\n",
      "|      null|       null|2013-08-01 00:00:...|        246|\n",
      "|      null|       null|2013-08-02 00:00:...|        224|\n",
      "|      null|       null|2013-08-03 00:00:...|        183|\n",
      "|      null|       null|2013-08-04 00:00:...|        187|\n",
      "|      null|       null|2013-08-05 00:00:...|        153|\n",
      "|      null|       null|2013-08-06 00:00:...|        258|\n",
      "|      null|       null|2013-08-07 00:00:...|        203|\n",
      "|      null|       null|2013-08-08 00:00:...|        154|\n",
      "|      null|       null|2013-08-09 00:00:...|        125|\n",
      "|      null|       null|2013-08-10 00:00:...|        270|\n",
      "|      null|       null|2013-08-11 00:00:...|        154|\n",
      "|      null|       null|2013-08-12 00:00:...|        255|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08839708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1485"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c129e18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-----------+\n",
      "|order_year|order_month|          order_date|order_count|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "|      2013|       null|                null|      30662|\n",
      "|      2013|       null|2013-07-25 00:00:...|        143|\n",
      "|      2013|       null|2013-07-26 00:00:...|        269|\n",
      "|      2013|       null|2013-07-27 00:00:...|        202|\n",
      "|      2013|       null|2013-07-28 00:00:...|        187|\n",
      "|      2013|       null|2013-07-29 00:00:...|        253|\n",
      "|      2013|       null|2013-07-30 00:00:...|        227|\n",
      "|      2013|       null|2013-07-31 00:00:...|        252|\n",
      "|      2013|       null|2013-08-01 00:00:...|        246|\n",
      "|      2013|       null|2013-08-02 00:00:...|        224|\n",
      "|      2013|       null|2013-08-03 00:00:...|        183|\n",
      "|      2013|       null|2013-08-04 00:00:...|        187|\n",
      "|      2013|       null|2013-08-05 00:00:...|        153|\n",
      "|      2013|       null|2013-08-06 00:00:...|        258|\n",
      "|      2013|       null|2013-08-07 00:00:...|        203|\n",
      "|      2013|       null|2013-08-08 00:00:...|        154|\n",
      "|      2013|       null|2013-08-09 00:00:...|        125|\n",
      "|      2013|       null|2013-08-10 00:00:...|        270|\n",
      "|      2013|       null|2013-08-11 00:00:...|        154|\n",
      "|      2013|       null|2013-08-12 00:00:...|        255|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    filter('order_year IS NOT NULL'). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf069655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "743"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    filter('order_year IS NOT NULL'). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1d44fc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-----------+\n",
      "|order_year|order_month|          order_date|order_count|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "|      2013|     201307|                null|       1533|\n",
      "|      2013|     201307|2013-07-25 00:00:...|        143|\n",
      "|      2013|     201307|2013-07-26 00:00:...|        269|\n",
      "|      2013|     201307|2013-07-27 00:00:...|        202|\n",
      "|      2013|     201307|2013-07-28 00:00:...|        187|\n",
      "|      2013|     201307|2013-07-29 00:00:...|        253|\n",
      "|      2013|     201307|2013-07-30 00:00:...|        227|\n",
      "|      2013|     201307|2013-07-31 00:00:...|        252|\n",
      "|      2013|     201308|                null|       5680|\n",
      "|      2013|     201308|2013-08-01 00:00:...|        246|\n",
      "|      2013|     201308|2013-08-02 00:00:...|        224|\n",
      "|      2013|     201308|2013-08-03 00:00:...|        183|\n",
      "|      2013|     201308|2013-08-04 00:00:...|        187|\n",
      "|      2013|     201308|2013-08-05 00:00:...|        153|\n",
      "|      2013|     201308|2013-08-06 00:00:...|        258|\n",
      "|      2013|     201308|2013-08-07 00:00:...|        203|\n",
      "|      2013|     201308|2013-08-08 00:00:...|        154|\n",
      "|      2013|     201308|2013-08-09 00:00:...|        125|\n",
      "|      2013|     201308|2013-08-10 00:00:...|        270|\n",
      "|      2013|     201308|2013-08-11 00:00:...|        154|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    filter('order_year IS NOT NULL AND order_month IS NOT NULL'). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1abc77df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-----------+\n",
      "|order_year|order_month|          order_date|order_count|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "|      2013|     201307|                null|       1533|\n",
      "|      2013|     201307|2013-07-25 00:00:...|        143|\n",
      "|      2013|     201307|2013-07-26 00:00:...|        269|\n",
      "|      2013|     201307|2013-07-27 00:00:...|        202|\n",
      "|      2013|     201307|2013-07-28 00:00:...|        187|\n",
      "|      2013|     201307|2013-07-29 00:00:...|        253|\n",
      "|      2013|     201307|2013-07-30 00:00:...|        227|\n",
      "|      2013|     201307|2013-07-31 00:00:...|        252|\n",
      "|      2013|     201308|                null|       5680|\n",
      "|      2013|     201308|2013-08-01 00:00:...|        246|\n",
      "|      2013|     201308|2013-08-02 00:00:...|        224|\n",
      "|      2013|     201308|2013-08-03 00:00:...|        183|\n",
      "|      2013|     201308|2013-08-04 00:00:...|        187|\n",
      "|      2013|     201308|2013-08-05 00:00:...|        153|\n",
      "|      2013|     201308|2013-08-06 00:00:...|        258|\n",
      "|      2013|     201308|2013-08-07 00:00:...|        203|\n",
      "|      2013|     201308|2013-08-08 00:00:...|        154|\n",
      "|      2013|     201308|2013-08-09 00:00:...|        125|\n",
      "|      2013|     201308|2013-08-10 00:00:...|        270|\n",
      "|      2013|     201308|2013-08-11 00:00:...|        154|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    filter('order_year IS NOT NULL AND order_month IS NOT NULL'). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "13e3981e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    filter('order_year IS NOT NULL AND order_month IS NOT NULL'). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f57aea96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    filter(\"order_date LIKE '2014-01%'\"). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3a6b0d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-----------+\n",
      "|order_year|order_month|          order_date|order_count|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "|      null|       null|2014-01-01 00:00:...|        135|\n",
      "|      null|       null|2014-01-02 00:00:...|        111|\n",
      "|      null|       null|2014-01-03 00:00:...|        250|\n",
      "|      null|       null|2014-01-04 00:00:...|        129|\n",
      "|      null|       null|2014-01-05 00:00:...|        266|\n",
      "|      null|       null|2014-01-06 00:00:...|        155|\n",
      "|      null|       null|2014-01-07 00:00:...|        163|\n",
      "|      null|       null|2014-01-08 00:00:...|        122|\n",
      "|      null|       null|2014-01-09 00:00:...|        207|\n",
      "|      null|       null|2014-01-10 00:00:...|        241|\n",
      "|      null|       null|2014-01-11 00:00:...|        281|\n",
      "|      null|       null|2014-01-12 00:00:...|        215|\n",
      "|      null|       null|2014-01-13 00:00:...|        179|\n",
      "|      null|       null|2014-01-14 00:00:...|        209|\n",
      "|      null|       null|2014-01-15 00:00:...|        243|\n",
      "|      null|       null|2014-01-16 00:00:...|        194|\n",
      "|      null|       null|2014-01-17 00:00:...|        149|\n",
      "|      null|       null|2014-01-18 00:00:...|        139|\n",
      "|      null|       null|2014-01-19 00:00:...|        217|\n",
      "|      null|       null|2014-01-20 00:00:...|        203|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    filter(\"order_date LIKE '2014-01%'\"). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "33f33c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------------------+-----------+\n",
      "|order_year|order_month|          order_date|order_count|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "|      2014|     201401|2014-01-01 00:00:...|        135|\n",
      "|      2014|     201401|2014-01-02 00:00:...|        111|\n",
      "|      2014|     201401|2014-01-03 00:00:...|        250|\n",
      "|      2014|     201401|2014-01-04 00:00:...|        129|\n",
      "|      2014|     201401|2014-01-05 00:00:...|        266|\n",
      "|      2014|     201401|2014-01-06 00:00:...|        155|\n",
      "|      2014|     201401|2014-01-07 00:00:...|        163|\n",
      "|      2014|     201401|2014-01-08 00:00:...|        122|\n",
      "|      2014|     201401|2014-01-09 00:00:...|        207|\n",
      "|      2014|     201401|2014-01-10 00:00:...|        241|\n",
      "|      2014|     201401|2014-01-11 00:00:...|        281|\n",
      "|      2014|     201401|2014-01-12 00:00:...|        215|\n",
      "|      2014|     201401|2014-01-13 00:00:...|        179|\n",
      "|      2014|     201401|2014-01-14 00:00:...|        209|\n",
      "|      2014|     201401|2014-01-15 00:00:...|        243|\n",
      "|      2014|     201401|2014-01-16 00:00:...|        194|\n",
      "|      2014|     201401|2014-01-17 00:00:...|        149|\n",
      "|      2014|     201401|2014-01-18 00:00:...|        139|\n",
      "|      2014|     201401|2014-01-19 00:00:...|        217|\n",
      "|      2014|     201401|2014-01-20 00:00:...|        203|\n",
      "+----------+-----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    filter(\"order_year IS NOT NULL AND order_month IS NOT NULL AND order_date LIKE '2014-01%'\"). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "735dfbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders. \\\n",
    "    cube(year('order_date').alias('order_year'),date_format('order_date','yyyyMM').alias('order_month'),'order_date'). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    filter(\"order_year IS NOT NULL AND order_month IS NOT NULL AND order_date LIKE '2014-01%'\"). \\\n",
    "    orderBy('order_year','order_month','order_date'). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ed1f2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 197 Overview of Sorting Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22fdbbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse/'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config(\"spark.driver.memory\", '6g'). \\\n",
    "        config('spark.executor.memory', '6g'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Basic Transformations'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()\n",
    "\n",
    "#        config('spark.executor.heartbeatInterval','20s'). \\\n",
    "#         config('spark.network.timeout','6000'). \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f169a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = '/public/airlines_all/airlines-part/flightmonth=200801'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9476527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac77b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark.read.parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01bc5f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------+\n",
      "|IsDepDelayed|IsArrDelayed|Cancelled|\n",
      "+------------+------------+---------+\n",
      "|          NO|          NO|        0|\n",
      "|         YES|         YES|        1|\n",
      "|          NO|         YES|        0|\n",
      "|         YES|          NO|        0|\n",
      "|         YES|         YES|        0|\n",
      "+------------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('IsDepDelayed','IsArrDelayed','Cancelled'). \\\n",
    "    distinct(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acbf58af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      " |-- IsArrDelayed: string (nullable = true)\n",
      " |-- IsDepDelayed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8659be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lpad, col,lit, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e8ff0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FlightDate|FlightCount|\n",
      "+----------+-----------+\n",
      "|  20080120|        247|\n",
      "|  20080130|        694|\n",
      "|  20080115|        299|\n",
      "|  20080118|        230|\n",
      "|  20080122|        788|\n",
      "|  20080104|        769|\n",
      "|  20080125|        526|\n",
      "|  20080102|        511|\n",
      "|  20080105|        456|\n",
      "|  20080111|        524|\n",
      "|  20080109|        377|\n",
      "|  20080127|        638|\n",
      "|  20080101|        552|\n",
      "|  20080128|        654|\n",
      "|  20080119|        876|\n",
      "|  20080106|        683|\n",
      "|  20080123|        530|\n",
      "|  20080117|        872|\n",
      "|  20080116|        532|\n",
      "|  20080112|        226|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('cancelled = 1'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            col('Year'),\n",
    "            lpad(col('Month'),2,'0'),\n",
    "            lpad(col('DayOfMonth'),2,'0')\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ac2a682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FlightDate|FlightCount|\n",
      "+----------+-----------+\n",
      "|  20080112|        226|\n",
      "|  20080118|        230|\n",
      "|  20080120|        247|\n",
      "|  20080115|        299|\n",
      "|  20080124|        322|\n",
      "|  20080110|        341|\n",
      "|  20080113|        359|\n",
      "|  20080109|        377|\n",
      "|  20080126|        416|\n",
      "|  20080105|        456|\n",
      "|  20080108|        463|\n",
      "|  20080103|        475|\n",
      "|  20080121|        475|\n",
      "|  20080102|        511|\n",
      "|  20080111|        524|\n",
      "|  20080125|        526|\n",
      "|  20080123|        530|\n",
      "|  20080116|        532|\n",
      "|  20080101|        552|\n",
      "|  20080107|        579|\n",
      "|  20080127|        638|\n",
      "|  20080128|        654|\n",
      "|  20080106|        683|\n",
      "|  20080130|        694|\n",
      "|  20080104|        769|\n",
      "|  20080122|        788|\n",
      "|  20080117|        872|\n",
      "|  20080119|        876|\n",
      "|  20080129|        889|\n",
      "|  20080114|        909|\n",
      "|  20080131|       1081|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('cancelled = 1'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            col('Year'),\n",
    "            lpad(col('Month'),2,'0'),\n",
    "            lpad(col('DayOfMonth'),2,'0')\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    orderBy('FlightCount'). \\\n",
    "    show(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "180cf9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FlightDate|FlightCount|\n",
      "+----------+-----------+\n",
      "|  20080112|        226|\n",
      "|  20080118|        230|\n",
      "|  20080120|        247|\n",
      "|  20080115|        299|\n",
      "|  20080124|        322|\n",
      "|  20080110|        341|\n",
      "|  20080113|        359|\n",
      "|  20080109|        377|\n",
      "|  20080126|        416|\n",
      "|  20080105|        456|\n",
      "|  20080108|        463|\n",
      "|  20080103|        475|\n",
      "|  20080121|        475|\n",
      "|  20080102|        511|\n",
      "|  20080111|        524|\n",
      "|  20080125|        526|\n",
      "|  20080123|        530|\n",
      "|  20080116|        532|\n",
      "|  20080101|        552|\n",
      "|  20080107|        579|\n",
      "|  20080127|        638|\n",
      "|  20080128|        654|\n",
      "|  20080106|        683|\n",
      "|  20080130|        694|\n",
      "|  20080104|        769|\n",
      "|  20080122|        788|\n",
      "|  20080117|        872|\n",
      "|  20080119|        876|\n",
      "|  20080129|        889|\n",
      "|  20080114|        909|\n",
      "|  20080131|       1081|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('cancelled = 1'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            col('Year'),\n",
    "            lpad(col('Month'),2,'0'),\n",
    "            lpad(col('DayOfMonth'),2,'0')\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    orderBy(col('FlightCount').asc()). \\\n",
    "    show(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63608a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FlightDate|FlightCount|\n",
      "+----------+-----------+\n",
      "|  20080131|       1081|\n",
      "|  20080114|        909|\n",
      "|  20080129|        889|\n",
      "|  20080119|        876|\n",
      "|  20080117|        872|\n",
      "|  20080122|        788|\n",
      "|  20080104|        769|\n",
      "|  20080130|        694|\n",
      "|  20080106|        683|\n",
      "|  20080128|        654|\n",
      "|  20080127|        638|\n",
      "|  20080107|        579|\n",
      "|  20080101|        552|\n",
      "|  20080116|        532|\n",
      "|  20080123|        530|\n",
      "|  20080125|        526|\n",
      "|  20080111|        524|\n",
      "|  20080102|        511|\n",
      "|  20080121|        475|\n",
      "|  20080103|        475|\n",
      "|  20080108|        463|\n",
      "|  20080105|        456|\n",
      "|  20080126|        416|\n",
      "|  20080109|        377|\n",
      "|  20080113|        359|\n",
      "|  20080110|        341|\n",
      "|  20080124|        322|\n",
      "|  20080115|        299|\n",
      "|  20080120|        247|\n",
      "|  20080118|        230|\n",
      "|  20080112|        226|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('cancelled = 1'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            col('Year'),\n",
    "            lpad(col('Month'),2,'0'),\n",
    "            lpad(col('DayOfMonth'),2,'0')\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    orderBy(col('FlightCount').desc()). \\\n",
    "    show(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fedbfd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------+------+\n",
      "|Year|Month|DayOfMonth|CRSDepTime|Origin|\n",
      "+----+-----+----------+----------+------+\n",
      "|2008|    1|        16|      1735|   BGR|\n",
      "|2008|    1|        17|      1701|   SYR|\n",
      "|2008|    1|        17|      1225|   SAV|\n",
      "|2008|    1|        17|      1530|   CVG|\n",
      "|2008|    1|        17|      1205|   STL|\n",
      "|2008|    1|        18|      1150|   STL|\n",
      "|2008|    1|        18|      1009|   MCI|\n",
      "|2008|    1|        19|       835|   TUL|\n",
      "|2008|    1|        20|      1935|   JFK|\n",
      "|2008|    1|        20|       830|   RDU|\n",
      "|2008|    1|        21|      1640|   CVG|\n",
      "|2008|    1|        21|      1204|   MSY|\n",
      "|2008|    1|        21|      1935|   JFK|\n",
      "|2008|    1|        21|      1830|   DCA|\n",
      "|2008|    1|        21|       700|   HSV|\n",
      "|2008|    1|        22|      1910|   ORD|\n",
      "|2008|    1|        22|      1320|   CVG|\n",
      "|2008|    1|        23|       908|   LGA|\n",
      "|2008|    1|        23|      1252|   CLT|\n",
      "|2008|    1|        23|       635|   GSP|\n",
      "+----+-----+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('Year', 'Month', 'DayOfMonth', 'CRSDepTime','Origin'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cfa72e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------+------+\n",
      "|Year|Month|DayOfMonth|CRSDepTime|Origin|\n",
      "+----+-----+----------+----------+------+\n",
      "|2008|    1|         1|        10|   LAX|\n",
      "|2008|    1|         1|        15|   SMF|\n",
      "|2008|    1|         1|        25|   SMF|\n",
      "|2008|    1|         1|        25|   PHX|\n",
      "|2008|    1|         1|        30|   ANC|\n",
      "|2008|    1|         1|        30|   LAX|\n",
      "|2008|    1|         1|        30|   LAS|\n",
      "|2008|    1|         1|        30|   ONT|\n",
      "|2008|    1|         1|        35|   MCO|\n",
      "|2008|    1|         1|        35|   SFO|\n",
      "|2008|    1|         1|        40|   LAX|\n",
      "|2008|    1|         1|        40|   LAS|\n",
      "|2008|    1|         1|        40|   LAX|\n",
      "|2008|    1|         1|        40|   SEA|\n",
      "|2008|    1|         1|        40|   SFO|\n",
      "|2008|    1|         1|        40|   SEA|\n",
      "|2008|    1|         1|        45|   PHX|\n",
      "|2008|    1|         1|        45|   LAS|\n",
      "|2008|    1|         1|        50|   ANC|\n",
      "|2008|    1|         1|        53|   PDX|\n",
      "+----+-----+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('Year', 'Month', 'DayOfMonth','CRSDepTime','Origin'). \\\n",
    "    orderBy('Year', 'Month', 'DayOfMonth','CRSDepTime'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70b14589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------+------+\n",
      "|Year|Month|DayOfMonth|CRSDepTime|Origin|\n",
      "+----+-----+----------+----------+------+\n",
      "|2008|    1|         1|      2359|   PHX|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   PHX|\n",
      "|2008|    1|         1|      2359|   SLC|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   SLC|\n",
      "|2008|    1|         1|      2359|   SEA|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   TUS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2358|   LAS|\n",
      "|2008|    1|         1|      2358|   LAS|\n",
      "|2008|    1|         1|      2356|   LAS|\n",
      "+----+-----+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('Year', 'Month', 'DayOfMonth','CRSDepTime','Origin'). \\\n",
    "    orderBy('Year', 'Month', 'DayOfMonth',col('CRSDepTime').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "783a5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, 10,\n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, None,\n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, '',\n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, 2,\n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa4e9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7387a06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e06128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper,when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6da17cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
       "If :func:`pyspark.sql.Column.otherwise` is not invoked, None is returned for unmatched\n",
       "conditions.\n",
       "\n",
       ".. versionadded:: 1.4.0\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "condition : :class:`~pyspark.sql.Column`\n",
       "    a boolean :class:`~pyspark.sql.Column` expression.\n",
       "value :\n",
       "    a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
       "\n",
       ">>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n",
       "[Row(age=3), Row(age=4)]\n",
       "\n",
       ">>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n",
       "[Row(age=3), Row(age=None)]\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/functions.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "when?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8fd7d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>bonus</th><th>nationality</th><th>phone_number</th><th>ssn</th></tr>\n",
       "<tr><td>1</td><td>Scott</td><td>Tiger</td><td>1000.0</td><td>10</td><td>united states</td><td>+1 123 456 7890</td><td>123 45 6789</td></tr>\n",
       "<tr><td>2</td><td>Henry</td><td>Ford</td><td>1250.0</td><td>null</td><td>India</td><td>+91 234 567 8901</td><td>456 78 9123</td></tr>\n",
       "<tr><td>3</td><td>Nick</td><td>Junior</td><td>750.0</td><td></td><td>united KINGDOM</td><td>+44 111 111 1111</td><td>222 33 4444</td></tr>\n",
       "<tr><td>4</td><td>Bill</td><td>Gomes</td><td>1500.0</td><td>2</td><td>AUSTRALIA</td><td>+61 987 654 3210</td><td>789 12 6118</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
       "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
       "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
       "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
       "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
       "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
       "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
       "+-----------+----------+---------+------+-----+--------------+----------------+-----------+"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1907b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|sort_column|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|          0|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|          1|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|          1|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|          1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('sort_column', when(upper(col('nationality')) == 'UNITED STATES', 0).otherwise(1)). \\\n",
    "    orderBy('sort_column','nationality'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9016d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper,when, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "026d20ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|sort_column|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|          0|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|          1|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|          1|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|          1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn(\n",
    "        'sort_column',\n",
    "        expr(\"\"\"CASE WHEN upper(nationality)='UNITED STATES' THEN 0 ELSE 1 END\"\"\")). \\\n",
    "    orderBy('sort_column','nationality'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9daa8062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e1605ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy('bonus'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5a996ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(col('bonus').cast('int')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "036c5394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|bonus_cast|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+----------+\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|      null|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|      null|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|         2|\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|        10|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus_cast',col('bonus').cast('int')). \\\n",
    "    orderBy(col('bonus').cast('int')). \\ \n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d65f53ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|bonus_cast|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|        10|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|         2|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|      null|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|      null|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus_cast',col('bonus').cast('int')). \\\n",
    "    orderBy(col('bonus').cast('int').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2acf46e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = col('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee35e127",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Column in module pyspark.sql.column object:\n",
      "\n",
      "class Column(builtins.object)\n",
      " |  A column in a DataFrame.\n",
      " |  \n",
      " |  :class:`Column` instances can be created by::\n",
      " |  \n",
      " |      # 1. Select a column out of a DataFrame\n",
      " |  \n",
      " |      df.colName\n",
      " |      df[\"colName\"]\n",
      " |  \n",
      " |      # 2. Create from an expression\n",
      " |      df.colName + 1\n",
      " |      1 / df.colName\n",
      " |  \n",
      " |  .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __and__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __bool__ = __nonzero__(self)\n",
      " |  \n",
      " |  __contains__(self, item)\n",
      " |      # container operators\n",
      " |  \n",
      " |  __div__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __eq__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ge__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __getattr__(self, item)\n",
      " |  \n",
      " |  __getitem__(self, k)\n",
      " |  \n",
      " |  __gt__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __init__(self, jc)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __invert__ = _(self)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __le__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __lt__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mod__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mul__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ne__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __neg__ = _(self)\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |  \n",
      " |  __or__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __pow__ = _(self, other)\n",
      " |      binary function\n",
      " |  \n",
      " |  __radd__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rand__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rdiv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmod__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rmul__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ror__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rpow__ = _(self, other)\n",
      " |      binary function\n",
      " |  \n",
      " |  __rsub__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rtruediv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __sub__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __truediv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  alias(self, *alias, **kwargs)\n",
      " |      Returns this column aliased with a new name or names (in the case of expressions that\n",
      " |      return more than one column, such as explode).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      alias : str\n",
      " |          desired column names (collects all positional arguments passed)\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      metadata: dict\n",
      " |          a dict of information to be stored in ``metadata`` attribute of the\n",
      " |          corresponding :class:`StructField <pyspark.sql.types.StructField>` (optional, keyword\n",
      " |          only argument)\n",
      " |      \n",
      " |          .. versionchanged:: 2.2.0\n",
      " |             Added optional ``metadata`` argument.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.age.alias(\"age2\")).collect()\n",
      " |      [Row(age2=2), Row(age2=5)]\n",
      " |      >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      " |      99\n",
      " |  \n",
      " |  asc = _(self)\n",
      " |      Returns a sort expression based on ascending order of the column.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom')]\n",
      " |  \n",
      " |  asc_nulls_first = _(self)\n",
      " |      Returns a sort expression based on ascending order of the column, and null values\n",
      " |      return before non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc_nulls_first()).collect()\n",
      " |      [Row(name=None), Row(name='Alice'), Row(name='Tom')]\n",
      " |  \n",
      " |  asc_nulls_last = _(self)\n",
      " |      Returns a sort expression based on ascending order of the column, and null values\n",
      " |      appear after non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom'), Row(name=None)]\n",
      " |  \n",
      " |  astype = cast(self, dataType)\n",
      " |      :func:`astype` is an alias for :func:`cast`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  between(self, lowerBound, upperBound)\n",
      " |      A boolean expression that is evaluated to true if the value of this\n",
      " |      expression is between the given columns.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.name, df.age.between(2, 4)).show()\n",
      " |      +-----+---------------------------+\n",
      " |      | name|((age >= 2) AND (age <= 4))|\n",
      " |      +-----+---------------------------+\n",
      " |      |Alice|                       true|\n",
      " |      |  Bob|                      false|\n",
      " |      +-----+---------------------------+\n",
      " |  \n",
      " |  bitwiseAND = _(self, other)\n",
      " |      Compute bitwise AND of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise and(&) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseAND(df.b)).collect()\n",
      " |      [Row((a & b)=10)]\n",
      " |  \n",
      " |  bitwiseOR = _(self, other)\n",
      " |      Compute bitwise OR of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise or(|) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseOR(df.b)).collect()\n",
      " |      [Row((a | b)=235)]\n",
      " |  \n",
      " |  bitwiseXOR = _(self, other)\n",
      " |      Compute bitwise XOR of this expression with another expression.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column` to calculate bitwise xor(^) with\n",
      " |          this :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseXOR(df.b)).collect()\n",
      " |      [Row((a ^ b)=225)]\n",
      " |  \n",
      " |  cast(self, dataType)\n",
      " |      Convert the column into type ``dataType``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |      >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |  \n",
      " |  contains = _(self, other)\n",
      " |      Contains the other element. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          string in line. A value as a literal or a :class:`Column`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.contains('o')).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  desc = _(self)\n",
      " |      Returns a sort expression based on the descending order of the column.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice')]\n",
      " |  \n",
      " |  desc_nulls_first = _(self)\n",
      " |      Returns a sort expression based on the descending order of the column, and null values\n",
      " |      appear before non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc_nulls_first()).collect()\n",
      " |      [Row(name=None), Row(name='Tom'), Row(name='Alice')]\n",
      " |  \n",
      " |  desc_nulls_last = _(self)\n",
      " |      Returns a sort expression based on the descending order of the column, and null values\n",
      " |      appear after non-null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc_nulls_last()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice'), Row(name=None)]\n",
      " |  \n",
      " |  dropFields(self, *fieldNames)\n",
      " |      An expression that drops fields in :class:`StructType` by name.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> from pyspark.sql.functions import col, lit\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
      " |      >>> df.withColumn('a', df['a'].dropFields('b')).show()\n",
      " |      +-----------------+\n",
      " |      |                a|\n",
      " |      +-----------------+\n",
      " |      |{2, 3, {4, 5, 6}}|\n",
      " |      +-----------------+\n",
      " |      \n",
      " |      >>> df.withColumn('a', df['a'].dropFields('b', 'c')).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{3, {4, 5, 6}}|\n",
      " |      +--------------+\n",
      " |      \n",
      " |      This method supports dropping multiple nested fields directly e.g.\n",
      " |      \n",
      " |      >>> df.withColumn(\"a\", col(\"a\").dropFields(\"e.g\", \"e.h\")).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{1, 2, 3, {4}}|\n",
      " |      +--------------+\n",
      " |      \n",
      " |      However, if you are going to add/replace multiple nested fields,\n",
      " |      it is preferred to extract out the nested struct before\n",
      " |      adding/replacing multiple fields e.g.\n",
      " |      \n",
      " |      >>> df.select(col(\"a\").withField(\n",
      " |      ...     \"e\", col(\"a.e\").dropFields(\"g\", \"h\")).alias(\"a\")\n",
      " |      ... ).show()\n",
      " |      +--------------+\n",
      " |      |             a|\n",
      " |      +--------------+\n",
      " |      |{1, 2, 3, {4}}|\n",
      " |      +--------------+\n",
      " |  \n",
      " |  endswith = _(self, other)\n",
      " |      String ends with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`Column` or str\n",
      " |          string at end of line (do not use a regex `$`)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.endswith('ice')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.endswith('ice$')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  eqNullSafe = _(self, other)\n",
      " |      Equality test that is safe for null values.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other\n",
      " |          a value or :class:`Column`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df1 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value='foo'),\n",
      " |      ...     Row(id=2, value=None)\n",
      " |      ... ])\n",
      " |      >>> df1.select(\n",
      " |      ...     df1['value'] == 'foo',\n",
      " |      ...     df1['value'].eqNullSafe('foo'),\n",
      " |      ...     df1['value'].eqNullSafe(None)\n",
      " |      ... ).show()\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |(value = foo)|(value <=> foo)|(value <=> NULL)|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |         true|           true|           false|\n",
      " |      |         null|          false|            true|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(value = 'bar'),\n",
      " |      ...     Row(value = None)\n",
      " |      ... ])\n",
      " |      >>> df1.join(df2, df1[\"value\"] == df2[\"value\"]).count()\n",
      " |      0\n",
      " |      >>> df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count()\n",
      " |      1\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value=float('NaN')),\n",
      " |      ...     Row(id=2, value=42.0),\n",
      " |      ...     Row(id=3, value=None)\n",
      " |      ... ])\n",
      " |      >>> df2.select(\n",
      " |      ...     df2['value'].eqNullSafe(None),\n",
      " |      ...     df2['value'].eqNullSafe(float('NaN')),\n",
      " |      ...     df2['value'].eqNullSafe(42.0)\n",
      " |      ... ).show()\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |(value <=> NULL)|(value <=> NaN)|(value <=> 42.0)|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |           false|           true|           false|\n",
      " |      |           false|          false|            true|\n",
      " |      |            true|          false|           false|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Unlike Pandas, PySpark doesn't consider NaN values to be NULL. See the\n",
      " |      `NaN Semantics <https://spark.apache.org/docs/latest/sql-ref-datatypes.html#nan-semantics>`_\n",
      " |      for details.\n",
      " |  \n",
      " |  getField(self, name)\n",
      " |      An expression that gets a field by name in a StructField.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
      " |      >>> df.select(df.r.getField(\"b\")).show()\n",
      " |      +---+\n",
      " |      |r.b|\n",
      " |      +---+\n",
      " |      |  b|\n",
      " |      +---+\n",
      " |      >>> df.select(df.r.a).show()\n",
      " |      +---+\n",
      " |      |r.a|\n",
      " |      +---+\n",
      " |      |  1|\n",
      " |      +---+\n",
      " |  \n",
      " |  getItem(self, key)\n",
      " |      An expression that gets an item at position ``ordinal`` out of a list,\n",
      " |      or gets an item by key out of a dict.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      " |      >>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n",
      " |      +----+------+\n",
      " |      |l[0]|d[key]|\n",
      " |      +----+------+\n",
      " |      |   1| value|\n",
      " |      +----+------+\n",
      " |  \n",
      " |  isNotNull = _(self)\n",
      " |      True if the current expression is NOT null.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNotNull()).collect()\n",
      " |      [Row(name='Tom', height=80)]\n",
      " |  \n",
      " |  isNull = _(self)\n",
      " |      True if the current expression is null.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNull()).collect()\n",
      " |      [Row(name='Alice', height=None)]\n",
      " |  \n",
      " |  isin(self, *cols)\n",
      " |      A boolean expression that is evaluated to true if the value of this\n",
      " |      expression is contained by the evaluated values of the arguments.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df.age.isin([1, 2, 3])].collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  like = _(self, other)\n",
      " |      SQL like expression. Returns a boolean :class:`Column` based on a SQL LIKE match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : str\n",
      " |          a SQL LIKE pattern\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.Column.rlike\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.like('Al%')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  name = alias(self, *alias, **kwargs)\n",
      " |      :func:`name` is an alias for :func:`alias`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  otherwise(self, value)\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value\n",
      " |          a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n",
      " |      +-----+-------------------------------------+\n",
      " |      | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      " |      +-----+-------------------------------------+\n",
      " |      |Alice|                                    0|\n",
      " |      |  Bob|                                    1|\n",
      " |      +-----+-------------------------------------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.when\n",
      " |  \n",
      " |  over(self, window)\n",
      " |      Define a windowing column.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      window : :class:`WindowSpec`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Column`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Window\n",
      " |      >>> window = Window.partitionBy(\"name\").orderBy(\"age\")                 .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      " |      >>> from pyspark.sql.functions import rank, min\n",
      " |      >>> from pyspark.sql.functions import desc\n",
      " |      >>> df.withColumn(\"rank\", rank().over(window))                 .withColumn(\"min\", min('age').over(window)).sort(desc(\"age\")).show()\n",
      " |      +---+-----+----+---+\n",
      " |      |age| name|rank|min|\n",
      " |      +---+-----+----+---+\n",
      " |      |  5|  Bob|   1|  5|\n",
      " |      |  2|Alice|   1|  2|\n",
      " |      +---+-----+----+---+\n",
      " |  \n",
      " |  rlike = _(self, other)\n",
      " |      SQL RLIKE expression (LIKE with Regex). Returns a boolean :class:`Column` based on a regex\n",
      " |      match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : str\n",
      " |          an extended regex expression\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.rlike('ice$')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  startswith = _(self, other)\n",
      " |      String starts with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`Column` or str\n",
      " |          string at start of line (do not use a regex `^`)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.filter(df.name.startswith('Al')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.startswith('^Al')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  substr(self, startPos, length)\n",
      " |      Return a :class:`Column` which is a substring of the column.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      startPos : :class:`Column` or int\n",
      " |          start position\n",
      " |      length : :class:`Column` or int\n",
      " |          length of the substring\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
      " |      [Row(col='Ali'), Row(col='Bob')]\n",
      " |  \n",
      " |  when(self, condition, value)\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      condition : :class:`Column`\n",
      " |          a boolean :class:`Column` expression.\n",
      " |      value\n",
      " |          a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      |Alice|                                                          -1|\n",
      " |      |  Bob|                                                           1|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.when\n",
      " |  \n",
      " |  withField(self, fieldName, col)\n",
      " |      An expression that adds/replaces a field in :class:`StructType` by name.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> from pyspark.sql.functions import lit\n",
      " |      >>> df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n",
      " |      >>> df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n",
      " |      +---+\n",
      " |      |  b|\n",
      " |      +---+\n",
      " |      |  3|\n",
      " |      +---+\n",
      " |      >>> df.withColumn('a', df['a'].withField('d', lit(4))).select('a.d').show()\n",
      " |      +---+\n",
      " |      |  d|\n",
      " |      +---+\n",
      " |      |  4|\n",
      " |      +---+\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e4d3a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method _ in module pyspark.sql.column:\n",
      "\n",
      "_() method of pyspark.sql.column.Column instance\n",
      "    Returns a sort expression based on ascending order of the column, and null values\n",
      "    appear after non-null values.\n",
      "    \n",
      "    .. versionadded:: 2.4.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      "    >>> df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()\n",
      "    [Row(name='Alice'), Row(name='Tom'), Row(name=None)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(c.asc_nulls_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d7abc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|bonus_cast|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+----------+\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|         2|\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|        10|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|      null|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|      null|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus_cast',col('bonus').cast('int')). \\\n",
    "    orderBy(col('bonus').cast('int').asc_nulls_last()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee010f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
