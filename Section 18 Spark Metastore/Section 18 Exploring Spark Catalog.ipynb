{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0adbde4b",
   "metadata": {},
   "source": [
    "## Section 18 Exploring Spark Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Spark Metastore'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82944be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f3e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b921694d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d94ac80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.catalog.Catalog at 0x7f7bacd05320>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a0e07f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        property\n",
       "\u001b[0;31mString form:\u001b[0m <property object at 0x7f7bacb1b9a8>\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Interface through which the user may create, drop, alter or query underlying\n",
       "databases, tables, functions, etc.\n",
       "\n",
       ".. versionadded:: 2.0.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`Catalog`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50927959",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Catalog in module pyspark.sql.catalog object:\n",
      "\n",
      "class Catalog(builtins.object)\n",
      " |  User-facing catalog API, accessible through `SparkSession.catalog`.\n",
      " |  \n",
      " |  This is a thin wrapper around its Scala implementation org.apache.spark.sql.catalog.Catalog.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sparkSession)\n",
      " |      Create a new Catalog that wraps the underlying JVM object.\n",
      " |  \n",
      " |  cacheTable(self, tableName)\n",
      " |      Caches the specified table in-memory.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  clearCache(self)\n",
      " |      Removes all cached tables from the in-memory cache.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  createExternalTable(self, tableName, path=None, source=None, schema=None, **options)\n",
      " |      Creates a table based on the dataset in a data source.\n",
      " |      \n",
      " |      It returns the DataFrame associated with the external table.\n",
      " |      \n",
      " |      The data source is specified by the ``source`` and a set of ``options``.\n",
      " |      If ``source`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used.\n",
      " |      \n",
      " |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      " |      created external table.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |  \n",
      " |  createTable(self, tableName, path=None, source=None, schema=None, description=None, **options)\n",
      " |      Creates a table based on the dataset in a data source.\n",
      " |      \n",
      " |      It returns the DataFrame associated with the table.\n",
      " |      \n",
      " |      The data source is specified by the ``source`` and a set of ``options``.\n",
      " |      If ``source`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n",
      " |      created from the data at the given path. Otherwise a managed table is created.\n",
      " |      \n",
      " |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      " |      created table.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      .. versionchanged:: 3.1\n",
      " |         Added the ``description`` parameter.\n",
      " |  \n",
      " |  currentDatabase(self)\n",
      " |      Returns the current default database in this session.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  dropGlobalTempView(self, viewName)\n",
      " |      Drops the global temporary view with the given view name in the catalog.\n",
      " |      If the view has been cached before, then it will also be uncached.\n",
      " |      Returns true if this view is dropped successfully, false otherwise.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.createDataFrame([(1, 1)]).createGlobalTempView(\"my_table\")\n",
      " |      >>> spark.table(\"global_temp.my_table\").collect()\n",
      " |      [Row(_1=1, _2=1)]\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"my_table\")\n",
      " |      >>> spark.table(\"global_temp.my_table\") # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      AnalysisException: ...\n",
      " |  \n",
      " |  dropTempView(self, viewName)\n",
      " |      Drops the local temporary view with the given view name in the catalog.\n",
      " |      If the view has been cached before, then it will also be uncached.\n",
      " |      Returns true if this view is dropped successfully, false otherwise.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The return type of this method was None in Spark 2.0, but changed to Boolean\n",
      " |      in Spark 2.1.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.createDataFrame([(1, 1)]).createTempView(\"my_table\")\n",
      " |      >>> spark.table(\"my_table\").collect()\n",
      " |      [Row(_1=1, _2=1)]\n",
      " |      >>> spark.catalog.dropTempView(\"my_table\")\n",
      " |      >>> spark.table(\"my_table\") # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      AnalysisException: ...\n",
      " |  \n",
      " |  isCached(self, tableName)\n",
      " |      Returns true if the table is currently cached in-memory.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  listColumns(self, tableName, dbName=None)\n",
      " |      Returns a list of columns for the given table/view in the specified database.\n",
      " |      \n",
      " |       If no database is specified, the current database is used.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |       Notes\n",
      " |       -----\n",
      " |       the order of arguments here is different from that of its JVM counterpart\n",
      " |       because Python does not support method overloading.\n",
      " |  \n",
      " |  listDatabases(self)\n",
      " |      Returns a list of databases available across all sessions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  listFunctions(self, dbName=None)\n",
      " |      Returns a list of functions registered in the specified database.\n",
      " |      \n",
      " |      If no database is specified, the current database is used.\n",
      " |      This includes all temporary functions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  listTables(self, dbName=None)\n",
      " |      Returns a list of tables/views in the specified database.\n",
      " |      \n",
      " |      If no database is specified, the current database is used.\n",
      " |      This includes all temporary views.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  recoverPartitions(self, tableName)\n",
      " |      Recovers all the partitions of the given table and update the catalog.\n",
      " |      \n",
      " |      Only works with a partitioned table, and not a view.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.1\n",
      " |  \n",
      " |  refreshByPath(self, path)\n",
      " |      Invalidates and refreshes all the cached data (and the associated metadata) for any\n",
      " |      DataFrame that contains the given data source path.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |  \n",
      " |  refreshTable(self, tableName)\n",
      " |      Invalidates and refreshes all the cached data and metadata of the given table.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  registerFunction(self, name, f, returnType=None)\n",
      " |      An alias for :func:`spark.udf.register`.\n",
      " |      See :meth:`pyspark.sql.UDFRegistration.register`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. deprecated:: 2.3.0\n",
      " |          Use :func:`spark.udf.register` instead.\n",
      " |  \n",
      " |  setCurrentDatabase(self, dbName)\n",
      " |      Sets the current default database in this session.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  uncacheTable(self, tableName)\n",
      " |      Removes the specified table from the in-memory cache.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5752b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {username}_demo_db CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "208d561e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE {username}_demo_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0656f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f\"{username}_demo_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ec1f64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_demo_db'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc85fc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1a52f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [(\"X\",)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6551fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l,schema=\"dummy STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a43e7883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3e24715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpartitionBy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Saves the content of the :class:`DataFrame` as the specified table.\n",
       "\n",
       "In the case the table already exists, behavior of this function depends on the\n",
       "save mode, specified by the `mode` function (default to throwing an exception).\n",
       "When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n",
       "the same as that of the existing table.\n",
       "\n",
       "* `append`: Append contents of this :class:`DataFrame` to existing data.\n",
       "* `overwrite`: Overwrite existing data.\n",
       "* `error` or `errorifexists`: Throw an exception if data already exists.\n",
       "* `ignore`: Silently ignore this operation if data already exists.\n",
       "\n",
       ".. versionadded:: 1.4.0\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "name : str\n",
       "    the table name\n",
       "format : str, optional\n",
       "    the format used to save\n",
       "mode : str, optional\n",
       "    one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)\n",
       "partitionBy : str or list\n",
       "    names of partitioning columns\n",
       "**options : dict\n",
       "    all other string options\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.saveAsTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f212fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"dual\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d22a7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"dual\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "543d8704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"dual\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56e05550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dual\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1a3098b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE dual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "646ed54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7f9eed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(dummy,StringType,true)))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c76f8659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a table based on the dataset in a data source.\n",
       "\n",
       "It returns the DataFrame associated with the table.\n",
       "\n",
       "The data source is specified by the ``source`` and a set of ``options``.\n",
       "If ``source`` is not specified, the default data source configured by\n",
       "``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n",
       "created from the data at the given path. Otherwise a managed table is created.\n",
       "\n",
       "Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
       "created table.\n",
       "\n",
       ".. versionadded:: 2.2.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "\n",
       ".. versionchanged:: 3.1\n",
       "   Added the ``description`` parameter.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog.createTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33a45a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "+-----+"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.createTable(\"dual\", schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "067064c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dual', database='itv011204_demo_db', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "371000b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `spark.write` not found.\n"
     ]
    }
   ],
   "source": [
    "spark.write?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ceffa9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsertInto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Inserts the content of the :class:`DataFrame` to the specified table.\n",
       "\n",
       "It requires that the schema of the :class:`DataFrame` is the same as the\n",
       "schema of the table.\n",
       "\n",
       "Optionally overwriting any existing data.\n",
       "\n",
       ".. versionadded:: 1.4\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.insertInto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "318a8150",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.insertInto(\"dual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c5ba3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"dual\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce8d65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.insertInto(\"dual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3eaeb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.insertInto(\"dual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb53f89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "<tr><td>X</td></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "|    X|\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table(\"dual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7458fa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "|    X|\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM dual\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96278974",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"dual\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cfced595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "|    X|\n",
      "|    X|\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM dual\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4aff6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "<tr><td>X</td></tr>\n",
       "<tr><td>X</td></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "|    X|\n",
       "|    X|\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table(\"dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a556a",
   "metadata": {},
   "source": [
    "###  217 Inferring Schema While creating Spark Metastore Tables using Spark Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d001703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Spark Metastore'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c68d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f30a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c009389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e5a4ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a80ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {username}_airtraffic CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5e382b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_airtraffic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfee0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f\"{username}_airtraffic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c83091c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_airtraffic'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f846dda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-14 17:26 /user/itv011204/airtraffic_all/airport-codes\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -mkdir /user/`whoami`/airtraffic_all\n",
    "hdfs dfs -cp -f /public/airlines_all/airport-codes /user/`whoami`/airtraffic_all\n",
    "hdfs dfs -ls /user/`whoami`/airtraffic_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4779abb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   3 itv011204 supergroup      11411 2024-02-14 17:26 /user/itv011204/airtraffic_all/airport-codes/airport-codes-na.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/`whoami`/airtraffic_all/airport-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de137b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yuma\tAZ\tUSA\tYUM\tCanada\tYZFLa\tYWKCanada\tYQYada\tYZP"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/itv011204/airtraffic_all/airport-codes/airport-codes-na.txt | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb2ea86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateExternalTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a table based on the dataset in a data source.\n",
       "\n",
       "It returns the DataFrame associated with the external table.\n",
       "\n",
       "The data source is specified by the ``source`` and a set of ``options``.\n",
       "If ``source`` is not specified, the default data source configured by\n",
       "``spark.sql.sources.default`` will be used.\n",
       "\n",
       "Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
       "created external table.\n",
       "\n",
       ".. versionadded:: 2.0.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog.createExternalTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3d1e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_path = f\"/user/{username}/airtraffic_all/airport-codes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b1a7d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/user/itv011204/airtraffic_all/airport-codes/'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_codes_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "084e02d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_airtraffic'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df41fa6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS airport_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56c1918d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>City</th><th>State</th><th>Country</th><th>IATA</th></tr>\n",
       "<tr><td>Abbotsford</td><td>BC</td><td>Canada</td><td>YXX</td></tr>\n",
       "<tr><td>Aberdeen</td><td>SD</td><td>USA</td><td>ABR</td></tr>\n",
       "<tr><td>Abilene</td><td>TX</td><td>USA</td><td>ABI</td></tr>\n",
       "<tr><td>Akron</td><td>OH</td><td>USA</td><td>CAK</td></tr>\n",
       "<tr><td>Alamosa</td><td>CO</td><td>USA</td><td>ALS</td></tr>\n",
       "<tr><td>Albany</td><td>GA</td><td>USA</td><td>ABY</td></tr>\n",
       "<tr><td>Albany</td><td>NY</td><td>USA</td><td>ALB</td></tr>\n",
       "<tr><td>Albuquerque</td><td>NM</td><td>USA</td><td>ABQ</td></tr>\n",
       "<tr><td>Alexandria</td><td>LA</td><td>USA</td><td>AEX</td></tr>\n",
       "<tr><td>Allentown</td><td>PA</td><td>USA</td><td>ABE</td></tr>\n",
       "<tr><td>Alliance</td><td>NE</td><td>USA</td><td>AIA</td></tr>\n",
       "<tr><td>Alpena</td><td>MI</td><td>USA</td><td>APN</td></tr>\n",
       "<tr><td>Altoona</td><td>PA</td><td>USA</td><td>AOO</td></tr>\n",
       "<tr><td>Amarillo</td><td>TX</td><td>USA</td><td>AMA</td></tr>\n",
       "<tr><td>Anahim Lake</td><td>BC</td><td>Canada</td><td>YAA</td></tr>\n",
       "<tr><td>Anchorage</td><td>AK</td><td>USA</td><td>ANC</td></tr>\n",
       "<tr><td>Appleton</td><td>WI</td><td>USA</td><td>ATW</td></tr>\n",
       "<tr><td>Arviat</td><td>NWT</td><td>Canada</td><td>YEK</td></tr>\n",
       "<tr><td>Asheville</td><td>NC</td><td>USA</td><td>AVL</td></tr>\n",
       "<tr><td>Aspen</td><td>CO</td><td>USA</td><td>ASE</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-----------+-----+-------+----+\n",
       "|       City|State|Country|IATA|\n",
       "+-----------+-----+-------+----+\n",
       "| Abbotsford|   BC| Canada| YXX|\n",
       "|   Aberdeen|   SD|    USA| ABR|\n",
       "|    Abilene|   TX|    USA| ABI|\n",
       "|      Akron|   OH|    USA| CAK|\n",
       "|    Alamosa|   CO|    USA| ALS|\n",
       "|     Albany|   GA|    USA| ABY|\n",
       "|     Albany|   NY|    USA| ALB|\n",
       "|Albuquerque|   NM|    USA| ABQ|\n",
       "| Alexandria|   LA|    USA| AEX|\n",
       "|  Allentown|   PA|    USA| ABE|\n",
       "|   Alliance|   NE|    USA| AIA|\n",
       "|     Alpena|   MI|    USA| APN|\n",
       "|    Altoona|   PA|    USA| AOO|\n",
       "|   Amarillo|   TX|    USA| AMA|\n",
       "|Anahim Lake|   BC| Canada| YAA|\n",
       "|  Anchorage|   AK|    USA| ANC|\n",
       "|   Appleton|   WI|    USA| ATW|\n",
       "|     Arviat|  NWT| Canada| YEK|\n",
       "|  Asheville|   NC|    USA| AVL|\n",
       "|      Aspen|   CO|    USA| ASE|\n",
       "+-----------+-----+-------+----+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.createExternalTable(\n",
    "    \"airport_codes\",\n",
    "    path = airport_codes_path,\n",
    "    source = 'CSV',\n",
    "    sep = '\\t',\n",
    "    header=\"true\",\n",
    "    inferSchema = \"true\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c86c6964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Catalog.listTables of <pyspark.sql.catalog.Catalog object at 0x7f789dd97400>>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d9fa1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='itv011204_airtraffic', description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2b2a807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|    City|   string|   null|\n",
      "|   State|   string|   null|\n",
      "| Country|   string|   null|\n",
      "|    IATA|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE TABLE airport_codes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af07c521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|    City|   string|   null|\n",
      "|   State|   string|   null|\n",
      "| Country|   string|   null|\n",
      "|    IATA|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE airport_codes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fbf913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                |comment|\n",
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "|City                        |string                                                                   |null   |\n",
      "|State                       |string                                                                   |null   |\n",
      "|Country                     |string                                                                   |null   |\n",
      "|IATA                        |string                                                                   |null   |\n",
      "|                            |                                                                         |       |\n",
      "|# Detailed Table Information|                                                                         |       |\n",
      "|Database                    |itv011204_airtraffic                                                     |       |\n",
      "|Table                       |airport_codes                                                            |       |\n",
      "|Owner                       |itv011204                                                                |       |\n",
      "|Created Time                |Wed Feb 14 17:33:20 EST 2024                                             |       |\n",
      "|Last Access                 |UNKNOWN                                                                  |       |\n",
      "|Created By                  |Spark 3.1.2                                                              |       |\n",
      "|Type                        |EXTERNAL                                                                 |       |\n",
      "|Provider                    |CSV                                                                      |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv011204/airtraffic_all/airport-codes|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                       |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat                         |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat                |       |\n",
      "|Storage Properties          |[inferSchema=true, sep=\t, header=true]                                   |       |\n",
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED airport_codes\").show(200,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bb61afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                |comment|\n",
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "|City                        |string                                                                   |null   |\n",
      "|State                       |string                                                                   |null   |\n",
      "|Country                     |string                                                                   |null   |\n",
      "|IATA                        |string                                                                   |null   |\n",
      "|                            |                                                                         |       |\n",
      "|# Detailed Table Information|                                                                         |       |\n",
      "|Database                    |itv011204_airtraffic                                                     |       |\n",
      "|Table                       |airport_codes                                                            |       |\n",
      "|Owner                       |itv011204                                                                |       |\n",
      "|Created Time                |Wed Feb 14 17:33:20 EST 2024                                             |       |\n",
      "|Last Access                 |UNKNOWN                                                                  |       |\n",
      "|Created By                  |Spark 3.1.2                                                              |       |\n",
      "|Type                        |EXTERNAL                                                                 |       |\n",
      "|Provider                    |CSV                                                                      |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv011204/airtraffic_all/airport-codes|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                       |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat                         |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat                |       |\n",
      "|Storage Properties          |[inferSchema=true, sep=\t, header=true]                                   |       |\n",
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FORMATTED airport_codes\").show(200,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b92bf28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|      City|State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|   BC| Canada| YXX|\n",
      "|  Aberdeen|   SD|    USA| ABR|\n",
      "|   Abilene|   TX|    USA| ABI|\n",
      "|     Akron|   OH|    USA| CAK|\n",
      "|   Alamosa|   CO|    USA| ALS|\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"airport_codes\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc5d5421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table(\"airport_codes\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad09a392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='City', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='State', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='Country', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='IATA', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns('airport_codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640974a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0f8cf43",
   "metadata": {},
   "source": [
    "### 218 Define Schema for Spark Metastore Tables using StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7453883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Spark Metastore'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "526567d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aeff3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d87d2eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9797a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_airtraffic'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05603192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {username}_hr_db CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3e2bb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_hr_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2d41492",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_hr_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ddfd53af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_hr_db'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "edbd2487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a table based on the dataset in a data source.\n",
       "\n",
       "It returns the DataFrame associated with the table.\n",
       "\n",
       "The data source is specified by the ``source`` and a set of ``options``.\n",
       "If ``source`` is not specified, the default data source configured by\n",
       "``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n",
       "created from the data at the given path. Otherwise a managed table is created.\n",
       "\n",
       "Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
       "created table.\n",
       "\n",
       ".. versionadded:: 2.2.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "\n",
       ".. versionchanged:: 3.1\n",
       "   Added the ``description`` parameter.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog.createTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "045b0cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, \\\n",
    "                                    StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f660f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesSchema = StructType([\n",
    "    StructField('employee_id',IntegerType()),\n",
    "    StructField('first_name',StringType()),\n",
    "    StructField('last_name',StringType()),\n",
    "    StructField('salary',FloatType()),\n",
    "    StructField('nationality',StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad53f82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(employee_id,IntegerType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(salary,FloatType,true),StructField(nationality,StringType,true)))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a432298",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on StructType in module pyspark.sql.types object:\n",
      "\n",
      "class StructType(DataType)\n",
      " |  Struct type, consisting of a list of :class:`StructField`.\n",
      " |  \n",
      " |  This is the data type representing a :class:`Row`.\n",
      " |  \n",
      " |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n",
      " |  A contained :class:`StructField` can be accessed by its name or position.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct1[\"f1\"]\n",
      " |  StructField(f1,StringType,true)\n",
      " |  >>> struct1[0]\n",
      " |  StructField(f1,StringType,true)\n",
      " |  \n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct1 == struct2\n",
      " |  True\n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
      " |  ...     StructField(\"f2\", IntegerType(), False)])\n",
      " |  >>> struct1 == struct2\n",
      " |  False\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StructType\n",
      " |      DataType\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |      Access fields by name or slice.\n",
      " |  \n",
      " |  __init__(self, fields=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate the fields\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of fields.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add(self, field, data_type=None, nullable=True, metadata=None)\n",
      " |      Construct a StructType by adding new elements to it, to define the schema.\n",
      " |      The method accepts either:\n",
      " |      \n",
      " |          a) A single parameter which is a StructField object.\n",
      " |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n",
      " |             metadata(optional). The data_type parameter may be either a String or a\n",
      " |             DataType object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field : str or :class:`StructField`\n",
      " |          Either the name of the field or a StructField object\n",
      " |      data_type : :class:`DataType`, optional\n",
      " |          If present, the DataType of the StructField to create\n",
      " |      nullable : bool, optional\n",
      " |          Whether the field to add should be nullable (default True)\n",
      " |      metadata : dict, optional\n",
      " |          Any additional metadata (default None)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StructType`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True), \\\n",
      " |      ...     StructField(\"f2\", StringType(), True, None)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |  \n",
      " |  fieldNames(self)\n",
      " |      Returns all field names in a list.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct.fieldNames()\n",
      " |      ['f1']\n",
      " |  \n",
      " |  fromInternal(self, obj)\n",
      " |      Converts an internal SQL object into a native Python object.\n",
      " |  \n",
      " |  jsonValue(self)\n",
      " |  \n",
      " |  needConversion(self)\n",
      " |      Does this type needs conversion between Python object and internal SQL object.\n",
      " |      \n",
      " |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      " |  \n",
      " |  simpleString(self)\n",
      " |  \n",
      " |  toInternal(self, obj)\n",
      " |      Converts a Python object into an internal SQL object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  fromJson(json) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DataType:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  json(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from DataType:\n",
      " |  \n",
      " |  typeName() from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DataType:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(employeesSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9fe000d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'struct<employee_id:int,first_name:string,last_name:string,salary:float,nationality:string>'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesSchema.simpleString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7e20e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a table based on the dataset in a data source.\n",
       "\n",
       "It returns the DataFrame associated with the table.\n",
       "\n",
       "The data source is specified by the ``source`` and a set of ``options``.\n",
       "If ``source`` is not specified, the default data source configured by\n",
       "``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n",
       "created from the data at the given path. Otherwise a managed table is created.\n",
       "\n",
       "Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
       "created table.\n",
       "\n",
       ".. versionadded:: 2.2.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "\n",
       ".. versionchanged:: 3.1\n",
       "   Added the ``description`` parameter.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog.createTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94a50988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_hr_db'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "64ad5230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+-----------+\n",
       "|employee_id|first_name|last_name|salary|nationality|\n",
       "+-----------+----------+---------+------+-----------+\n",
       "+-----------+----------+---------+------+-----------+"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.createTable(\n",
    "    'employees',\n",
    "    schema=employeesSchema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b66b288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employees', database='itv011204_hr_db', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3197b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dual', database='itv011204_demo_db', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(f'{username}_demo_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94424475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='itv011204_airtraffic', description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(f'{username}_airtraffic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "129d6348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='employee_id', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='first_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='last_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='salary', description=None, dataType='float', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='nationality', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns('employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52ece369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-----------+\n",
      "|       database|tableName|isTemporary|\n",
      "+---------------+---------+-----------+\n",
      "|itv011204_hr_db|employees|      false|\n",
      "+---------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af7ba2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+\n",
      "|   col_name|data_type|comment|\n",
      "+-----------+---------+-------+\n",
      "|employee_id|      int|   null|\n",
      "| first_name|   string|   null|\n",
      "|  last_name|   string|   null|\n",
      "|     salary|    float|   null|\n",
      "|nationality|   string|   null|\n",
      "+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE employees\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36c3afe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                          |comment|\n",
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "|employee_id                 |int                                                                                |null   |\n",
      "|first_name                  |string                                                                             |null   |\n",
      "|last_name                   |string                                                                             |null   |\n",
      "|salary                      |float                                                                              |null   |\n",
      "|nationality                 |string                                                                             |null   |\n",
      "|                            |                                                                                   |       |\n",
      "|# Detailed Table Information|                                                                                   |       |\n",
      "|Database                    |itv011204_hr_db                                                                    |       |\n",
      "|Table                       |employees                                                                          |       |\n",
      "|Owner                       |itv011204                                                                          |       |\n",
      "|Created Time                |Wed Feb 14 18:04:57 EST 2024                                                       |       |\n",
      "|Last Access                 |UNKNOWN                                                                            |       |\n",
      "|Created By                  |Spark 3.1.2                                                                        |       |\n",
      "|Type                        |MANAGED                                                                            |       |\n",
      "|Provider                    |parquet                                                                            |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv011204/warehouse/itv011204_hr_db.db/employees|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                        |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                      |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                     |       |\n",
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED employees\").show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f6f0073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                          |comment|\n",
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "|employee_id                 |int                                                                                |null   |\n",
      "|first_name                  |string                                                                             |null   |\n",
      "|last_name                   |string                                                                             |null   |\n",
      "|salary                      |float                                                                              |null   |\n",
      "|nationality                 |string                                                                             |null   |\n",
      "|                            |                                                                                   |       |\n",
      "|# Detailed Table Information|                                                                                   |       |\n",
      "|Database                    |itv011204_hr_db                                                                    |       |\n",
      "|Table                       |employees                                                                          |       |\n",
      "|Owner                       |itv011204                                                                          |       |\n",
      "|Created Time                |Wed Feb 14 18:04:57 EST 2024                                                       |       |\n",
      "|Last Access                 |UNKNOWN                                                                            |       |\n",
      "|Created By                  |Spark 3.1.2                                                                        |       |\n",
      "|Type                        |MANAGED                                                                            |       |\n",
      "|Provider                    |parquet                                                                            |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv011204/warehouse/itv011204_hr_db.db/employees|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                        |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                      |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                     |       |\n",
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FORMATTED employees\").show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed3b4105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                         |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE `itv011204_hr_db`.`employees` (\n",
      "  `employee_id` INT,\n",
      "  `first_name` STRING,\n",
      "  `last_name` STRING,\n",
      "  `salary` FLOAT,\n",
      "  `nationality` STRING)\n",
      "USING parquet\n",
      "|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CREATE TABLE employees\").show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4954cb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+\n",
      "|database            |tableName    |isTemporary|\n",
      "+--------------------+-------------+-----------+\n",
      "|itv011204_airtraffic|airport_codes|false      |\n",
      "+--------------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SHOW TABLES FROM {username}_airtraffic\").show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4b9e96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                 |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE `itv011204_airtraffic`.`airport_codes` (\n",
      "  `City` STRING,\n",
      "  `State` STRING,\n",
      "  `Country` STRING,\n",
      "  `IATA` STRING)\n",
      "USING CSV\n",
      "OPTIONS (\n",
      "  `inferSchema` 'true',\n",
      "  `sep` '\t',\n",
      "  `header` 'true')\n",
      "LOCATION 'hdfs://m01.itversity.com:9000/user/itv011204/airtraffic_all/airport-codes'\n",
      "|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SHOW CREATE TABLE {username}_airtraffic.airport_codes\").show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943729d",
   "metadata": {},
   "source": [
    "### 219 Insering into Existing Spark Metastore tables using Spark Data Frame APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aeb79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Spark Metastore'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e27c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e321a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df375eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db697ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c4a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_hr_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2670f81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_hr_db'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c47b5459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"TRUNCATE TABLE employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8979b303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employees', database='itv011204_hr_db', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "465aaabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='employee_id', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='first_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='last_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='salary', description=None, dataType='float', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='nationality', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns('employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06a32b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \"united states\"),\n",
    "             (2, \"Henry\", \"Ford\", 1250.0, \"India\"),\n",
    "             (3, \"Nick\", \"Junior\", 750.0, \"united KINGDOM\"),\n",
    "             (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69ec6bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+-----------+\n",
       "|employee_id|first_name|last_name|salary|nationality|\n",
       "+-----------+----------+---------+------+-----------+\n",
       "+-----------+----------+---------+------+-----------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table('employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d00c9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(employee_id,IntegerType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(salary,FloatType,true),StructField(nationality,StringType,true)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table('employees').schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33548796",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "        schema = \"\"\"\n",
    "                    employee_id INT, first_name STRING, last_name STRING,\n",
    "                    salary FLOAT, nationality STRING\n",
    "                \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b568da5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|\n",
      "|          2|     Henry|     Ford|1250.0|         India|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3d5242c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(employee_id,IntegerType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(salary,FloatType,true),StructField(nationality,StringType,true)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c133b42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(employee_id,IntegerType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(salary,FloatType,true),StructField(nationality,StringType,true)))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table('employees').schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "301ab2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.write.insertInto(\"employees\",overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e857f2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th></tr>\n",
       "<tr><td>3</td><td>Nick</td><td>Junior</td><td>750.0</td><td>united KINGDOM</td></tr>\n",
       "<tr><td>1</td><td>Scott</td><td>Tiger</td><td>1000.0</td><td>united states</td></tr>\n",
       "<tr><td>4</td><td>Bill</td><td>Gomes</td><td>1500.0</td><td>AUSTRALIA</td></tr>\n",
       "<tr><td>2</td><td>Henry</td><td>Ford</td><td>1250.0</td><td>India</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+--------------+\n",
       "|employee_id|first_name|last_name|salary|   nationality|\n",
       "+-----------+----------+---------+------+--------------+\n",
       "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
       "|          1|     Scott|    Tiger|1000.0| united states|\n",
       "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
       "|          2|     Henry|     Ford|1250.0|         India|\n",
       "+-----------+----------+---------+------+--------------+"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ce34a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
      "|          1|     Scott|    Tiger|1000.0| united states|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
      "|          2|     Henry|     Ford|1250.0|         India|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table('employees').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6887130a",
   "metadata": {},
   "source": [
    "### 220 Read and Process Data from Metastore Tables using DF APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deec334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Spark Metastore'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3b862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18396114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf3eeed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a9a4454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435d762a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Function(name='!', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name='%', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name='&', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseAnd', isTemporary=True),\n",
       " Function(name='*', description=None, className='org.apache.spark.sql.catalyst.expressions.Multiply', isTemporary=True),\n",
       " Function(name='+', description=None, className='org.apache.spark.sql.catalyst.expressions.Add', isTemporary=True),\n",
       " Function(name='-', description=None, className='org.apache.spark.sql.catalyst.expressions.Subtract', isTemporary=True),\n",
       " Function(name='/', description=None, className='org.apache.spark.sql.catalyst.expressions.Divide', isTemporary=True),\n",
       " Function(name='<', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThan', isTemporary=True),\n",
       " Function(name='<=', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThanOrEqual', isTemporary=True),\n",
       " Function(name='<=>', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualNullSafe', isTemporary=True),\n",
       " Function(name='=', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name='==', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name='>', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThan', isTemporary=True),\n",
       " Function(name='>=', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual', isTemporary=True),\n",
       " Function(name='^', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseXor', isTemporary=True),\n",
       " Function(name='abs', description=None, className='org.apache.spark.sql.catalyst.expressions.Abs', isTemporary=True),\n",
       " Function(name='acos', description=None, className='org.apache.spark.sql.catalyst.expressions.Acos', isTemporary=True),\n",
       " Function(name='acosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Acosh', isTemporary=True),\n",
       " Function(name='add_months', description=None, className='org.apache.spark.sql.catalyst.expressions.AddMonths', isTemporary=True),\n",
       " Function(name='aggregate', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayAggregate', isTemporary=True),\n",
       " Function(name='and', description=None, className='org.apache.spark.sql.catalyst.expressions.And', isTemporary=True),\n",
       " Function(name='any', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='approx_count_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus', isTemporary=True),\n",
       " Function(name='approx_percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name='array', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateArray', isTemporary=True),\n",
       " Function(name='array_contains', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayContains', isTemporary=True),\n",
       " Function(name='array_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayDistinct', isTemporary=True),\n",
       " Function(name='array_except', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExcept', isTemporary=True),\n",
       " Function(name='array_intersect', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayIntersect', isTemporary=True),\n",
       " Function(name='array_join', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayJoin', isTemporary=True),\n",
       " Function(name='array_max', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMax', isTemporary=True),\n",
       " Function(name='array_min', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMin', isTemporary=True),\n",
       " Function(name='array_position', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayPosition', isTemporary=True),\n",
       " Function(name='array_remove', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRemove', isTemporary=True),\n",
       " Function(name='array_repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRepeat', isTemporary=True),\n",
       " Function(name='array_sort', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraySort', isTemporary=True),\n",
       " Function(name='array_union', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayUnion', isTemporary=True),\n",
       " Function(name='arrays_overlap', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysOverlap', isTemporary=True),\n",
       " Function(name='arrays_zip', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysZip', isTemporary=True),\n",
       " Function(name='ascii', description=None, className='org.apache.spark.sql.catalyst.expressions.Ascii', isTemporary=True),\n",
       " Function(name='asin', description=None, className='org.apache.spark.sql.catalyst.expressions.Asin', isTemporary=True),\n",
       " Function(name='asinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Asinh', isTemporary=True),\n",
       " Function(name='assert_true', description=None, className='org.apache.spark.sql.catalyst.expressions.AssertTrue', isTemporary=True),\n",
       " Function(name='atan', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan', isTemporary=True),\n",
       " Function(name='atan2', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan2', isTemporary=True),\n",
       " Function(name='atanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Atanh', isTemporary=True),\n",
       " Function(name='avg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name='base64', description=None, className='org.apache.spark.sql.catalyst.expressions.Base64', isTemporary=True),\n",
       " Function(name='bigint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bin', description=None, className='org.apache.spark.sql.catalyst.expressions.Bin', isTemporary=True),\n",
       " Function(name='binary', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bit_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitAndAgg', isTemporary=True),\n",
       " Function(name='bit_count', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseCount', isTemporary=True),\n",
       " Function(name='bit_length', description=None, className='org.apache.spark.sql.catalyst.expressions.BitLength', isTemporary=True),\n",
       " Function(name='bit_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitOrAgg', isTemporary=True),\n",
       " Function(name='bit_xor', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitXorAgg', isTemporary=True),\n",
       " Function(name='bool_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
       " Function(name='bool_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bround', description=None, className='org.apache.spark.sql.catalyst.expressions.BRound', isTemporary=True),\n",
       " Function(name='cardinality', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name='cast', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='cbrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Cbrt', isTemporary=True),\n",
       " Function(name='ceil', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
       " Function(name='ceiling', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
       " Function(name='char', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
       " Function(name='char_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='character_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='chr', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
       " Function(name='coalesce', description=None, className='org.apache.spark.sql.catalyst.expressions.Coalesce', isTemporary=True),\n",
       " Function(name='collect_list', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True),\n",
       " Function(name='collect_set', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet', isTemporary=True),\n",
       " Function(name='concat', description=None, className='org.apache.spark.sql.catalyst.expressions.Concat', isTemporary=True),\n",
       " Function(name='concat_ws', description=None, className='org.apache.spark.sql.catalyst.expressions.ConcatWs', isTemporary=True),\n",
       " Function(name='conv', description=None, className='org.apache.spark.sql.catalyst.expressions.Conv', isTemporary=True),\n",
       " Function(name='corr', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Corr', isTemporary=True),\n",
       " Function(name='cos', description=None, className='org.apache.spark.sql.catalyst.expressions.Cos', isTemporary=True),\n",
       " Function(name='cosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Cosh', isTemporary=True),\n",
       " Function(name='cot', description=None, className='org.apache.spark.sql.catalyst.expressions.Cot', isTemporary=True),\n",
       " Function(name='count', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Count', isTemporary=True),\n",
       " Function(name='count_if', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountIf', isTemporary=True),\n",
       " Function(name='count_min_sketch', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAgg', isTemporary=True),\n",
       " Function(name='covar_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation', isTemporary=True),\n",
       " Function(name='covar_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovSample', isTemporary=True),\n",
       " Function(name='crc32', description=None, className='org.apache.spark.sql.catalyst.expressions.Crc32', isTemporary=True),\n",
       " Function(name='cube', description=None, className='org.apache.spark.sql.catalyst.expressions.Cube', isTemporary=True),\n",
       " Function(name='cume_dist', description=None, className='org.apache.spark.sql.catalyst.expressions.CumeDist', isTemporary=True),\n",
       " Function(name='current_catalog', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentCatalog', isTemporary=True),\n",
       " Function(name='current_database', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True),\n",
       " Function(name='current_date', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDate', isTemporary=True),\n",
       " Function(name='current_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
       " Function(name='current_timezone', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimeZone', isTemporary=True),\n",
       " Function(name='date', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='date_add', description=None, className='org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True),\n",
       " Function(name='date_format', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFormatClass', isTemporary=True),\n",
       " Function(name='date_from_unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFromUnixDate', isTemporary=True),\n",
       " Function(name='date_part', description=None, className='org.apache.spark.sql.catalyst.expressions.DatePart', isTemporary=True),\n",
       " Function(name='date_sub', description=None, className='org.apache.spark.sql.catalyst.expressions.DateSub', isTemporary=True),\n",
       " Function(name='date_trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncTimestamp', isTemporary=True),\n",
       " Function(name='datediff', description=None, className='org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True),\n",
       " Function(name='day', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name='dayofmonth', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name='dayofweek', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfWeek', isTemporary=True),\n",
       " Function(name='dayofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfYear', isTemporary=True),\n",
       " Function(name='decimal', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='decode', description=None, className='org.apache.spark.sql.catalyst.expressions.Decode', isTemporary=True),\n",
       " Function(name='degrees', description=None, className='org.apache.spark.sql.catalyst.expressions.ToDegrees', isTemporary=True),\n",
       " Function(name='dense_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.DenseRank', isTemporary=True),\n",
       " Function(name='div', description=None, className='org.apache.spark.sql.catalyst.expressions.IntegralDivide', isTemporary=True),\n",
       " Function(name='double', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='e', description=None, className='org.apache.spark.sql.catalyst.expressions.EulerNumber', isTemporary=True),\n",
       " Function(name='element_at', description=None, className='org.apache.spark.sql.catalyst.expressions.ElementAt', isTemporary=True),\n",
       " Function(name='elt', description=None, className='org.apache.spark.sql.catalyst.expressions.Elt', isTemporary=True),\n",
       " Function(name='encode', description=None, className='org.apache.spark.sql.catalyst.expressions.Encode', isTemporary=True),\n",
       " Function(name='every', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
       " Function(name='exists', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExists', isTemporary=True),\n",
       " Function(name='exp', description=None, className='org.apache.spark.sql.catalyst.expressions.Exp', isTemporary=True),\n",
       " Function(name='explode', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
       " Function(name='explode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
       " Function(name='expm1', description=None, className='org.apache.spark.sql.catalyst.expressions.Expm1', isTemporary=True),\n",
       " Function(name='extract', description=None, className='org.apache.spark.sql.catalyst.expressions.Extract', isTemporary=True),\n",
       " Function(name='factorial', description=None, className='org.apache.spark.sql.catalyst.expressions.Factorial', isTemporary=True),\n",
       " Function(name='filter', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayFilter', isTemporary=True),\n",
       " Function(name='find_in_set', description=None, className='org.apache.spark.sql.catalyst.expressions.FindInSet', isTemporary=True),\n",
       " Function(name='first', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name='first_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name='flatten', description=None, className='org.apache.spark.sql.catalyst.expressions.Flatten', isTemporary=True),\n",
       " Function(name='float', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='floor', description=None, className='org.apache.spark.sql.catalyst.expressions.Floor', isTemporary=True),\n",
       " Function(name='forall', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayForAll', isTemporary=True),\n",
       " Function(name='format_number', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatNumber', isTemporary=True),\n",
       " Function(name='format_string', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name='from_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.CsvToStructs', isTemporary=True),\n",
       " Function(name='from_json', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonToStructs', isTemporary=True),\n",
       " Function(name='from_unixtime', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUnixTime', isTemporary=True),\n",
       " Function(name='from_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp', isTemporary=True),\n",
       " Function(name='get_json_object', description=None, className='org.apache.spark.sql.catalyst.expressions.GetJsonObject', isTemporary=True),\n",
       " Function(name='greatest', description=None, className='org.apache.spark.sql.catalyst.expressions.Greatest', isTemporary=True),\n",
       " Function(name='grouping', description=None, className='org.apache.spark.sql.catalyst.expressions.Grouping', isTemporary=True),\n",
       " Function(name='grouping_id', description=None, className='org.apache.spark.sql.catalyst.expressions.GroupingID', isTemporary=True),\n",
       " Function(name='hash', description=None, className='org.apache.spark.sql.catalyst.expressions.Murmur3Hash', isTemporary=True),\n",
       " Function(name='hex', description=None, className='org.apache.spark.sql.catalyst.expressions.Hex', isTemporary=True),\n",
       " Function(name='hour', description=None, className='org.apache.spark.sql.catalyst.expressions.Hour', isTemporary=True),\n",
       " Function(name='hypot', description=None, className='org.apache.spark.sql.catalyst.expressions.Hypot', isTemporary=True),\n",
       " Function(name='if', description=None, className='org.apache.spark.sql.catalyst.expressions.If', isTemporary=True),\n",
       " Function(name='ifnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IfNull', isTemporary=True),\n",
       " Function(name='in', description=None, className='org.apache.spark.sql.catalyst.expressions.In', isTemporary=True),\n",
       " Function(name='initcap', description=None, className='org.apache.spark.sql.catalyst.expressions.InitCap', isTemporary=True),\n",
       " Function(name='inline', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name='inline_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name='input_file_block_length', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockLength', isTemporary=True),\n",
       " Function(name='input_file_block_start', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockStart', isTemporary=True),\n",
       " Function(name='input_file_name', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileName', isTemporary=True),\n",
       " Function(name='instr', description=None, className='org.apache.spark.sql.catalyst.expressions.StringInstr', isTemporary=True),\n",
       " Function(name='int', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='isnan', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNaN', isTemporary=True),\n",
       " Function(name='isnotnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNotNull', isTemporary=True),\n",
       " Function(name='isnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNull', isTemporary=True),\n",
       " Function(name='java_method', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name='json_array_length', description=None, className='org.apache.spark.sql.catalyst.expressions.LengthOfJsonArray', isTemporary=True),\n",
       " Function(name='json_object_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonObjectKeys', isTemporary=True),\n",
       " Function(name='json_tuple', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonTuple', isTemporary=True),\n",
       " Function(name='kurtosis', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis', isTemporary=True),\n",
       " Function(name='lag', description=None, className='org.apache.spark.sql.catalyst.expressions.Lag', isTemporary=True),\n",
       " Function(name='last', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name='last_day', description=None, className='org.apache.spark.sql.catalyst.expressions.LastDay', isTemporary=True),\n",
       " Function(name='last_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name='lcase', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name='lead', description=None, className='org.apache.spark.sql.catalyst.expressions.Lead', isTemporary=True),\n",
       " Function(name='least', description=None, className='org.apache.spark.sql.catalyst.expressions.Least', isTemporary=True),\n",
       " Function(name='left', description=None, className='org.apache.spark.sql.catalyst.expressions.Left', isTemporary=True),\n",
       " Function(name='length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='levenshtein', description=None, className='org.apache.spark.sql.catalyst.expressions.Levenshtein', isTemporary=True),\n",
       " Function(name='like', description=None, className='org.apache.spark.sql.catalyst.expressions.Like', isTemporary=True),\n",
       " Function(name='ln', description=None, className='org.apache.spark.sql.catalyst.expressions.Log', isTemporary=True),\n",
       " Function(name='locate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name='log', description=None, className='org.apache.spark.sql.catalyst.expressions.Logarithm', isTemporary=True),\n",
       " Function(name='log10', description=None, className='org.apache.spark.sql.catalyst.expressions.Log10', isTemporary=True),\n",
       " Function(name='log1p', description=None, className='org.apache.spark.sql.catalyst.expressions.Log1p', isTemporary=True),\n",
       " Function(name='log2', description=None, className='org.apache.spark.sql.catalyst.expressions.Log2', isTemporary=True),\n",
       " Function(name='lower', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name='lpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLPad', isTemporary=True),\n",
       " Function(name='ltrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimLeft', isTemporary=True),\n",
       " Function(name='make_date', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeDate', isTemporary=True),\n",
       " Function(name='make_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeInterval', isTemporary=True),\n",
       " Function(name='make_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeTimestamp', isTemporary=True),\n",
       " Function(name='map', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateMap', isTemporary=True),\n",
       " Function(name='map_concat', description=None, className='org.apache.spark.sql.catalyst.expressions.MapConcat', isTemporary=True),\n",
       " Function(name='map_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapEntries', isTemporary=True),\n",
       " Function(name='map_filter', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFilter', isTemporary=True),\n",
       " Function(name='map_from_arrays', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromArrays', isTemporary=True),\n",
       " Function(name='map_from_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromEntries', isTemporary=True),\n",
       " Function(name='map_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.MapKeys', isTemporary=True),\n",
       " Function(name='map_values', description=None, className='org.apache.spark.sql.catalyst.expressions.MapValues', isTemporary=True),\n",
       " Function(name='map_zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.MapZipWith', isTemporary=True),\n",
       " Function(name='max', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Max', isTemporary=True),\n",
       " Function(name='max_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MaxBy', isTemporary=True),\n",
       " Function(name='md5', description=None, className='org.apache.spark.sql.catalyst.expressions.Md5', isTemporary=True),\n",
       " Function(name='mean', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name='min', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Min', isTemporary=True),\n",
       " Function(name='min_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MinBy', isTemporary=True),\n",
       " Function(name='minute', description=None, className='org.apache.spark.sql.catalyst.expressions.Minute', isTemporary=True),\n",
       " Function(name='mod', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name='monotonically_increasing_id', description=None, className='org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID', isTemporary=True),\n",
       " Function(name='month', description=None, className='org.apache.spark.sql.catalyst.expressions.Month', isTemporary=True),\n",
       " Function(name='months_between', description=None, className='org.apache.spark.sql.catalyst.expressions.MonthsBetween', isTemporary=True),\n",
       " Function(name='named_struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
       " Function(name='nanvl', description=None, className='org.apache.spark.sql.catalyst.expressions.NaNvl', isTemporary=True),\n",
       " Function(name='negative', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryMinus', isTemporary=True),\n",
       " Function(name='next_day', description=None, className='org.apache.spark.sql.catalyst.expressions.NextDay', isTemporary=True),\n",
       " Function(name='not', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name='now', description=None, className='org.apache.spark.sql.catalyst.expressions.Now', isTemporary=True),\n",
       " Function(name='nth_value', description=None, className='org.apache.spark.sql.catalyst.expressions.NthValue', isTemporary=True),\n",
       " Function(name='ntile', description=None, className='org.apache.spark.sql.catalyst.expressions.NTile', isTemporary=True),\n",
       " Function(name='nullif', description=None, className='org.apache.spark.sql.catalyst.expressions.NullIf', isTemporary=True),\n",
       " Function(name='nvl', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True),\n",
       " Function(name='nvl2', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl2', isTemporary=True),\n",
       " Function(name='octet_length', description=None, className='org.apache.spark.sql.catalyst.expressions.OctetLength', isTemporary=True),\n",
       " Function(name='or', description=None, className='org.apache.spark.sql.catalyst.expressions.Or', isTemporary=True),\n",
       " Function(name='overlay', description=None, className='org.apache.spark.sql.catalyst.expressions.Overlay', isTemporary=True),\n",
       " Function(name='parse_url', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseUrl', isTemporary=True),\n",
       " Function(name='percent_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.PercentRank', isTemporary=True),\n",
       " Function(name='percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Percentile', isTemporary=True),\n",
       " Function(name='percentile_approx', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name='pi', description=None, className='org.apache.spark.sql.catalyst.expressions.Pi', isTemporary=True),\n",
       " Function(name='pmod', description=None, className='org.apache.spark.sql.catalyst.expressions.Pmod', isTemporary=True),\n",
       " Function(name='posexplode', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name='posexplode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name='position', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name='positive', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryPositive', isTemporary=True),\n",
       " Function(name='pow', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name='power', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name='printf', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name='quarter', description=None, className='org.apache.spark.sql.catalyst.expressions.Quarter', isTemporary=True),\n",
       " Function(name='radians', description=None, className='org.apache.spark.sql.catalyst.expressions.ToRadians', isTemporary=True),\n",
       " Function(name='raise_error', description=None, className='org.apache.spark.sql.catalyst.expressions.RaiseError', isTemporary=True),\n",
       " Function(name='rand', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
       " Function(name='randn', description=None, className='org.apache.spark.sql.catalyst.expressions.Randn', isTemporary=True),\n",
       " Function(name='random', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
       " Function(name='rank', description=None, className='org.apache.spark.sql.catalyst.expressions.Rank', isTemporary=True),\n",
       " Function(name='reflect', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name='regexp_extract', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtract', isTemporary=True),\n",
       " Function(name='regexp_extract_all', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtractAll', isTemporary=True),\n",
       " Function(name='regexp_replace', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpReplace', isTemporary=True),\n",
       " Function(name='repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRepeat', isTemporary=True),\n",
       " Function(name='replace', description=None, className='org.apache.spark.sql.catalyst.expressions.StringReplace', isTemporary=True),\n",
       " Function(name='reverse', description=None, className='org.apache.spark.sql.catalyst.expressions.Reverse', isTemporary=True),\n",
       " Function(name='right', description=None, className='org.apache.spark.sql.catalyst.expressions.Right', isTemporary=True),\n",
       " Function(name='rint', description=None, className='org.apache.spark.sql.catalyst.expressions.Rint', isTemporary=True),\n",
       " Function(name='rlike', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
       " Function(name='rollup', description=None, className='org.apache.spark.sql.catalyst.expressions.Rollup', isTemporary=True),\n",
       " Function(name='round', description=None, className='org.apache.spark.sql.catalyst.expressions.Round', isTemporary=True),\n",
       " Function(name='row_number', description=None, className='org.apache.spark.sql.catalyst.expressions.RowNumber', isTemporary=True),\n",
       " Function(name='rpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRPad', isTemporary=True),\n",
       " Function(name='rtrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimRight', isTemporary=True),\n",
       " Function(name='schema_of_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfCsv', isTemporary=True),\n",
       " Function(name='schema_of_json', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfJson', isTemporary=True),\n",
       " Function(name='second', description=None, className='org.apache.spark.sql.catalyst.expressions.Second', isTemporary=True),\n",
       " Function(name='sentences', description=None, className='org.apache.spark.sql.catalyst.expressions.Sentences', isTemporary=True),\n",
       " Function(name='sequence', description=None, className='org.apache.spark.sql.catalyst.expressions.Sequence', isTemporary=True),\n",
       " Function(name='sha', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name='sha1', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name='sha2', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha2', isTemporary=True),\n",
       " Function(name='shiftleft', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftLeft', isTemporary=True),\n",
       " Function(name='shiftright', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRight', isTemporary=True),\n",
       " Function(name='shiftrightunsigned', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned', isTemporary=True),\n",
       " Function(name='shuffle', description=None, className='org.apache.spark.sql.catalyst.expressions.Shuffle', isTemporary=True),\n",
       " Function(name='sign', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name='signum', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name='sin', description=None, className='org.apache.spark.sql.catalyst.expressions.Sin', isTemporary=True),\n",
       " Function(name='sinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Sinh', isTemporary=True),\n",
       " Function(name='size', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name='skewness', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Skewness', isTemporary=True),\n",
       " Function(name='slice', description=None, className='org.apache.spark.sql.catalyst.expressions.Slice', isTemporary=True),\n",
       " Function(name='smallint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='some', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='sort_array', description=None, className='org.apache.spark.sql.catalyst.expressions.SortArray', isTemporary=True),\n",
       " Function(name='soundex', description=None, className='org.apache.spark.sql.catalyst.expressions.SoundEx', isTemporary=True),\n",
       " Function(name='space', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSpace', isTemporary=True),\n",
       " Function(name='spark_partition_id', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkPartitionID', isTemporary=True),\n",
       " Function(name='split', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSplit', isTemporary=True),\n",
       " Function(name='sqrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Sqrt', isTemporary=True),\n",
       " Function(name='stack', description=None, className='org.apache.spark.sql.catalyst.expressions.Stack', isTemporary=True),\n",
       " Function(name='std', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='stddev', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='stddev_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop', isTemporary=True),\n",
       " Function(name='stddev_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='str_to_map', description=None, className='org.apache.spark.sql.catalyst.expressions.StringToMap', isTemporary=True),\n",
       " Function(name='string', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
       " Function(name='substr', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name='substring', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name='substring_index', description=None, className='org.apache.spark.sql.catalyst.expressions.SubstringIndex', isTemporary=True),\n",
       " Function(name='sum', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Sum', isTemporary=True),\n",
       " Function(name='tan', description=None, className='org.apache.spark.sql.catalyst.expressions.Tan', isTemporary=True),\n",
       " Function(name='tanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Tanh', isTemporary=True),\n",
       " Function(name='timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='timestamp_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.MicrosToTimestamp', isTemporary=True),\n",
       " Function(name='timestamp_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.MillisToTimestamp', isTemporary=True),\n",
       " Function(name='timestamp_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.SecondsToTimestamp', isTemporary=True),\n",
       " Function(name='tinyint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='to_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToCsv', isTemporary=True),\n",
       " Function(name='to_date', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToDate', isTemporary=True),\n",
       " Function(name='to_json', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToJson', isTemporary=True),\n",
       " Function(name='to_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToTimestamp', isTemporary=True),\n",
       " Function(name='to_unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp', isTemporary=True),\n",
       " Function(name='to_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp', isTemporary=True),\n",
       " Function(name='transform', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayTransform', isTemporary=True),\n",
       " Function(name='transform_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformKeys', isTemporary=True),\n",
       " Function(name='transform_values', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformValues', isTemporary=True),\n",
       " Function(name='translate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTranslate', isTemporary=True),\n",
       " Function(name='trim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrim', isTemporary=True),\n",
       " Function(name='trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncDate', isTemporary=True),\n",
       " Function(name='typeof', description=None, className='org.apache.spark.sql.catalyst.expressions.TypeOf', isTemporary=True),\n",
       " Function(name='ucase', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name='unbase64', description=None, className='org.apache.spark.sql.catalyst.expressions.UnBase64', isTemporary=True),\n",
       " Function(name='unhex', description=None, className='org.apache.spark.sql.catalyst.expressions.Unhex', isTemporary=True),\n",
       " Function(name='unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixDate', isTemporary=True),\n",
       " Function(name='unix_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMicros', isTemporary=True),\n",
       " Function(name='unix_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMillis', isTemporary=True),\n",
       " Function(name='unix_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixSeconds', isTemporary=True),\n",
       " Function(name='unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixTimestamp', isTemporary=True),\n",
       " Function(name='upper', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name='uuid', description=None, className='org.apache.spark.sql.catalyst.expressions.Uuid', isTemporary=True),\n",
       " Function(name='var_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop', isTemporary=True),\n",
       " Function(name='var_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name='variance', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name='version', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkVersion', isTemporary=True),\n",
       " Function(name='weekday', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekDay', isTemporary=True),\n",
       " Function(name='weekofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekOfYear', isTemporary=True),\n",
       " Function(name='when', description=None, className='org.apache.spark.sql.catalyst.expressions.CaseWhen', isTemporary=True),\n",
       " Function(name='width_bucket', description=None, className='org.apache.spark.sql.catalyst.expressions.WidthBucket', isTemporary=True),\n",
       " Function(name='window', description=None, className='org.apache.spark.sql.catalyst.expressions.TimeWindow', isTemporary=True),\n",
       " Function(name='xpath', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathList', isTemporary=True),\n",
       " Function(name='xpath_boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean', isTemporary=True),\n",
       " Function(name='xpath_double', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name='xpath_float', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathFloat', isTemporary=True),\n",
       " Function(name='xpath_int', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathInt', isTemporary=True),\n",
       " Function(name='xpath_long', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathLong', isTemporary=True),\n",
       " Function(name='xpath_number', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name='xpath_short', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathShort', isTemporary=True),\n",
       " Function(name='xpath_string', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathString', isTemporary=True),\n",
       " Function(name='xxhash64', description=None, className='org.apache.spark.sql.catalyst.expressions.XxHash64', isTemporary=True),\n",
       " Function(name='year', description=None, className='org.apache.spark.sql.catalyst.expressions.Year', isTemporary=True),\n",
       " Function(name='zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.ZipWith', isTemporary=True),\n",
       " Function(name='|', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseOr', isTemporary=True),\n",
       " Function(name='~', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseNot', isTemporary=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listFunctions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edc604a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {username}_airlines CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bf49d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6238d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_airlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ed0961d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_airlines'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d72029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_codes_path = f\"/user/{username}/airtraffic_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6550c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {username}_airlines.airport_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc0b733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_df =spark. \\\n",
    "    read. \\\n",
    "    csv(airports_codes_path,\n",
    "       sep='\\t',\n",
    "       header=True,\n",
    "       inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b1600ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_codes_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66da358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|      City|State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|   BC| Canada| YXX|\n",
      "|  Aberdeen|   SD|    USA| ABR|\n",
      "|   Abilene|   TX|    USA| ABI|\n",
      "|     Akron|   OH|    USA| CAK|\n",
      "|   Alamosa|   CO|    USA| ALS|\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eebbe671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(airport_codes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76de84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_df.write.saveAsTable(\"airport_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9e98a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes = spark.read.table(\"airport_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba18dbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(airport_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52f38fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                 |comment|\n",
      "+----------------------------+------------------------------------------------------------------------------------------+-------+\n",
      "|City                        |string                                                                                    |null   |\n",
      "|State                       |string                                                                                    |null   |\n",
      "|Country                     |string                                                                                    |null   |\n",
      "|IATA                        |string                                                                                    |null   |\n",
      "|                            |                                                                                          |       |\n",
      "|# Detailed Table Information|                                                                                          |       |\n",
      "|Database                    |itv011204_airlines                                                                        |       |\n",
      "|Table                       |airport_codes                                                                             |       |\n",
      "|Owner                       |itv011204                                                                                 |       |\n",
      "|Created Time                |Thu Feb 15 09:15:24 EST 2024                                                              |       |\n",
      "|Last Access                 |UNKNOWN                                                                                   |       |\n",
      "|Created By                  |Spark 3.1.2                                                                               |       |\n",
      "|Type                        |MANAGED                                                                                   |       |\n",
      "|Provider                    |parquet                                                                                   |       |\n",
      "|Statistics                  |9048 bytes                                                                                |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv011204/warehouse/itv011204_airlines.db/airport_codes|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                               |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                             |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                            |       |\n",
      "+----------------------------+------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FORMATTED airport_codes\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97d1821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- IATA: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a94d4bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|State|count|\n",
      "+-----+-----+\n",
      "|   BC|   22|\n",
      "|   SD|    7|\n",
      "|   NY|   18|\n",
      "|   NM|    9|\n",
      "|   NE|    9|\n",
      "|   MI|   18|\n",
      "|  NWT|    4|\n",
      "|   NC|   10|\n",
      "|   NJ|    3|\n",
      "|   MD|    3|\n",
      "|   WV|    8|\n",
      "|   MN|    8|\n",
      "|   IL|   12|\n",
      "|   ID|    6|\n",
      "|   IA|    8|\n",
      "|   MO|    8|\n",
      "|   SC|    6|\n",
      "|   VA|    7|\n",
      "|  PEI|    1|\n",
      "|   TN|    6|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes.\\\n",
    "    groupBy('State'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e76c36e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,lit, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d31e1fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|State|AirportCount|\n",
      "+-----+------------+\n",
      "|   CA|          29|\n",
      "|   TX|          26|\n",
      "|   AK|          25|\n",
      "|   BC|          22|\n",
      "|   NY|          18|\n",
      "|   MI|          18|\n",
      "|   FL|          18|\n",
      "|   ON|          18|\n",
      "|   MT|          14|\n",
      "|   PA|          13|\n",
      "|   PQ|          13|\n",
      "|   IL|          12|\n",
      "|   CO|          12|\n",
      "|   WY|          10|\n",
      "|   NC|          10|\n",
      "|   NM|           9|\n",
      "|   NE|           9|\n",
      "|   GA|           9|\n",
      "|   KS|           9|\n",
      "|   WA|           9|\n",
      "+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes.\\\n",
    "    groupBy('State'). \\\n",
    "    agg(count(lit(1)).alias('AirportCount')). \\\n",
    "    orderBy(col(\"AirportCount\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e3d9ad",
   "metadata": {},
   "source": [
    "### 221 Create Spark Metastore Partitioned Tables using Data Frame APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef4da68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.sql.ui.port','0'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Exploring spark catalog'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c10864",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a table based on the dataset in a data source.\n",
       "\n",
       "It returns the DataFrame associated with the table.\n",
       "\n",
       "The data source is specified by the ``source`` and a set of ``options``.\n",
       "If ``source`` is not specified, the default data source configured by\n",
       "``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n",
       "created from the data at the given path. Otherwise a managed table is created.\n",
       "\n",
       "Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
       "created table.\n",
       "\n",
       ".. versionadded:: 2.2.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "\n",
       ".. versionchanged:: 3.1\n",
       "   Added the ``description`` parameter.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog.createTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89b20b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4aedfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions','2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea58269e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f'DROP DATABASE IF EXISTS {username}_retail CASCADE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f8a1e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f'CREATE DATABASE IF NOT EXISTS {username}_retail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64ba37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_retail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81ed9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_path  ='/public/retail_db/orders'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "780bf95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 hdfs supergroup    2999944 2021-01-28 09:27 /public/retail_db/orders/part-00000\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /public/retail_db/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b4c4a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS orders_part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f493b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /user/`whoami`/retail_db/orders_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4094792c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `/user/itv011204/retail_db/orders_part': No such file or directory\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\nhdfs dfs -rm -R /user/`whoami`/retail_db/orders_part\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-020c881010d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nhdfs dfs -rm -R /user/`whoami`/retail_db/orders_part\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2371\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2372\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\nhdfs dfs -rm -R /user/`whoami`/retail_db/orders_part\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -rm -R /user/`whoami`/retail_db/orders_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89a511b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4794fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark. \\\n",
    "    read. \\\n",
    "    csv(\n",
    "        orders_path,\n",
    "        schema = \"\"\"order_id INT, order_date DATE, \n",
    "                order_customer_id INT, order_status STRING\"\"\"\n",
    "    ). \\\n",
    "    withColumn('order_month',date_format(col('order_date'),'yyyyMM')). \\\n",
    "    write. \\\n",
    "    partitionBy('order_month'). \\\n",
    "    parquet(f'/user/{username}/retail_db/orders_part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fe237e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 items\n",
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/_SUCCESS\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201307\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201308\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201309\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201310\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201311\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201312\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201401\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201402\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201403\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201404\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201405\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201406\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201407\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/`whoami`/retail_db/orders_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b306bc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/_SUCCESS\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201307\n",
      "-rw-r--r--   3 itv011204 supergroup      14435 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201307/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201308\n",
      "-rw-r--r--   3 itv011204 supergroup      49997 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201308/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201309\n",
      "-rw-r--r--   3 itv011204 supergroup      51358 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201309/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201310\n",
      "-rw-r--r--   3 itv011204 supergroup      47051 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201310/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201311\n",
      "-rw-r--r--   3 itv011204 supergroup      55949 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201311/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201312\n",
      "-rw-r--r--   3 itv011204 supergroup      51794 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201312/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201401\n",
      "-rw-r--r--   3 itv011204 supergroup      51938 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201401/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201402\n",
      "-rw-r--r--   3 itv011204 supergroup      49591 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201402/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201403\n",
      "-rw-r--r--   3 itv011204 supergroup      50816 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201403/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201404\n",
      "-rw-r--r--   3 itv011204 supergroup      49799 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201404/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201405\n",
      "-rw-r--r--   3 itv011204 supergroup      48183 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201405/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201406\n",
      "-rw-r--r--   3 itv011204 supergroup      46828 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201406/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201407\n",
      "-rw-r--r--   3 itv011204 supergroup      39605 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201407/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls -R /user/`whoami`/retail_db/orders_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "592f8c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|2013-07-25|            11599|         CLOSED|\n",
      "|       2|2013-07-25|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|            12111|       COMPLETE|\n",
      "|       4|2013-07-25|             8827|         CLOSED|\n",
      "|       5|2013-07-25|            11318|       COMPLETE|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/{username}/retail_db/orders_part/order_month=201307').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5a0f15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1533"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/{username}/retail_db/orders_part/order_month=201307').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab8b2518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|   45168|2014-05-01|             2383|       COMPLETE|\n",
      "|   45169|2014-05-01|             7212|PENDING_PAYMENT|\n",
      "|   45170|2014-05-01|             2400|SUSPECTED_FRAUD|\n",
      "|   45171|2014-05-01|             9003|PENDING_PAYMENT|\n",
      "|   45172|2014-05-01|             2508|PENDING_PAYMENT|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/itv011204/retail_db/orders_part/order_month=201405').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88caa0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5467"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/itv011204/retail_db/orders_part/order_month=201405').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8b9003a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|   15488|2013-11-01|             8987|PENDING_PAYMENT|     201311|\n",
      "|   15489|2013-11-01|             5359|PENDING_PAYMENT|     201311|\n",
      "|   15490|2013-11-01|            10149|       COMPLETE|     201311|\n",
      "|   15491|2013-11-01|            10635|        ON_HOLD|     201311|\n",
      "|   15492|2013-11-01|             7784|PENDING_PAYMENT|     201311|\n",
      "|   15493|2013-11-01|             1104|        ON_HOLD|     201311|\n",
      "|   15494|2013-11-01|             7313|     PROCESSING|     201311|\n",
      "|   15495|2013-11-01|             7067|         CLOSED|     201311|\n",
      "|   15496|2013-11-01|            12153|PENDING_PAYMENT|     201311|\n",
      "|   15497|2013-11-01|            11115|PENDING_PAYMENT|     201311|\n",
      "|   15498|2013-11-01|            11195|       COMPLETE|     201311|\n",
      "|   15499|2013-11-01|             7113|         CLOSED|     201311|\n",
      "|   15500|2013-11-01|             6780|PENDING_PAYMENT|     201311|\n",
      "|   15501|2013-11-01|             9703|        ON_HOLD|     201311|\n",
      "|   15502|2013-11-01|            10009|       COMPLETE|     201311|\n",
      "|   15503|2013-11-01|             6521|PENDING_PAYMENT|     201311|\n",
      "|   15504|2013-11-01|            10601|PENDING_PAYMENT|     201311|\n",
      "|   15505|2013-11-01|             1068|PENDING_PAYMENT|     201311|\n",
      "|   15506|2013-11-01|             2742|PENDING_PAYMENT|     201311|\n",
      "|   15507|2013-11-01|             3503|       COMPLETE|     201311|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/itv011204/retail_db/orders_part').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c3d5094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/itv011204/retail_db/orders_part').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "805eed22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>order_id</th><th>order_date</th><th>order_customer_id</th><th>order_status</th><th>order_month</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+----------+-----------------+------------+-----------+\n",
       "|order_id|order_date|order_customer_id|order_status|order_month|\n",
       "+--------+----------+-----------------+------------+-----------+\n",
       "+--------+----------+-----------------+------------+-----------+"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark. \\\n",
    "    catalog. \\\n",
    "    createTable('orders_part',\n",
    "        path=f'/user/{username}/retail_db/orders_part',\n",
    "        source='parquet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2444693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+------------+-----------+\n",
      "|order_id|order_date|order_customer_id|order_status|order_month|\n",
      "+--------+----------+-----------------+------------+-----------+\n",
      "+--------+----------+-----------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table('orders_part').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8c3f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|partition|\n",
      "+---------+\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW PARTITIONS orders_part\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91525c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.recoverPartitions('orders_part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b91f070c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         partition|\n",
      "+------------------+\n",
      "|order_month=201307|\n",
      "|order_month=201308|\n",
      "|order_month=201309|\n",
      "|order_month=201310|\n",
      "|order_month=201311|\n",
      "|order_month=201312|\n",
      "|order_month=201401|\n",
      "|order_month=201402|\n",
      "|order_month=201403|\n",
      "|order_month=201404|\n",
      "|order_month=201405|\n",
      "|order_month=201406|\n",
      "|order_month=201407|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW PARTITIONS orders_part\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c46038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|   15488|2013-11-01|             8987|PENDING_PAYMENT|     201311|\n",
      "|   15489|2013-11-01|             5359|PENDING_PAYMENT|     201311|\n",
      "|   15490|2013-11-01|            10149|       COMPLETE|     201311|\n",
      "|   15491|2013-11-01|            10635|        ON_HOLD|     201311|\n",
      "|   15492|2013-11-01|             7784|PENDING_PAYMENT|     201311|\n",
      "|   15493|2013-11-01|             1104|        ON_HOLD|     201311|\n",
      "|   15494|2013-11-01|             7313|     PROCESSING|     201311|\n",
      "|   15495|2013-11-01|             7067|         CLOSED|     201311|\n",
      "|   15496|2013-11-01|            12153|PENDING_PAYMENT|     201311|\n",
      "|   15497|2013-11-01|            11115|PENDING_PAYMENT|     201311|\n",
      "|   15498|2013-11-01|            11195|       COMPLETE|     201311|\n",
      "|   15499|2013-11-01|             7113|         CLOSED|     201311|\n",
      "|   15500|2013-11-01|             6780|PENDING_PAYMENT|     201311|\n",
      "|   15501|2013-11-01|             9703|        ON_HOLD|     201311|\n",
      "|   15502|2013-11-01|            10009|       COMPLETE|     201311|\n",
      "|   15503|2013-11-01|             6521|PENDING_PAYMENT|     201311|\n",
      "|   15504|2013-11-01|            10601|PENDING_PAYMENT|     201311|\n",
      "|   15505|2013-11-01|             1068|PENDING_PAYMENT|     201311|\n",
      "|   15506|2013-11-01|             2742|PENDING_PAYMENT|     201311|\n",
      "|   15507|2013-11-01|             3503|       COMPLETE|     201311|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table('orders_part').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bb69983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|order_month|count(1)|\n",
      "+-----------+--------+\n",
      "|     201405|    5467|\n",
      "|     201308|    5680|\n",
      "|     201404|    5657|\n",
      "|     201311|    6381|\n",
      "|     201401|    5908|\n",
      "|     201309|    5841|\n",
      "|     201312|    5892|\n",
      "|     201403|    5778|\n",
      "|     201402|    5635|\n",
      "|     201310|    5335|\n",
      "|     201406|    5308|\n",
      "|     201407|    4468|\n",
      "|     201307|    1533|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT order_month, count(1) FROM orders_part GROUP BY order_month\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33749ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+-------+\n",
      "|col_name               |data_type|comment|\n",
      "+-----------------------+---------+-------+\n",
      "|order_id               |int      |null   |\n",
      "|order_date             |date     |null   |\n",
      "|order_customer_id      |int      |null   |\n",
      "|order_status           |string   |null   |\n",
      "|order_month            |int      |null   |\n",
      "|# Partition Information|         |       |\n",
      "|# col_name             |data_type|comment|\n",
      "|order_month            |int      |null   |\n",
      "+-----------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE orders_part\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f64d0fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|order_month|count|\n",
      "+-----------+-----+\n",
      "|     201311| 6381|\n",
      "|     201401| 5908|\n",
      "|     201309| 5841|\n",
      "|     201308| 5680|\n",
      "|     201404| 5657|\n",
      "|     201405| 5467|\n",
      "|     201310| 5335|\n",
      "|     201406| 5308|\n",
      "|     201407| 4468|\n",
      "|     201403| 5778|\n",
      "|     201402| 5635|\n",
      "|     201307| 1533|\n",
      "|     201312| 5892|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table('orders_part'). \\\n",
    "    groupBy('order_month'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464dac55",
   "metadata": {},
   "source": [
    "### 222 Saving as Spark Metastore Partitioned table using Data Frame APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63fa99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.sql.ui.port','0'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Exploring spark catalog'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf43b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions','2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d6ed31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e3baa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_retail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cf45020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_retail'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538f831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_path = '/public/retail_db/orders'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1808ec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 hdfs supergroup    2999944 2021-01-28 09:27 /public/retail_db/orders/part-00000\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /public/retail_db/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9482534e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "014-06-12 00:00:00.0,4229,PENDING\n",
      "68861,2014-06-13 00:00:00.0,3031,PENDING_PAYMENT\n",
      "68862,2014-06-15 00:00:00.0,7326,PROCESSING\n",
      "68863,2014-06-16 00:00:00.0,3361,CLOSED\n",
      "68864,2014-06-18 00:00:00.0,9634,ON_HOLD\n",
      "68865,2014-06-19 00:00:00.0,4567,SUSPECTED_FRAUD\n",
      "68866,2014-06-20 00:00:00.0,3890,PENDING_PAYMENT\n",
      "68867,2014-06-23 00:00:00.0,869,CANCELED\n",
      "68868,2014-06-24 00:00:00.0,10184,PENDING\n",
      "68869,2014-06-25 00:00:00.0,7456,PROCESSING\n",
      "68870,2014-06-26 00:00:00.0,3343,COMPLETE\n",
      "68871,2014-06-28 00:00:00.0,4960,PENDING\n",
      "68872,2014-06-29 00:00:00.0,3354,COMPLETE\n",
      "68873,2014-06-30 00:00:00.0,4545,PENDING\n",
      "68874,2014-07-03 00:00:00.0,1601,COMPLETE\n",
      "68875,2014-07-04 00:00:00.0,10637,ON_HOLD\n",
      "68876,2014-07-06 00:00:00.0,4124,COMPLETE\n",
      "68877,2014-07-07 00:00:00.0,9692,ON_HOLD\n",
      "68878,2014-07-08 00:00:00.0,6753,COMPLETE\n",
      "68879,2014-07-09 00:00:00.0,778,COMPLETE\n",
      "68880,2014-07-13 00:00:00.0,1117,COMPLETE\n",
      "68881,2014-07-19 00:00:00.0,2518,PENDING_PAYMENT\n",
      "68882,2014-07-22 00:00:00.0,10000,ON_HOLD\n",
      "68883,2014-07-23 00:00:00.0,5533,COMPLETE\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -tail /public/retail_db/orders/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1be75c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS orders_part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "681444fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61b80852",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark. \\\n",
    "    read. \\\n",
    "    csv(orders_path,\n",
    "        schema = \"\"\"order_id INT, order_date DATE,\n",
    "                order_customer_id INT, order_status STRING\n",
    "                \"\"\"\n",
    "    ). \\\n",
    "    withColumn('order_month',date_format('order_date','yyyyMM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de1b7aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_month: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27077133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|       1|2013-07-25|            11599|         CLOSED|     201307|\n",
      "|       2|2013-07-25|              256|PENDING_PAYMENT|     201307|\n",
      "|       3|2013-07-25|            12111|       COMPLETE|     201307|\n",
      "|       4|2013-07-25|             8827|         CLOSED|     201307|\n",
      "|       5|2013-07-25|            11318|       COMPLETE|     201307|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9619bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders. \\\n",
    "    write. \\\n",
    "    saveAsTable(\n",
    "        'orders_part',\n",
    "        mode='overwrite',\n",
    "        partitionBy='order_month'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddd91996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/_SUCCESS\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201307\n",
      "-rw-r--r--   3 itv011204 supergroup      14435 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201307/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201308\n",
      "-rw-r--r--   3 itv011204 supergroup      49997 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201308/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201309\n",
      "-rw-r--r--   3 itv011204 supergroup      51358 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201309/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201310\n",
      "-rw-r--r--   3 itv011204 supergroup      47051 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201310/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201311\n",
      "-rw-r--r--   3 itv011204 supergroup      55949 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201311/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201312\n",
      "-rw-r--r--   3 itv011204 supergroup      51794 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201312/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201401\n",
      "-rw-r--r--   3 itv011204 supergroup      51938 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201401/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201402\n",
      "-rw-r--r--   3 itv011204 supergroup      49591 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201402/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201403\n",
      "-rw-r--r--   3 itv011204 supergroup      50816 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201403/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201404\n",
      "-rw-r--r--   3 itv011204 supergroup      49799 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201404/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201405\n",
      "-rw-r--r--   3 itv011204 supergroup      48183 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201405/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201406\n",
      "-rw-r--r--   3 itv011204 supergroup      46828 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201406/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201407\n",
      "-rw-r--r--   3 itv011204 supergroup      39605 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201407/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "\n",
    "hdfs dfs -ls -R /user/`whoami`/warehouse/`whoami`_retail.db/orders_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1e9db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|2013-07-25|            11599|         CLOSED|\n",
      "|       2|2013-07-25|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|            12111|       COMPLETE|\n",
      "|       4|2013-07-25|             8827|         CLOSED|\n",
      "|       5|2013-07-25|            11318|       COMPLETE|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f\"/user/{username}/warehouse/{username}_retail.db/orders_part/order_month=201307\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "749330bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|   15488|2013-11-01|             8987|PENDING_PAYMENT|     201311|\n",
      "|   15489|2013-11-01|             5359|PENDING_PAYMENT|     201311|\n",
      "|   15490|2013-11-01|            10149|       COMPLETE|     201311|\n",
      "|   15491|2013-11-01|            10635|        ON_HOLD|     201311|\n",
      "|   15492|2013-11-01|             7784|PENDING_PAYMENT|     201311|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f\"/user/{username}/warehouse/{username}_retail.db/orders_part\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68146215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         partition|\n",
      "+------------------+\n",
      "|order_month=201307|\n",
      "|order_month=201308|\n",
      "|order_month=201309|\n",
      "|order_month=201310|\n",
      "|order_month=201311|\n",
      "|order_month=201312|\n",
      "|order_month=201401|\n",
      "|order_month=201402|\n",
      "|order_month=201403|\n",
      "|order_month=201404|\n",
      "|order_month=201405|\n",
      "|order_month=201406|\n",
      "|order_month=201407|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW PARTITIONS orders_part\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f97f5b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|   15488|2013-11-01|             8987|PENDING_PAYMENT|     201311|\n",
      "|   15489|2013-11-01|             5359|PENDING_PAYMENT|     201311|\n",
      "|   15490|2013-11-01|            10149|       COMPLETE|     201311|\n",
      "|   15491|2013-11-01|            10635|        ON_HOLD|     201311|\n",
      "|   15492|2013-11-01|             7784|PENDING_PAYMENT|     201311|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table('orders_part').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "feede0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|order_month|count(1)|\n",
      "+-----------+--------+\n",
      "|     201403|    5778|\n",
      "|     201308|    5680|\n",
      "|     201404|    5657|\n",
      "|     201406|    5308|\n",
      "|     201402|    5635|\n",
      "+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT order_month, count(1) from orders_part GROUP BY order_month\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f01bd28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|order_month|count|\n",
      "+-----------+-----+\n",
      "|     201406| 5308|\n",
      "|     201403| 5778|\n",
      "|     201308| 5680|\n",
      "|     201404| 5657|\n",
      "|     201311| 6381|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark. \\\n",
    "    read. \\\n",
    "    table(\"orders_part\"). \\\n",
    "    groupBy('order_month'). \\\n",
    "    count(). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4fc95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 223 Creating Temporary views on top of Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65506f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.sql.ui.port','0'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Exploring spark catalog'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1b3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions','2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1571b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_airlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aefb226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_airlines'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6968031f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='itv011204_airlines', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "854a92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_cods_path = f\"/public/airlines_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d8f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_df = spark. \\\n",
    "    read. \\\n",
    "    csv(\n",
    "        airport_cods_path,\n",
    "        sep = '\\t',\n",
    "        header = True,\n",
    "        inferSchema = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "616e0baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_codes_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b922841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- IATA: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a46f2cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|      City|State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|   BC| Canada| YXX|\n",
      "|  Aberdeen|   SD|    USA| ABR|\n",
      "|   Abilene|   TX|    USA| ABI|\n",
      "|     Akron|   OH|    USA| CAK|\n",
      "|   Alamosa|   CO|    USA| ALS|\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb995b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='itv011204_airlines', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2016a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_df.createTempView('airport_codes_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41037541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='itv011204_airlines', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='airport_codes_v', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c6ebee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createGlobalTempView in module pyspark.sql.dataframe:\n",
      "\n",
      "createGlobalTempView(name) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Creates a global temporary view with this :class:`DataFrame`.\n",
      "    \n",
      "    The lifetime of this temporary view is tied to this Spark application.\n",
      "    throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      "    catalog.\n",
      "    \n",
      "    .. versionadded:: 2.1.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.createGlobalTempView(\"people\")\n",
      "    >>> df2 = spark.sql(\"select * from global_temp.people\")\n",
      "    >>> sorted(df.collect()) == sorted(df2.collect())\n",
      "    True\n",
      "    >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      "    >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(airport_codes_df.createGlobalTempView)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf1b5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_df.createGlobalTempView('airport_codes_g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "035208fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>database</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>itv011204_airlines</td><td>airport_codes</td><td>false</td></tr>\n",
       "<tr><td></td><td>airport_codes_v</td><td>true</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------------+---------------+-----------+\n",
       "|          database|      tableName|isTemporary|\n",
       "+------------------+---------------+-----------+\n",
       "|itv011204_airlines|  airport_codes|      false|\n",
       "|                  |airport_codes_v|       true|\n",
       "+------------------+---------------+-----------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"show tables;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3659099d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>namespace</th><th>viewName</th><th>isTemporary</th></tr>\n",
       "<tr><td>global_temp</td><td>airport_codes_g</td><td>true</td></tr>\n",
       "<tr><td></td><td>airport_codes_v</td><td>true</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+---------------+-----------+\n",
       "|  namespace|       viewName|isTemporary|\n",
       "+-----------+---------------+-----------+\n",
       "|global_temp|airport_codes_g|       true|\n",
       "|           |airport_codes_v|       true|\n",
       "+-----------+---------------+-----------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"show views in global_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df0c938f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|      City|State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|   BC| Canada| YXX|\n",
      "|  Aberdeen|   SD|    USA| ABR|\n",
      "|   Abilene|   TX|    USA| ABI|\n",
      "|     Akron|   OH|    USA| CAK|\n",
      "|   Alamosa|   CO|    USA| ALS|\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from global_temp.airport_codes_g\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6b29d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|      City|State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|   BC| Canada| YXX|\n",
      "|  Aberdeen|   SD|    USA| ABR|\n",
      "|   Abilene|   TX|    USA| ABI|\n",
      "|     Akron|   OH|    USA| CAK|\n",
      "|   Alamosa|   CO|    USA| ALS|\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table('global_temp.airport_codes_g').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65346841",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes = spark.read.table('airport_codes_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f0105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49fcde07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|state|count|\n",
      "+-----+-----+\n",
      "|   BC|   22|\n",
      "|   SD|    7|\n",
      "|   NY|   18|\n",
      "|   NM|    9|\n",
      "|   NE|    9|\n",
      "|   MI|   18|\n",
      "|  NWT|    4|\n",
      "|   NC|   10|\n",
      "|   NJ|    3|\n",
      "|   MD|    3|\n",
      "|   WV|    8|\n",
      "|   MN|    8|\n",
      "|   IL|   12|\n",
      "|   ID|    6|\n",
      "|   IA|    8|\n",
      "|   MO|    8|\n",
      "|   SC|    6|\n",
      "|   VA|    7|\n",
      "|  PEI|    1|\n",
      "|   TN|    6|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes. \\\n",
    "    groupBy(\"state\"). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9663f408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|state|airport_count|\n",
      "+-----+-------------+\n",
      "|   CA|           29|\n",
      "|   TX|           26|\n",
      "|   AK|           25|\n",
      "|   BC|           22|\n",
      "|   NY|           18|\n",
      "|   ON|           18|\n",
      "|   MI|           18|\n",
      "|   FL|           18|\n",
      "|   MT|           14|\n",
      "|   PA|           13|\n",
      "|   PQ|           13|\n",
      "|   IL|           12|\n",
      "|   CO|           12|\n",
      "|   NC|           10|\n",
      "|   WY|           10|\n",
      "|   NE|            9|\n",
      "|   WI|            9|\n",
      "|   WA|            9|\n",
      "|   GA|            9|\n",
      "|   NM|            9|\n",
      "+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT state, count(1) airport_count FROM airport_codes_v GROUP BY state ORDER BY airport_count desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "411afc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Chennai\n",
      "Hello Trichy\n",
      "Hello Madurai\n",
      "Hello Coimbatore\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose City by entering number 1\n",
      "Enter number of seats you want to book 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-33a862c72fea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mno_of_seats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter number of seats you want to book\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"price for \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mno_of_seats\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" to \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mCities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcity_selected\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" is \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mPrice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcity_selected\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mno_of_seats\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not int"
     ]
    }
   ],
   "source": [
    "Cities = ['Chennai', 'Trichy', 'Madurai', 'Coimbatore']\n",
    "Price = [10,20,30,40]\n",
    "\n",
    "serial =1\n",
    "\n",
    "for city in Cities:\n",
    "    print(\"Hello\"+\" \"+ city)\n",
    "    serial = serial + 1\n",
    "\n",
    "city_selected = int(input(\"Choose City by entering number\"))\n",
    "no_of_seats = int(input(\"Enter number of seats you want to book\"))\n",
    "\n",
    "print(\"price for \"+no_of_seats+\" to \"+ Cities[city_selected-1] + \" is \"+Price[city_selected -1]*no_of_seats  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1be07699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 1: Hello\n",
      "String 2: World\n"
     ]
    }
   ],
   "source": [
    "str1=\"Hello\"\n",
    "str2=\"World\"\n",
    "print (\"String 1:\",str1)\n",
    "print (\"String 2:\",str2)\n",
    "str=str1+str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31621247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.sql.ui.port','0'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Exploring spark catalog'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e60acd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49b33a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f'CREATE DATABASE IF NOT EXISTS {username}_retail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d04d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_retail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1020a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_retail'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aa2b09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS orders_part2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3949f17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 hdfs supergroup    2999944 2021-01-28 09:27 /public/retail_db/orders/part-00000\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /public/retail_db/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cc6bd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 items\n",
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/_SUCCESS\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201307\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201308\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201309\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201310\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201311\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201312\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201401\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201402\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201403\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201404\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201405\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201406\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201407\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/`whoami`/retail_db/orders_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9694d0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/itv011204/retail_db/orders_part\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -rm -R -skipTrash /user/`whoami`/retail_db/orders_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2d2b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_path = \"/public/retail_db/orders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a035cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af9e5037",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read. \\\n",
    "    csv(\n",
    "        orders_path,\n",
    "        schema = \"\"\"\n",
    "                    order_id INT, order_date DATE,\n",
    "                    order_customer_id INT, order_status STRING\n",
    "                \"\"\"\n",
    "    ). \\\n",
    "    withColumn('order_year',date_format('order_date','yyyy')). \\\n",
    "    withColumn('order_month',date_format('order_date','MM')). \\\n",
    "    write. \\\n",
    "    partitionBy('order_year','order_month'). \\\n",
    "    parquet(f'/user/{username}/retail_db/orders_part2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3c0aba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/_SUCCESS\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2013\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/`whoami`/retail_db/orders_part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "837b8e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/_SUCCESS\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2013\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2013/order_month=07\n",
      "-rw-r--r--   3 itv011204 supergroup      14435 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2013/order_month=07/part-00000-344acf22-02e6-4a65-9d9b-be9ee31a6267.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2013/order_month=08\n",
      "-rw-r--r--   3 itv011204 supergroup      49997 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2013/order_month=08/part-00000-344acf22-02e6-4a65-9d9b-be9ee31a6267.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2013/order_month=09\n",
      "-rw-r--r--   3 itv011204 supergroup      51358 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2013/order_month=09/part-00000-344acf22-02e6-4a65-9d9b-be9ee31a6267.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2013/order_month=10\n",
      "-rw-r--r--   3 itv011204 supergroup      47051 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2013/order_month=10/part-00000-344acf22-02e6-4a65-9d9b-be9ee31a6267.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2013/order_month=11\n",
      "-rw-r--r--   3 itv011204 supergroup      55949 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2013/order_month=11/part-00000-344acf22-02e6-4a65-9d9b-be9ee31a6267.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2013/order_month=12\n",
      "-rw-r--r--   3 itv011204 supergroup      51794 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2013/order_month=12/part-00000-344acf22-02e6-4a65-9d9b-be9ee31a6267.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014/order_month=01\n",
      "-rw-r--r--   3 itv011204 supergroup      51938 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014/order_month=01/part-00000-344acf22-02e6-4a65-9d9b-be9ee31a6267.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014/order_month=02\n",
      "-rw-r--r--   3 itv011204 supergroup      49591 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014/order_month=02/part-00000-344acf22-02e6-4a65-9d9b-be9ee31a6267.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014/order_month=03\n",
      "-rw-r--r--   3 itv011204 supergroup      50816 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014/order_month=03/part-00000-344acf22-02e6-4a65-9d9b-be9ee31a6267.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014/order_month=04\n",
      "-rw-r--r--   3 itv011204 supergroup      49799 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014/order_month=04/part-00000-344acf22-02e6-4a65-9d9b-be9ee31a6267.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014/order_month=05\n",
      "-rw-r--r--   3 itv011204 supergroup      48183 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014/order_month=05/part-00000-344acf22-02e6-4a65-9d9b-be9ee31a6267.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014/order_month=06\n",
      "-rw-r--r--   3 itv011204 supergroup      46828 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014/order_month=06/part-00000-344acf22-02e6-4a65-9d9b-be9ee31a6267.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014/order_month=07\n",
      "-rw-r--r--   3 itv011204 supergroup      39605 2024-02-18 14:12 /user/itv011204/retail_db/orders_part2/order_year=2014/order_month=07/part-00000-344acf22-02e6-4a65-9d9b-be9ee31a6267.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls -R /user/`whoami`/retail_db/orders_part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b83d6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|   49733|2014-06-01|             8572|PENDING_PAYMENT|\n",
      "|   49734|2014-06-01|             3644|       COMPLETE|\n",
      "|   49735|2014-06-01|             9457|       COMPLETE|\n",
      "|   49736|2014-06-01|             8753|         CLOSED|\n",
      "|   49737|2014-06-01|             3977|PENDING_PAYMENT|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/{username}/retail_db/orders_part2/order_year=2014/order_month=06').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b438990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|   25876|2014-01-01|             3414|PENDING_PAYMENT|          1|\n",
      "|   25877|2014-01-01|             5549|PENDING_PAYMENT|          1|\n",
      "|   25878|2014-01-01|             9084|        PENDING|          1|\n",
      "|   25879|2014-01-01|             5118|        PENDING|          1|\n",
      "|   25880|2014-01-01|            10146|       CANCELED|          1|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/{username}/retail_db/orders_part2/order_year=2014').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95c83a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+----------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_year|order_month|\n",
      "+--------+----------+-----------------+---------------+----------+-----------+\n",
      "|   15488|2013-11-01|             8987|PENDING_PAYMENT|      2013|         11|\n",
      "|   15489|2013-11-01|             5359|PENDING_PAYMENT|      2013|         11|\n",
      "|   15490|2013-11-01|            10149|       COMPLETE|      2013|         11|\n",
      "|   15491|2013-11-01|            10635|        ON_HOLD|      2013|         11|\n",
      "|   15492|2013-11-01|             7784|PENDING_PAYMENT|      2013|         11|\n",
      "+--------+----------+-----------------+---------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/{username}/retail_db/orders_part2').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d972da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>order_id</th><th>order_date</th><th>order_customer_id</th><th>order_status</th><th>order_year</th><th>order_month</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+----------+-----------------+------------+----------+-----------+\n",
       "|order_id|order_date|order_customer_id|order_status|order_year|order_month|\n",
       "+--------+----------+-----------------+------------+----------+-----------+\n",
       "+--------+----------+-----------------+------------+----------+-----------+"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark. \\\n",
    "    catalog. \\\n",
    "    createTable(\n",
    "        'orders_part2',\n",
    "        path=f'/user/{username}/retail_db/orders_part2',\n",
    "        source='parquet'        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3af1b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>order_id</th><th>order_date</th><th>order_customer_id</th><th>order_status</th><th>order_year</th><th>order_month</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+----------+-----------------+------------+----------+-----------+\n",
       "|order_id|order_date|order_customer_id|order_status|order_year|order_month|\n",
       "+--------+----------+-----------------+------------+----------+-----------+\n",
       "+--------+----------+-----------------+------------+----------+-----------+"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table('orders_part2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df460cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>col_name</th><th>data_type</th><th>comment</th></tr>\n",
       "<tr><td>order_id</td><td>int</td><td>null</td></tr>\n",
       "<tr><td>order_date</td><td>date</td><td>null</td></tr>\n",
       "<tr><td>order_customer_id</td><td>int</td><td>null</td></tr>\n",
       "<tr><td>order_status</td><td>string</td><td>null</td></tr>\n",
       "<tr><td>order_year</td><td>int</td><td>null</td></tr>\n",
       "<tr><td>order_month</td><td>int</td><td>null</td></tr>\n",
       "<tr><td># Partition Infor...</td><td></td><td></td></tr>\n",
       "<tr><td># col_name</td><td>data_type</td><td>comment</td></tr>\n",
       "<tr><td>order_year</td><td>int</td><td>null</td></tr>\n",
       "<tr><td>order_month</td><td>int</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+---------+-------+\n",
       "|            col_name|data_type|comment|\n",
       "+--------------------+---------+-------+\n",
       "|            order_id|      int|   null|\n",
       "|          order_date|     date|   null|\n",
       "|   order_customer_id|      int|   null|\n",
       "|        order_status|   string|   null|\n",
       "|          order_year|      int|   null|\n",
       "|         order_month|      int|   null|\n",
       "|# Partition Infor...|         |       |\n",
       "|          # col_name|data_type|comment|\n",
       "|          order_year|      int|   null|\n",
       "|         order_month|      int|   null|\n",
       "+--------------------+---------+-------+"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"describe orders_part2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bed282d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>partition</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+\n",
       "|partition|\n",
       "+---------+\n",
       "+---------+"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SHOW PARTITIONS orders_part2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bb64a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.recoverPartitions('orders_part2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f07525f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|partition                     |\n",
      "+------------------------------+\n",
      "|order_year=2013/order_month=07|\n",
      "|order_year=2013/order_month=08|\n",
      "|order_year=2013/order_month=09|\n",
      "|order_year=2013/order_month=10|\n",
      "|order_year=2013/order_month=11|\n",
      "|order_year=2013/order_month=12|\n",
      "|order_year=2014/order_month=01|\n",
      "|order_year=2014/order_month=02|\n",
      "|order_year=2014/order_month=03|\n",
      "|order_year=2014/order_month=04|\n",
      "|order_year=2014/order_month=05|\n",
      "|order_year=2014/order_month=06|\n",
      "|order_year=2014/order_month=07|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show partitions orders_part2\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "967fb6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+----------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_year|order_month|\n",
      "+--------+----------+-----------------+---------------+----------+-----------+\n",
      "|   15488|2013-11-01|             8987|PENDING_PAYMENT|      2013|         11|\n",
      "|   15489|2013-11-01|             5359|PENDING_PAYMENT|      2013|         11|\n",
      "|   15490|2013-11-01|            10149|       COMPLETE|      2013|         11|\n",
      "|   15491|2013-11-01|            10635|        ON_HOLD|      2013|         11|\n",
      "|   15492|2013-11-01|             7784|PENDING_PAYMENT|      2013|         11|\n",
      "+--------+----------+-----------------+---------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table('orders_part2').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a025f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read. \\\n",
    "    csv(\n",
    "        orders_path,\n",
    "        schema = \"\"\"\n",
    "                    order_id INT, order_date DATE,\n",
    "                    order_customer_id INT, order_status STRING\n",
    "                \"\"\"\n",
    "    ). \\\n",
    "    withColumn('order_year',date_format('order_date','yyyy')). \\\n",
    "    withColumn('order_month',date_format('order_date','MM')). \\\n",
    "    write. \\\n",
    "    partitionBy('order_year','order_month'). \\\n",
    "    saveAsTable('orders_part3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87c24ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='orders_part', database='itv011204_retail', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='orders_part2', database='itv011204_retail', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='orders_part3', database='itv011204_retail', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c26e238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+----------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_year|order_month|\n",
      "+--------+----------+-----------------+---------------+----------+-----------+\n",
      "|   15488|2013-11-01|             8987|PENDING_PAYMENT|      2013|         11|\n",
      "|   15489|2013-11-01|             5359|PENDING_PAYMENT|      2013|         11|\n",
      "|   15490|2013-11-01|            10149|       COMPLETE|      2013|         11|\n",
      "|   15491|2013-11-01|            10635|        ON_HOLD|      2013|         11|\n",
      "|   15492|2013-11-01|             7784|PENDING_PAYMENT|      2013|         11|\n",
      "|   15493|2013-11-01|             1104|        ON_HOLD|      2013|         11|\n",
      "|   15494|2013-11-01|             7313|     PROCESSING|      2013|         11|\n",
      "|   15495|2013-11-01|             7067|         CLOSED|      2013|         11|\n",
      "|   15496|2013-11-01|            12153|PENDING_PAYMENT|      2013|         11|\n",
      "|   15497|2013-11-01|            11115|PENDING_PAYMENT|      2013|         11|\n",
      "+--------+----------+-----------------+---------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from orders_part3 limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f42777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   68883|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from orders_part3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f92c228a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>col_name</th><th>data_type</th><th>comment</th></tr>\n",
       "<tr><td>order_id</td><td>int</td><td>null</td></tr>\n",
       "<tr><td>order_date</td><td>date</td><td>null</td></tr>\n",
       "<tr><td>order_customer_id</td><td>int</td><td>null</td></tr>\n",
       "<tr><td>order_status</td><td>string</td><td>null</td></tr>\n",
       "<tr><td>order_year</td><td>string</td><td>null</td></tr>\n",
       "<tr><td>order_month</td><td>string</td><td>null</td></tr>\n",
       "<tr><td># Partition Infor...</td><td></td><td></td></tr>\n",
       "<tr><td># col_name</td><td>data_type</td><td>comment</td></tr>\n",
       "<tr><td>order_year</td><td>string</td><td>null</td></tr>\n",
       "<tr><td>order_month</td><td>string</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+---------+-------+\n",
       "|            col_name|data_type|comment|\n",
       "+--------------------+---------+-------+\n",
       "|            order_id|      int|   null|\n",
       "|          order_date|     date|   null|\n",
       "|   order_customer_id|      int|   null|\n",
       "|        order_status|   string|   null|\n",
       "|          order_year|   string|   null|\n",
       "|         order_month|   string|   null|\n",
       "|# Partition Infor...|         |       |\n",
       "|          # col_name|data_type|comment|\n",
       "|          order_year|   string|   null|\n",
       "|         order_month|   string|   null|\n",
       "+--------------------+---------+-------+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"describe orders_part3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8721513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-18 14:44 /user/itv011204/warehouse/itv011204_retail.db/orders_part3/_SUCCESS\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:44 /user/itv011204/warehouse/itv011204_retail.db/orders_part3/order_year=2013\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-18 14:44 /user/itv011204/warehouse/itv011204_retail.db/orders_part3/order_year=2014\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/`whoami`/warehouse/`whoami`_retail.db/orders_part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3b1ecce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                          |comment|\n",
      "+----------------------------+-------------------------------------------------------------------+-------+\n",
      "|order_id                    |int                                                                |null   |\n",
      "|order_date                  |date                                                               |null   |\n",
      "|order_customer_id           |int                                                                |null   |\n",
      "|order_status                |string                                                             |null   |\n",
      "|order_year                  |int                                                                |null   |\n",
      "|order_month                 |int                                                                |null   |\n",
      "|# Partition Information     |                                                                   |       |\n",
      "|# col_name                  |data_type                                                          |comment|\n",
      "|order_year                  |int                                                                |null   |\n",
      "|order_month                 |int                                                                |null   |\n",
      "|                            |                                                                   |       |\n",
      "|# Detailed Table Information|                                                                   |       |\n",
      "|Database                    |itv011204_retail                                                   |       |\n",
      "|Table                       |orders_part2                                                       |       |\n",
      "|Owner                       |itv011204                                                          |       |\n",
      "|Created Time                |Sun Feb 18 14:17:08 EST 2024                                       |       |\n",
      "|Last Access                 |UNKNOWN                                                            |       |\n",
      "|Created By                  |Spark 3.1.2                                                        |       |\n",
      "|Type                        |EXTERNAL                                                           |       |\n",
      "|Provider                    |parquet                                                            |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv011204/retail_db/orders_part2|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe        |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat      |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat     |       |\n",
      "|Partition Provider          |Catalog                                                            |       |\n",
      "+----------------------------+-------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted orders_part2\").show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3809a356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                             |comment|\n",
      "+----------------------------+--------------------------------------------------------------------------------------+-------+\n",
      "|order_id                    |int                                                                                   |null   |\n",
      "|order_date                  |date                                                                                  |null   |\n",
      "|order_customer_id           |int                                                                                   |null   |\n",
      "|order_status                |string                                                                                |null   |\n",
      "|order_month                 |string                                                                                |null   |\n",
      "|# Partition Information     |                                                                                      |       |\n",
      "|# col_name                  |data_type                                                                             |comment|\n",
      "|order_month                 |string                                                                                |null   |\n",
      "|                            |                                                                                      |       |\n",
      "|# Detailed Table Information|                                                                                      |       |\n",
      "|Database                    |itv011204_retail                                                                      |       |\n",
      "|Table                       |orders_part                                                                           |       |\n",
      "|Owner                       |itv011204                                                                             |       |\n",
      "|Created Time                |Thu Feb 15 10:25:21 EST 2024                                                          |       |\n",
      "|Last Access                 |UNKNOWN                                                                               |       |\n",
      "|Created By                  |Spark 3.1.2                                                                           |       |\n",
      "|Type                        |MANAGED                                                                               |       |\n",
      "|Provider                    |parquet                                                                               |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv011204/warehouse/itv011204_retail.db/orders_part|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                           |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                         |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                        |       |\n",
      "|Partition Provider          |Catalog                                                                               |       |\n",
      "+----------------------------+--------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted orders_part\").show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d667cde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                              |comment|\n",
      "+----------------------------+---------------------------------------------------------------------------------------+-------+\n",
      "|order_id                    |int                                                                                    |null   |\n",
      "|order_date                  |date                                                                                   |null   |\n",
      "|order_customer_id           |int                                                                                    |null   |\n",
      "|order_status                |string                                                                                 |null   |\n",
      "|order_year                  |string                                                                                 |null   |\n",
      "|order_month                 |string                                                                                 |null   |\n",
      "|# Partition Information     |                                                                                       |       |\n",
      "|# col_name                  |data_type                                                                              |comment|\n",
      "|order_year                  |string                                                                                 |null   |\n",
      "|order_month                 |string                                                                                 |null   |\n",
      "|                            |                                                                                       |       |\n",
      "|# Detailed Table Information|                                                                                       |       |\n",
      "|Database                    |itv011204_retail                                                                       |       |\n",
      "|Table                       |orders_part3                                                                           |       |\n",
      "|Owner                       |itv011204                                                                              |       |\n",
      "|Created Time                |Sun Feb 18 14:44:25 EST 2024                                                           |       |\n",
      "|Last Access                 |UNKNOWN                                                                                |       |\n",
      "|Created By                  |Spark 3.1.2                                                                            |       |\n",
      "|Type                        |MANAGED                                                                                |       |\n",
      "|Provider                    |parquet                                                                                |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv011204/warehouse/itv011204_retail.db/orders_part3|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                            |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                          |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                         |       |\n",
      "|Partition Provider          |Catalog                                                                                |       |\n",
      "+----------------------------+---------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted orders_part3\").show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94085a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|   15488|2013-11-01|             8987|PENDING_PAYMENT|         11|\n",
      "|   15489|2013-11-01|             5359|PENDING_PAYMENT|         11|\n",
      "|   15490|2013-11-01|            10149|       COMPLETE|         11|\n",
      "|   15491|2013-11-01|            10635|        ON_HOLD|         11|\n",
      "|   15492|2013-11-01|             7784|PENDING_PAYMENT|         11|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/user/itv011204/warehouse/itv011204_retail.db/orders_part3/order_year=2013\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f056f5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30662"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\"/user/itv011204/warehouse/itv011204_retail.db/orders_part3/order_year=2013\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6559b621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|   25876|2014-01-01|             3414|PENDING_PAYMENT|          1|\n",
      "|   25877|2014-01-01|             5549|PENDING_PAYMENT|          1|\n",
      "|   25878|2014-01-01|             9084|        PENDING|          1|\n",
      "|   25879|2014-01-01|             5118|        PENDING|          1|\n",
      "|   25880|2014-01-01|            10146|       CANCELED|          1|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/user/itv011204/warehouse/itv011204_retail.db/orders_part3/order_year=2014\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b6b544c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38221"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\"/user/itv011204/warehouse/itv011204_retail.db/orders_part3/order_year=2014\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00a6d300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30662+38221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a1d7634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|partition                     |\n",
      "+------------------------------+\n",
      "|order_year=2013/order_month=07|\n",
      "|order_year=2013/order_month=08|\n",
      "|order_year=2013/order_month=09|\n",
      "|order_year=2013/order_month=10|\n",
      "|order_year=2013/order_month=11|\n",
      "|order_year=2013/order_month=12|\n",
      "|order_year=2014/order_month=01|\n",
      "|order_year=2014/order_month=02|\n",
      "|order_year=2014/order_month=03|\n",
      "|order_year=2014/order_month=04|\n",
      "|order_year=2014/order_month=05|\n",
      "|order_year=2014/order_month=06|\n",
      "|order_year=2014/order_month=07|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show partitions orders_part3\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29b96350",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_airlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbf1b604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='itv011204_airlines', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5b559b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_codes_path=f\"/public/airlines_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e14ce3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 hdfs supergroup      11411 2021-01-28 10:48 /public/airlines_all/airport-codes/airport-codes-na.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /public/airlines_all/airport-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8f4967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_df = spark.read.csv(airports_codes_path, sep='\\t',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d96df6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_df.createTempView(\"airport_codes_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26bd466c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='itv011204_airlines', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='airport_codes_v', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07d27eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>database</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>itv011204_airlines</td><td>airport_codes</td><td>false</td></tr>\n",
       "<tr><td></td><td>airport_codes_v</td><td>true</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------------+---------------+-----------+\n",
       "|          database|      tableName|isTemporary|\n",
       "+------------------+---------------+-----------+\n",
       "|itv011204_airlines|  airport_codes|      false|\n",
       "|                  |airport_codes_v|       true|\n",
       "+------------------+---------------+-----------+"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0b4c4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|City    |string   |null   |\n",
      "|State   |string   |null   |\n",
      "|Country |string   |null   |\n",
      "|IATA    |string   |null   |\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted airport_codes_v\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a30e734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                 |comment|\n",
      "+----------------------------+------------------------------------------------------------------------------------------+-------+\n",
      "|City                        |string                                                                                    |null   |\n",
      "|State                       |string                                                                                    |null   |\n",
      "|Country                     |string                                                                                    |null   |\n",
      "|IATA                        |string                                                                                    |null   |\n",
      "|                            |                                                                                          |       |\n",
      "|# Detailed Table Information|                                                                                          |       |\n",
      "|Database                    |itv011204_airlines                                                                        |       |\n",
      "|Table                       |airport_codes                                                                             |       |\n",
      "|Owner                       |itv011204                                                                                 |       |\n",
      "|Created Time                |Thu Feb 15 09:15:24 EST 2024                                                              |       |\n",
      "|Last Access                 |UNKNOWN                                                                                   |       |\n",
      "|Created By                  |Spark 3.1.2                                                                               |       |\n",
      "|Type                        |MANAGED                                                                                   |       |\n",
      "|Provider                    |parquet                                                                                   |       |\n",
      "|Statistics                  |9048 bytes                                                                                |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv011204/warehouse/itv011204_airlines.db/airport_codes|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                               |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                             |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                            |       |\n",
      "+----------------------------+------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe formatted airport_codes\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc9b87a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:15 /user/itv011204/warehouse/itv011204_airlines.db/airport_codes\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/`whoami`/warehouse/itv011204_airlines.db/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9eff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.sql.ui.port','0'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Exploring spark catalog'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08e1900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.sql.ui.port','0'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Exploring spark catalog'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1623f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3878b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions','2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_retail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fe8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d5ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85226c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE orders(\n",
    "        order_id INT,\n",
    "        order_date STRING,\n",
    "        order_customer_id INT,\n",
    "        order_status STRING\n",
    "    ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce4710",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    LOAD DATA LOCAL INPATH '/data/retail_db/orders' INTO TABLE orders\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbddb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(1) from orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fed632",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE order_items (\n",
    "        order_item_id INT,\n",
    "        order_item_order_id INT,\n",
    "        order_item_product_id INT,\n",
    "        order_item_quantity INT,\n",
    "        order_item_subtotal FLOAT,\n",
    "        order_item_product_price FLOAT\n",
    "    ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66005108",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    LOAD DATA LOCAL inPATH '/data/retail_db/order_items' into table order_items\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e2756",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from order_items limit 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from orders limit 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb530b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * , ROW_NUMBER() over (Order by order_id) as rowid from orders\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f94b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select * from (\n",
    "        SELECT * , ROW_NUMBER() over (Order by order_id) as rowid from orders\n",
    "    )t\n",
    "     WHERE rowid>5 AND rowid<11\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select * from (\n",
    "        SELECT * , ROW_NUMBER() over (Order by order_id) as rowid from orders WHERE order_status in ('COMPLETE','CLOSED')\n",
    "    )t\n",
    "     WHERE rowid>10 AND rowid<16\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = spark.read.json('/public/retail_db_json/products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d175f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "products.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "products.createTempView('products_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71077f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "288639ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6f2fea5650f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m spark.sql(\"\"\"\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mSELECT\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_item_subtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mAS\u001b[0m \u001b[0mrevenue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT o.order_date,\n",
    "        p.product_id,\n",
    "        p.product_name,\n",
    "        round(sum(oi.order_item_subtotal),2) AS revenue\n",
    "    FROM orders as o JOIN order_items as oi\n",
    "        ON o.order_id = oi.order_item_order_id\n",
    "    JOIN products_v as p\n",
    "        ON p.product_id = oi.order_item_product_id\n",
    "    WHERE o.order_status IN ('COMPLETE','CLOSED')\n",
    "    GROUP BY o.order_date,\n",
    "        p.product_id,\n",
    "        p.product_name\n",
    "    ORDER BY o.order_date, revenue DESC\n",
    "\"\"\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20cb98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
