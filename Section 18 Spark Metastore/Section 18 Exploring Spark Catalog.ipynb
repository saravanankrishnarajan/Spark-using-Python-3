{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a8f8ba",
   "metadata": {},
   "source": [
    "## Section 18 Exploring Spark Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690bae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Spark Metastore'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a459f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aabb6ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1872c8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16dcd1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.catalog.Catalog at 0x7f7bacd05320>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fdedc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        property\n",
       "\u001b[0;31mString form:\u001b[0m <property object at 0x7f7bacb1b9a8>\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Interface through which the user may create, drop, alter or query underlying\n",
       "databases, tables, functions, etc.\n",
       "\n",
       ".. versionadded:: 2.0.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`Catalog`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca361a78",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Catalog in module pyspark.sql.catalog object:\n",
      "\n",
      "class Catalog(builtins.object)\n",
      " |  User-facing catalog API, accessible through `SparkSession.catalog`.\n",
      " |  \n",
      " |  This is a thin wrapper around its Scala implementation org.apache.spark.sql.catalog.Catalog.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sparkSession)\n",
      " |      Create a new Catalog that wraps the underlying JVM object.\n",
      " |  \n",
      " |  cacheTable(self, tableName)\n",
      " |      Caches the specified table in-memory.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  clearCache(self)\n",
      " |      Removes all cached tables from the in-memory cache.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  createExternalTable(self, tableName, path=None, source=None, schema=None, **options)\n",
      " |      Creates a table based on the dataset in a data source.\n",
      " |      \n",
      " |      It returns the DataFrame associated with the external table.\n",
      " |      \n",
      " |      The data source is specified by the ``source`` and a set of ``options``.\n",
      " |      If ``source`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used.\n",
      " |      \n",
      " |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      " |      created external table.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |  \n",
      " |  createTable(self, tableName, path=None, source=None, schema=None, description=None, **options)\n",
      " |      Creates a table based on the dataset in a data source.\n",
      " |      \n",
      " |      It returns the DataFrame associated with the table.\n",
      " |      \n",
      " |      The data source is specified by the ``source`` and a set of ``options``.\n",
      " |      If ``source`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n",
      " |      created from the data at the given path. Otherwise a managed table is created.\n",
      " |      \n",
      " |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      " |      created table.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      .. versionchanged:: 3.1\n",
      " |         Added the ``description`` parameter.\n",
      " |  \n",
      " |  currentDatabase(self)\n",
      " |      Returns the current default database in this session.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  dropGlobalTempView(self, viewName)\n",
      " |      Drops the global temporary view with the given view name in the catalog.\n",
      " |      If the view has been cached before, then it will also be uncached.\n",
      " |      Returns true if this view is dropped successfully, false otherwise.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.createDataFrame([(1, 1)]).createGlobalTempView(\"my_table\")\n",
      " |      >>> spark.table(\"global_temp.my_table\").collect()\n",
      " |      [Row(_1=1, _2=1)]\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"my_table\")\n",
      " |      >>> spark.table(\"global_temp.my_table\") # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      AnalysisException: ...\n",
      " |  \n",
      " |  dropTempView(self, viewName)\n",
      " |      Drops the local temporary view with the given view name in the catalog.\n",
      " |      If the view has been cached before, then it will also be uncached.\n",
      " |      Returns true if this view is dropped successfully, false otherwise.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The return type of this method was None in Spark 2.0, but changed to Boolean\n",
      " |      in Spark 2.1.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.createDataFrame([(1, 1)]).createTempView(\"my_table\")\n",
      " |      >>> spark.table(\"my_table\").collect()\n",
      " |      [Row(_1=1, _2=1)]\n",
      " |      >>> spark.catalog.dropTempView(\"my_table\")\n",
      " |      >>> spark.table(\"my_table\") # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      AnalysisException: ...\n",
      " |  \n",
      " |  isCached(self, tableName)\n",
      " |      Returns true if the table is currently cached in-memory.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  listColumns(self, tableName, dbName=None)\n",
      " |      Returns a list of columns for the given table/view in the specified database.\n",
      " |      \n",
      " |       If no database is specified, the current database is used.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |       Notes\n",
      " |       -----\n",
      " |       the order of arguments here is different from that of its JVM counterpart\n",
      " |       because Python does not support method overloading.\n",
      " |  \n",
      " |  listDatabases(self)\n",
      " |      Returns a list of databases available across all sessions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  listFunctions(self, dbName=None)\n",
      " |      Returns a list of functions registered in the specified database.\n",
      " |      \n",
      " |      If no database is specified, the current database is used.\n",
      " |      This includes all temporary functions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  listTables(self, dbName=None)\n",
      " |      Returns a list of tables/views in the specified database.\n",
      " |      \n",
      " |      If no database is specified, the current database is used.\n",
      " |      This includes all temporary views.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  recoverPartitions(self, tableName)\n",
      " |      Recovers all the partitions of the given table and update the catalog.\n",
      " |      \n",
      " |      Only works with a partitioned table, and not a view.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.1\n",
      " |  \n",
      " |  refreshByPath(self, path)\n",
      " |      Invalidates and refreshes all the cached data (and the associated metadata) for any\n",
      " |      DataFrame that contains the given data source path.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |  \n",
      " |  refreshTable(self, tableName)\n",
      " |      Invalidates and refreshes all the cached data and metadata of the given table.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  registerFunction(self, name, f, returnType=None)\n",
      " |      An alias for :func:`spark.udf.register`.\n",
      " |      See :meth:`pyspark.sql.UDFRegistration.register`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. deprecated:: 2.3.0\n",
      " |          Use :func:`spark.udf.register` instead.\n",
      " |  \n",
      " |  setCurrentDatabase(self, dbName)\n",
      " |      Sets the current default database in this session.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  uncacheTable(self, tableName)\n",
      " |      Removes the specified table from the in-memory cache.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "611a49e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {username}_demo_db CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fa6b30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE {username}_demo_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38e28b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f\"{username}_demo_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe161a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_demo_db'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61456bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28f1bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [(\"X\",)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25dbb3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l,schema=\"dummy STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bf3dfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fc1c808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpartitionBy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Saves the content of the :class:`DataFrame` as the specified table.\n",
       "\n",
       "In the case the table already exists, behavior of this function depends on the\n",
       "save mode, specified by the `mode` function (default to throwing an exception).\n",
       "When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n",
       "the same as that of the existing table.\n",
       "\n",
       "* `append`: Append contents of this :class:`DataFrame` to existing data.\n",
       "* `overwrite`: Overwrite existing data.\n",
       "* `error` or `errorifexists`: Throw an exception if data already exists.\n",
       "* `ignore`: Silently ignore this operation if data already exists.\n",
       "\n",
       ".. versionadded:: 1.4.0\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "name : str\n",
       "    the table name\n",
       "format : str, optional\n",
       "    the format used to save\n",
       "mode : str, optional\n",
       "    one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)\n",
       "partitionBy : str or list\n",
       "    names of partitioning columns\n",
       "**options : dict\n",
       "    all other string options\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.saveAsTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3791d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"dual\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c728b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"dual\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8dec707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"dual\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9aac23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dual\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "539baf4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE dual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de380248",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fec4c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(dummy,StringType,true)))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0eeebc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a table based on the dataset in a data source.\n",
       "\n",
       "It returns the DataFrame associated with the table.\n",
       "\n",
       "The data source is specified by the ``source`` and a set of ``options``.\n",
       "If ``source`` is not specified, the default data source configured by\n",
       "``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n",
       "created from the data at the given path. Otherwise a managed table is created.\n",
       "\n",
       "Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
       "created table.\n",
       "\n",
       ".. versionadded:: 2.2.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "\n",
       ".. versionchanged:: 3.1\n",
       "   Added the ``description`` parameter.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog.createTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be6bfee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "+-----+"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.createTable(\"dual\", schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4161e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dual', database='itv011204_demo_db', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f32e4464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `spark.write` not found.\n"
     ]
    }
   ],
   "source": [
    "spark.write?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8280e640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsertInto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Inserts the content of the :class:`DataFrame` to the specified table.\n",
       "\n",
       "It requires that the schema of the :class:`DataFrame` is the same as the\n",
       "schema of the table.\n",
       "\n",
       "Optionally overwriting any existing data.\n",
       "\n",
       ".. versionadded:: 1.4\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.insertInto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a4d459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.insertInto(\"dual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8bef1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"dual\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d91f7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.insertInto(\"dual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b2bd347",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.insertInto(\"dual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0097b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "<tr><td>X</td></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "|    X|\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table(\"dual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7af44acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "|    X|\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM dual\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3382338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"dual\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11ec17aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "|    X|\n",
      "|    X|\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM dual\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3db43c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "<tr><td>X</td></tr>\n",
       "<tr><td>X</td></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "|    X|\n",
       "|    X|\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table(\"dual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea2fa8",
   "metadata": {},
   "source": [
    "###  217 Inferring Schema While creating Spark Metastore Tables using Spark Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66020f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Spark Metastore'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11072d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d357d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d78cfab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "015091f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf0ced2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {username}_airtraffic CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b22d8252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_airtraffic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "930c2a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f\"{username}_airtraffic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a8c1d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_airtraffic'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e7cf2754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-14 17:26 /user/itv011204/airtraffic_all/airport-codes\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -mkdir /user/`whoami`/airtraffic_all\n",
    "hdfs dfs -cp -f /public/airlines_all/airport-codes /user/`whoami`/airtraffic_all\n",
    "hdfs dfs -ls /user/`whoami`/airtraffic_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f446908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   3 itv011204 supergroup      11411 2024-02-14 17:26 /user/itv011204/airtraffic_all/airport-codes/airport-codes-na.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/`whoami`/airtraffic_all/airport-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0d1c287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yuma\tAZ\tUSA\tYUM\tCanada\tYZFLa\tYWKCanada\tYQYada\tYZP"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/itv011204/airtraffic_all/airport-codes/airport-codes-na.txt | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11d4c090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateExternalTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a table based on the dataset in a data source.\n",
       "\n",
       "It returns the DataFrame associated with the external table.\n",
       "\n",
       "The data source is specified by the ``source`` and a set of ``options``.\n",
       "If ``source`` is not specified, the default data source configured by\n",
       "``spark.sql.sources.default`` will be used.\n",
       "\n",
       "Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
       "created external table.\n",
       "\n",
       ".. versionadded:: 2.0.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog.createExternalTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3472392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_path = f\"/user/{username}/airtraffic_all/airport-codes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbe8a083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/user/itv011204/airtraffic_all/airport-codes/'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_codes_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c702427d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_airtraffic'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6e3489f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS airport_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c2ea561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>City</th><th>State</th><th>Country</th><th>IATA</th></tr>\n",
       "<tr><td>Abbotsford</td><td>BC</td><td>Canada</td><td>YXX</td></tr>\n",
       "<tr><td>Aberdeen</td><td>SD</td><td>USA</td><td>ABR</td></tr>\n",
       "<tr><td>Abilene</td><td>TX</td><td>USA</td><td>ABI</td></tr>\n",
       "<tr><td>Akron</td><td>OH</td><td>USA</td><td>CAK</td></tr>\n",
       "<tr><td>Alamosa</td><td>CO</td><td>USA</td><td>ALS</td></tr>\n",
       "<tr><td>Albany</td><td>GA</td><td>USA</td><td>ABY</td></tr>\n",
       "<tr><td>Albany</td><td>NY</td><td>USA</td><td>ALB</td></tr>\n",
       "<tr><td>Albuquerque</td><td>NM</td><td>USA</td><td>ABQ</td></tr>\n",
       "<tr><td>Alexandria</td><td>LA</td><td>USA</td><td>AEX</td></tr>\n",
       "<tr><td>Allentown</td><td>PA</td><td>USA</td><td>ABE</td></tr>\n",
       "<tr><td>Alliance</td><td>NE</td><td>USA</td><td>AIA</td></tr>\n",
       "<tr><td>Alpena</td><td>MI</td><td>USA</td><td>APN</td></tr>\n",
       "<tr><td>Altoona</td><td>PA</td><td>USA</td><td>AOO</td></tr>\n",
       "<tr><td>Amarillo</td><td>TX</td><td>USA</td><td>AMA</td></tr>\n",
       "<tr><td>Anahim Lake</td><td>BC</td><td>Canada</td><td>YAA</td></tr>\n",
       "<tr><td>Anchorage</td><td>AK</td><td>USA</td><td>ANC</td></tr>\n",
       "<tr><td>Appleton</td><td>WI</td><td>USA</td><td>ATW</td></tr>\n",
       "<tr><td>Arviat</td><td>NWT</td><td>Canada</td><td>YEK</td></tr>\n",
       "<tr><td>Asheville</td><td>NC</td><td>USA</td><td>AVL</td></tr>\n",
       "<tr><td>Aspen</td><td>CO</td><td>USA</td><td>ASE</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-----------+-----+-------+----+\n",
       "|       City|State|Country|IATA|\n",
       "+-----------+-----+-------+----+\n",
       "| Abbotsford|   BC| Canada| YXX|\n",
       "|   Aberdeen|   SD|    USA| ABR|\n",
       "|    Abilene|   TX|    USA| ABI|\n",
       "|      Akron|   OH|    USA| CAK|\n",
       "|    Alamosa|   CO|    USA| ALS|\n",
       "|     Albany|   GA|    USA| ABY|\n",
       "|     Albany|   NY|    USA| ALB|\n",
       "|Albuquerque|   NM|    USA| ABQ|\n",
       "| Alexandria|   LA|    USA| AEX|\n",
       "|  Allentown|   PA|    USA| ABE|\n",
       "|   Alliance|   NE|    USA| AIA|\n",
       "|     Alpena|   MI|    USA| APN|\n",
       "|    Altoona|   PA|    USA| AOO|\n",
       "|   Amarillo|   TX|    USA| AMA|\n",
       "|Anahim Lake|   BC| Canada| YAA|\n",
       "|  Anchorage|   AK|    USA| ANC|\n",
       "|   Appleton|   WI|    USA| ATW|\n",
       "|     Arviat|  NWT| Canada| YEK|\n",
       "|  Asheville|   NC|    USA| AVL|\n",
       "|      Aspen|   CO|    USA| ASE|\n",
       "+-----------+-----+-------+----+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.createExternalTable(\n",
    "    \"airport_codes\",\n",
    "    path = airport_codes_path,\n",
    "    source = 'CSV',\n",
    "    sep = '\\t',\n",
    "    header=\"true\",\n",
    "    inferSchema = \"true\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15befc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Catalog.listTables of <pyspark.sql.catalog.Catalog object at 0x7f789dd97400>>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af7f6806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='itv011204_airtraffic', description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb1c7b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|    City|   string|   null|\n",
      "|   State|   string|   null|\n",
      "| Country|   string|   null|\n",
      "|    IATA|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE TABLE airport_codes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a6730d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|    City|   string|   null|\n",
      "|   State|   string|   null|\n",
      "| Country|   string|   null|\n",
      "|    IATA|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE airport_codes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "618e2584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                |comment|\n",
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "|City                        |string                                                                   |null   |\n",
      "|State                       |string                                                                   |null   |\n",
      "|Country                     |string                                                                   |null   |\n",
      "|IATA                        |string                                                                   |null   |\n",
      "|                            |                                                                         |       |\n",
      "|# Detailed Table Information|                                                                         |       |\n",
      "|Database                    |itv011204_airtraffic                                                     |       |\n",
      "|Table                       |airport_codes                                                            |       |\n",
      "|Owner                       |itv011204                                                                |       |\n",
      "|Created Time                |Wed Feb 14 17:33:20 EST 2024                                             |       |\n",
      "|Last Access                 |UNKNOWN                                                                  |       |\n",
      "|Created By                  |Spark 3.1.2                                                              |       |\n",
      "|Type                        |EXTERNAL                                                                 |       |\n",
      "|Provider                    |CSV                                                                      |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv011204/airtraffic_all/airport-codes|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                       |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat                         |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat                |       |\n",
      "|Storage Properties          |[inferSchema=true, sep=\t, header=true]                                   |       |\n",
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED airport_codes\").show(200,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f84d21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                |comment|\n",
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "|City                        |string                                                                   |null   |\n",
      "|State                       |string                                                                   |null   |\n",
      "|Country                     |string                                                                   |null   |\n",
      "|IATA                        |string                                                                   |null   |\n",
      "|                            |                                                                         |       |\n",
      "|# Detailed Table Information|                                                                         |       |\n",
      "|Database                    |itv011204_airtraffic                                                     |       |\n",
      "|Table                       |airport_codes                                                            |       |\n",
      "|Owner                       |itv011204                                                                |       |\n",
      "|Created Time                |Wed Feb 14 17:33:20 EST 2024                                             |       |\n",
      "|Last Access                 |UNKNOWN                                                                  |       |\n",
      "|Created By                  |Spark 3.1.2                                                              |       |\n",
      "|Type                        |EXTERNAL                                                                 |       |\n",
      "|Provider                    |CSV                                                                      |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv011204/airtraffic_all/airport-codes|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                       |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat                         |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat                |       |\n",
      "|Storage Properties          |[inferSchema=true, sep=\t, header=true]                                   |       |\n",
      "+----------------------------+-------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FORMATTED airport_codes\").show(200,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17da24a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|      City|State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|   BC| Canada| YXX|\n",
      "|  Aberdeen|   SD|    USA| ABR|\n",
      "|   Abilene|   TX|    USA| ABI|\n",
      "|     Akron|   OH|    USA| CAK|\n",
      "|   Alamosa|   CO|    USA| ALS|\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"airport_codes\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be0156b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table(\"airport_codes\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f995a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='City', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='State', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='Country', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='IATA', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns('airport_codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db55363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca2f3fbf",
   "metadata": {},
   "source": [
    "### 218 Define Schema for Spark Metastore Tables using StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ce835b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Spark Metastore'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6dbc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4340d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dac1f2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afc7144d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_airtraffic'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1af8c114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {username}_hr_db CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa7db39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_hr_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a48fd823",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_hr_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47655335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_hr_db'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "223adb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a table based on the dataset in a data source.\n",
       "\n",
       "It returns the DataFrame associated with the table.\n",
       "\n",
       "The data source is specified by the ``source`` and a set of ``options``.\n",
       "If ``source`` is not specified, the default data source configured by\n",
       "``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n",
       "created from the data at the given path. Otherwise a managed table is created.\n",
       "\n",
       "Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
       "created table.\n",
       "\n",
       ".. versionadded:: 2.2.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "\n",
       ".. versionchanged:: 3.1\n",
       "   Added the ``description`` parameter.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog.createTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d5d9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, \\\n",
    "                                    StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a78fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesSchema = StructType([\n",
    "    StructField('employee_id',IntegerType()),\n",
    "    StructField('first_name',StringType()),\n",
    "    StructField('last_name',StringType()),\n",
    "    StructField('salary',FloatType()),\n",
    "    StructField('nationality',StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88a17a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(employee_id,IntegerType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(salary,FloatType,true),StructField(nationality,StringType,true)))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d72e02e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on StructType in module pyspark.sql.types object:\n",
      "\n",
      "class StructType(DataType)\n",
      " |  Struct type, consisting of a list of :class:`StructField`.\n",
      " |  \n",
      " |  This is the data type representing a :class:`Row`.\n",
      " |  \n",
      " |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n",
      " |  A contained :class:`StructField` can be accessed by its name or position.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct1[\"f1\"]\n",
      " |  StructField(f1,StringType,true)\n",
      " |  >>> struct1[0]\n",
      " |  StructField(f1,StringType,true)\n",
      " |  \n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct1 == struct2\n",
      " |  True\n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
      " |  ...     StructField(\"f2\", IntegerType(), False)])\n",
      " |  >>> struct1 == struct2\n",
      " |  False\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StructType\n",
      " |      DataType\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |      Access fields by name or slice.\n",
      " |  \n",
      " |  __init__(self, fields=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate the fields\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of fields.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add(self, field, data_type=None, nullable=True, metadata=None)\n",
      " |      Construct a StructType by adding new elements to it, to define the schema.\n",
      " |      The method accepts either:\n",
      " |      \n",
      " |          a) A single parameter which is a StructField object.\n",
      " |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n",
      " |             metadata(optional). The data_type parameter may be either a String or a\n",
      " |             DataType object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field : str or :class:`StructField`\n",
      " |          Either the name of the field or a StructField object\n",
      " |      data_type : :class:`DataType`, optional\n",
      " |          If present, the DataType of the StructField to create\n",
      " |      nullable : bool, optional\n",
      " |          Whether the field to add should be nullable (default True)\n",
      " |      metadata : dict, optional\n",
      " |          Any additional metadata (default None)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StructType`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True), \\\n",
      " |      ...     StructField(\"f2\", StringType(), True, None)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |  \n",
      " |  fieldNames(self)\n",
      " |      Returns all field names in a list.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct.fieldNames()\n",
      " |      ['f1']\n",
      " |  \n",
      " |  fromInternal(self, obj)\n",
      " |      Converts an internal SQL object into a native Python object.\n",
      " |  \n",
      " |  jsonValue(self)\n",
      " |  \n",
      " |  needConversion(self)\n",
      " |      Does this type needs conversion between Python object and internal SQL object.\n",
      " |      \n",
      " |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      " |  \n",
      " |  simpleString(self)\n",
      " |  \n",
      " |  toInternal(self, obj)\n",
      " |      Converts a Python object into an internal SQL object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  fromJson(json) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DataType:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  json(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from DataType:\n",
      " |  \n",
      " |  typeName() from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DataType:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(employeesSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fafbfb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'struct<employee_id:int,first_name:string,last_name:string,salary:float,nationality:string>'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesSchema.simpleString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d790080c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a table based on the dataset in a data source.\n",
       "\n",
       "It returns the DataFrame associated with the table.\n",
       "\n",
       "The data source is specified by the ``source`` and a set of ``options``.\n",
       "If ``source`` is not specified, the default data source configured by\n",
       "``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n",
       "created from the data at the given path. Otherwise a managed table is created.\n",
       "\n",
       "Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
       "created table.\n",
       "\n",
       ".. versionadded:: 2.2.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "\n",
       ".. versionchanged:: 3.1\n",
       "   Added the ``description`` parameter.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog.createTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bbeb18aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_hr_db'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "353eb6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+-----------+\n",
       "|employee_id|first_name|last_name|salary|nationality|\n",
       "+-----------+----------+---------+------+-----------+\n",
       "+-----------+----------+---------+------+-----------+"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.createTable(\n",
    "    'employees',\n",
    "    schema=employeesSchema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4aba2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employees', database='itv011204_hr_db', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0cdea6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dual', database='itv011204_demo_db', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(f'{username}_demo_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79e1c0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='itv011204_airtraffic', description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(f'{username}_airtraffic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e228076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='employee_id', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='first_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='last_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='salary', description=None, dataType='float', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='nationality', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns('employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6cddb9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-----------+\n",
      "|       database|tableName|isTemporary|\n",
      "+---------------+---------+-----------+\n",
      "|itv011204_hr_db|employees|      false|\n",
      "+---------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "acf19674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+\n",
      "|   col_name|data_type|comment|\n",
      "+-----------+---------+-------+\n",
      "|employee_id|      int|   null|\n",
      "| first_name|   string|   null|\n",
      "|  last_name|   string|   null|\n",
      "|     salary|    float|   null|\n",
      "|nationality|   string|   null|\n",
      "+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE employees\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "81ba6306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                          |comment|\n",
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "|employee_id                 |int                                                                                |null   |\n",
      "|first_name                  |string                                                                             |null   |\n",
      "|last_name                   |string                                                                             |null   |\n",
      "|salary                      |float                                                                              |null   |\n",
      "|nationality                 |string                                                                             |null   |\n",
      "|                            |                                                                                   |       |\n",
      "|# Detailed Table Information|                                                                                   |       |\n",
      "|Database                    |itv011204_hr_db                                                                    |       |\n",
      "|Table                       |employees                                                                          |       |\n",
      "|Owner                       |itv011204                                                                          |       |\n",
      "|Created Time                |Wed Feb 14 18:04:57 EST 2024                                                       |       |\n",
      "|Last Access                 |UNKNOWN                                                                            |       |\n",
      "|Created By                  |Spark 3.1.2                                                                        |       |\n",
      "|Type                        |MANAGED                                                                            |       |\n",
      "|Provider                    |parquet                                                                            |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv011204/warehouse/itv011204_hr_db.db/employees|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                        |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                      |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                     |       |\n",
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED employees\").show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b9fbbaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                          |comment|\n",
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "|employee_id                 |int                                                                                |null   |\n",
      "|first_name                  |string                                                                             |null   |\n",
      "|last_name                   |string                                                                             |null   |\n",
      "|salary                      |float                                                                              |null   |\n",
      "|nationality                 |string                                                                             |null   |\n",
      "|                            |                                                                                   |       |\n",
      "|# Detailed Table Information|                                                                                   |       |\n",
      "|Database                    |itv011204_hr_db                                                                    |       |\n",
      "|Table                       |employees                                                                          |       |\n",
      "|Owner                       |itv011204                                                                          |       |\n",
      "|Created Time                |Wed Feb 14 18:04:57 EST 2024                                                       |       |\n",
      "|Last Access                 |UNKNOWN                                                                            |       |\n",
      "|Created By                  |Spark 3.1.2                                                                        |       |\n",
      "|Type                        |MANAGED                                                                            |       |\n",
      "|Provider                    |parquet                                                                            |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv011204/warehouse/itv011204_hr_db.db/employees|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                        |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                      |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                     |       |\n",
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FORMATTED employees\").show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c09fb176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                         |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE `itv011204_hr_db`.`employees` (\n",
      "  `employee_id` INT,\n",
      "  `first_name` STRING,\n",
      "  `last_name` STRING,\n",
      "  `salary` FLOAT,\n",
      "  `nationality` STRING)\n",
      "USING parquet\n",
      "|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CREATE TABLE employees\").show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9dade8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+\n",
      "|database            |tableName    |isTemporary|\n",
      "+--------------------+-------------+-----------+\n",
      "|itv011204_airtraffic|airport_codes|false      |\n",
      "+--------------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SHOW TABLES FROM {username}_airtraffic\").show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "16a54f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                 |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE `itv011204_airtraffic`.`airport_codes` (\n",
      "  `City` STRING,\n",
      "  `State` STRING,\n",
      "  `Country` STRING,\n",
      "  `IATA` STRING)\n",
      "USING CSV\n",
      "OPTIONS (\n",
      "  `inferSchema` 'true',\n",
      "  `sep` '\t',\n",
      "  `header` 'true')\n",
      "LOCATION 'hdfs://m01.itversity.com:9000/user/itv011204/airtraffic_all/airport-codes'\n",
      "|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SHOW CREATE TABLE {username}_airtraffic.airport_codes\").show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755a939",
   "metadata": {},
   "source": [
    "### 219 Insering into Existing Spark Metastore tables using Spark Data Frame APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db1f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Spark Metastore'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eafddb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7548e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ca8996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16fa13bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0469ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_hr_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "370e5074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_hr_db'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6639c870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"TRUNCATE TABLE employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7dc1929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employees', database='itv011204_hr_db', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f05b5acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='employee_id', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='first_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='last_name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='salary', description=None, dataType='float', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='nationality', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns('employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c0a05cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \"united states\"),\n",
    "             (2, \"Henry\", \"Ford\", 1250.0, \"India\"),\n",
    "             (3, \"Nick\", \"Junior\", 750.0, \"united KINGDOM\"),\n",
    "             (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba5d0b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+-----------+\n",
       "|employee_id|first_name|last_name|salary|nationality|\n",
       "+-----------+----------+---------+------+-----------+\n",
       "+-----------+----------+---------+------+-----------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table('employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0db08920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(employee_id,IntegerType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(salary,FloatType,true),StructField(nationality,StringType,true)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table('employees').schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0cfb277",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "        schema = \"\"\"\n",
    "                    employee_id INT, first_name STRING, last_name STRING,\n",
    "                    salary FLOAT, nationality STRING\n",
    "                \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ba24be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states|\n",
      "|          2|     Henry|     Ford|1250.0|         India|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61936923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(employee_id,IntegerType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(salary,FloatType,true),StructField(nationality,StringType,true)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "156a8a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(employee_id,IntegerType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(salary,FloatType,true),StructField(nationality,StringType,true)))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table('employees').schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd56dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.write.insertInto(\"employees\",overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d141891b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th></tr>\n",
       "<tr><td>3</td><td>Nick</td><td>Junior</td><td>750.0</td><td>united KINGDOM</td></tr>\n",
       "<tr><td>1</td><td>Scott</td><td>Tiger</td><td>1000.0</td><td>united states</td></tr>\n",
       "<tr><td>4</td><td>Bill</td><td>Gomes</td><td>1500.0</td><td>AUSTRALIA</td></tr>\n",
       "<tr><td>2</td><td>Henry</td><td>Ford</td><td>1250.0</td><td>India</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+--------------+\n",
       "|employee_id|first_name|last_name|salary|   nationality|\n",
       "+-----------+----------+---------+------+--------------+\n",
       "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
       "|          1|     Scott|    Tiger|1000.0| united states|\n",
       "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
       "|          2|     Henry|     Ford|1250.0|         India|\n",
       "+-----------+----------+---------+------+--------------+"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd714923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|\n",
      "|          1|     Scott|    Tiger|1000.0| united states|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|\n",
      "|          2|     Henry|     Ford|1250.0|         India|\n",
      "+-----------+----------+---------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table('employees').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadd7a4c",
   "metadata": {},
   "source": [
    "### 220 Read and Process Data from Metastore Tables using DF APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "054c10d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Spark Metastore'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa0fa4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d59bf57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "349ab122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff496a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79a065cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Function(name='!', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name='%', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name='&', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseAnd', isTemporary=True),\n",
       " Function(name='*', description=None, className='org.apache.spark.sql.catalyst.expressions.Multiply', isTemporary=True),\n",
       " Function(name='+', description=None, className='org.apache.spark.sql.catalyst.expressions.Add', isTemporary=True),\n",
       " Function(name='-', description=None, className='org.apache.spark.sql.catalyst.expressions.Subtract', isTemporary=True),\n",
       " Function(name='/', description=None, className='org.apache.spark.sql.catalyst.expressions.Divide', isTemporary=True),\n",
       " Function(name='<', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThan', isTemporary=True),\n",
       " Function(name='<=', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThanOrEqual', isTemporary=True),\n",
       " Function(name='<=>', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualNullSafe', isTemporary=True),\n",
       " Function(name='=', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name='==', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name='>', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThan', isTemporary=True),\n",
       " Function(name='>=', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual', isTemporary=True),\n",
       " Function(name='^', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseXor', isTemporary=True),\n",
       " Function(name='abs', description=None, className='org.apache.spark.sql.catalyst.expressions.Abs', isTemporary=True),\n",
       " Function(name='acos', description=None, className='org.apache.spark.sql.catalyst.expressions.Acos', isTemporary=True),\n",
       " Function(name='acosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Acosh', isTemporary=True),\n",
       " Function(name='add_months', description=None, className='org.apache.spark.sql.catalyst.expressions.AddMonths', isTemporary=True),\n",
       " Function(name='aggregate', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayAggregate', isTemporary=True),\n",
       " Function(name='and', description=None, className='org.apache.spark.sql.catalyst.expressions.And', isTemporary=True),\n",
       " Function(name='any', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='approx_count_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus', isTemporary=True),\n",
       " Function(name='approx_percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name='array', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateArray', isTemporary=True),\n",
       " Function(name='array_contains', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayContains', isTemporary=True),\n",
       " Function(name='array_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayDistinct', isTemporary=True),\n",
       " Function(name='array_except', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExcept', isTemporary=True),\n",
       " Function(name='array_intersect', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayIntersect', isTemporary=True),\n",
       " Function(name='array_join', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayJoin', isTemporary=True),\n",
       " Function(name='array_max', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMax', isTemporary=True),\n",
       " Function(name='array_min', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMin', isTemporary=True),\n",
       " Function(name='array_position', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayPosition', isTemporary=True),\n",
       " Function(name='array_remove', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRemove', isTemporary=True),\n",
       " Function(name='array_repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRepeat', isTemporary=True),\n",
       " Function(name='array_sort', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraySort', isTemporary=True),\n",
       " Function(name='array_union', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayUnion', isTemporary=True),\n",
       " Function(name='arrays_overlap', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysOverlap', isTemporary=True),\n",
       " Function(name='arrays_zip', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysZip', isTemporary=True),\n",
       " Function(name='ascii', description=None, className='org.apache.spark.sql.catalyst.expressions.Ascii', isTemporary=True),\n",
       " Function(name='asin', description=None, className='org.apache.spark.sql.catalyst.expressions.Asin', isTemporary=True),\n",
       " Function(name='asinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Asinh', isTemporary=True),\n",
       " Function(name='assert_true', description=None, className='org.apache.spark.sql.catalyst.expressions.AssertTrue', isTemporary=True),\n",
       " Function(name='atan', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan', isTemporary=True),\n",
       " Function(name='atan2', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan2', isTemporary=True),\n",
       " Function(name='atanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Atanh', isTemporary=True),\n",
       " Function(name='avg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name='base64', description=None, className='org.apache.spark.sql.catalyst.expressions.Base64', isTemporary=True),\n",
       " Function(name='bigint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bin', description=None, className='org.apache.spark.sql.catalyst.expressions.Bin', isTemporary=True),\n",
       " Function(name='binary', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bit_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitAndAgg', isTemporary=True),\n",
       " Function(name='bit_count', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseCount', isTemporary=True),\n",
       " Function(name='bit_length', description=None, className='org.apache.spark.sql.catalyst.expressions.BitLength', isTemporary=True),\n",
       " Function(name='bit_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitOrAgg', isTemporary=True),\n",
       " Function(name='bit_xor', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitXorAgg', isTemporary=True),\n",
       " Function(name='bool_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
       " Function(name='bool_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='bround', description=None, className='org.apache.spark.sql.catalyst.expressions.BRound', isTemporary=True),\n",
       " Function(name='cardinality', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name='cast', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='cbrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Cbrt', isTemporary=True),\n",
       " Function(name='ceil', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
       " Function(name='ceiling', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
       " Function(name='char', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
       " Function(name='char_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='character_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='chr', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
       " Function(name='coalesce', description=None, className='org.apache.spark.sql.catalyst.expressions.Coalesce', isTemporary=True),\n",
       " Function(name='collect_list', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True),\n",
       " Function(name='collect_set', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet', isTemporary=True),\n",
       " Function(name='concat', description=None, className='org.apache.spark.sql.catalyst.expressions.Concat', isTemporary=True),\n",
       " Function(name='concat_ws', description=None, className='org.apache.spark.sql.catalyst.expressions.ConcatWs', isTemporary=True),\n",
       " Function(name='conv', description=None, className='org.apache.spark.sql.catalyst.expressions.Conv', isTemporary=True),\n",
       " Function(name='corr', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Corr', isTemporary=True),\n",
       " Function(name='cos', description=None, className='org.apache.spark.sql.catalyst.expressions.Cos', isTemporary=True),\n",
       " Function(name='cosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Cosh', isTemporary=True),\n",
       " Function(name='cot', description=None, className='org.apache.spark.sql.catalyst.expressions.Cot', isTemporary=True),\n",
       " Function(name='count', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Count', isTemporary=True),\n",
       " Function(name='count_if', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountIf', isTemporary=True),\n",
       " Function(name='count_min_sketch', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAgg', isTemporary=True),\n",
       " Function(name='covar_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation', isTemporary=True),\n",
       " Function(name='covar_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovSample', isTemporary=True),\n",
       " Function(name='crc32', description=None, className='org.apache.spark.sql.catalyst.expressions.Crc32', isTemporary=True),\n",
       " Function(name='cube', description=None, className='org.apache.spark.sql.catalyst.expressions.Cube', isTemporary=True),\n",
       " Function(name='cume_dist', description=None, className='org.apache.spark.sql.catalyst.expressions.CumeDist', isTemporary=True),\n",
       " Function(name='current_catalog', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentCatalog', isTemporary=True),\n",
       " Function(name='current_database', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True),\n",
       " Function(name='current_date', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDate', isTemporary=True),\n",
       " Function(name='current_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
       " Function(name='current_timezone', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimeZone', isTemporary=True),\n",
       " Function(name='date', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='date_add', description=None, className='org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True),\n",
       " Function(name='date_format', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFormatClass', isTemporary=True),\n",
       " Function(name='date_from_unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFromUnixDate', isTemporary=True),\n",
       " Function(name='date_part', description=None, className='org.apache.spark.sql.catalyst.expressions.DatePart', isTemporary=True),\n",
       " Function(name='date_sub', description=None, className='org.apache.spark.sql.catalyst.expressions.DateSub', isTemporary=True),\n",
       " Function(name='date_trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncTimestamp', isTemporary=True),\n",
       " Function(name='datediff', description=None, className='org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True),\n",
       " Function(name='day', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name='dayofmonth', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name='dayofweek', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfWeek', isTemporary=True),\n",
       " Function(name='dayofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfYear', isTemporary=True),\n",
       " Function(name='decimal', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='decode', description=None, className='org.apache.spark.sql.catalyst.expressions.Decode', isTemporary=True),\n",
       " Function(name='degrees', description=None, className='org.apache.spark.sql.catalyst.expressions.ToDegrees', isTemporary=True),\n",
       " Function(name='dense_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.DenseRank', isTemporary=True),\n",
       " Function(name='div', description=None, className='org.apache.spark.sql.catalyst.expressions.IntegralDivide', isTemporary=True),\n",
       " Function(name='double', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='e', description=None, className='org.apache.spark.sql.catalyst.expressions.EulerNumber', isTemporary=True),\n",
       " Function(name='element_at', description=None, className='org.apache.spark.sql.catalyst.expressions.ElementAt', isTemporary=True),\n",
       " Function(name='elt', description=None, className='org.apache.spark.sql.catalyst.expressions.Elt', isTemporary=True),\n",
       " Function(name='encode', description=None, className='org.apache.spark.sql.catalyst.expressions.Encode', isTemporary=True),\n",
       " Function(name='every', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
       " Function(name='exists', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExists', isTemporary=True),\n",
       " Function(name='exp', description=None, className='org.apache.spark.sql.catalyst.expressions.Exp', isTemporary=True),\n",
       " Function(name='explode', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
       " Function(name='explode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
       " Function(name='expm1', description=None, className='org.apache.spark.sql.catalyst.expressions.Expm1', isTemporary=True),\n",
       " Function(name='extract', description=None, className='org.apache.spark.sql.catalyst.expressions.Extract', isTemporary=True),\n",
       " Function(name='factorial', description=None, className='org.apache.spark.sql.catalyst.expressions.Factorial', isTemporary=True),\n",
       " Function(name='filter', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayFilter', isTemporary=True),\n",
       " Function(name='find_in_set', description=None, className='org.apache.spark.sql.catalyst.expressions.FindInSet', isTemporary=True),\n",
       " Function(name='first', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name='first_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name='flatten', description=None, className='org.apache.spark.sql.catalyst.expressions.Flatten', isTemporary=True),\n",
       " Function(name='float', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='floor', description=None, className='org.apache.spark.sql.catalyst.expressions.Floor', isTemporary=True),\n",
       " Function(name='forall', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayForAll', isTemporary=True),\n",
       " Function(name='format_number', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatNumber', isTemporary=True),\n",
       " Function(name='format_string', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name='from_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.CsvToStructs', isTemporary=True),\n",
       " Function(name='from_json', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonToStructs', isTemporary=True),\n",
       " Function(name='from_unixtime', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUnixTime', isTemporary=True),\n",
       " Function(name='from_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp', isTemporary=True),\n",
       " Function(name='get_json_object', description=None, className='org.apache.spark.sql.catalyst.expressions.GetJsonObject', isTemporary=True),\n",
       " Function(name='greatest', description=None, className='org.apache.spark.sql.catalyst.expressions.Greatest', isTemporary=True),\n",
       " Function(name='grouping', description=None, className='org.apache.spark.sql.catalyst.expressions.Grouping', isTemporary=True),\n",
       " Function(name='grouping_id', description=None, className='org.apache.spark.sql.catalyst.expressions.GroupingID', isTemporary=True),\n",
       " Function(name='hash', description=None, className='org.apache.spark.sql.catalyst.expressions.Murmur3Hash', isTemporary=True),\n",
       " Function(name='hex', description=None, className='org.apache.spark.sql.catalyst.expressions.Hex', isTemporary=True),\n",
       " Function(name='hour', description=None, className='org.apache.spark.sql.catalyst.expressions.Hour', isTemporary=True),\n",
       " Function(name='hypot', description=None, className='org.apache.spark.sql.catalyst.expressions.Hypot', isTemporary=True),\n",
       " Function(name='if', description=None, className='org.apache.spark.sql.catalyst.expressions.If', isTemporary=True),\n",
       " Function(name='ifnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IfNull', isTemporary=True),\n",
       " Function(name='in', description=None, className='org.apache.spark.sql.catalyst.expressions.In', isTemporary=True),\n",
       " Function(name='initcap', description=None, className='org.apache.spark.sql.catalyst.expressions.InitCap', isTemporary=True),\n",
       " Function(name='inline', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name='inline_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name='input_file_block_length', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockLength', isTemporary=True),\n",
       " Function(name='input_file_block_start', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockStart', isTemporary=True),\n",
       " Function(name='input_file_name', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileName', isTemporary=True),\n",
       " Function(name='instr', description=None, className='org.apache.spark.sql.catalyst.expressions.StringInstr', isTemporary=True),\n",
       " Function(name='int', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='isnan', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNaN', isTemporary=True),\n",
       " Function(name='isnotnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNotNull', isTemporary=True),\n",
       " Function(name='isnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNull', isTemporary=True),\n",
       " Function(name='java_method', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name='json_array_length', description=None, className='org.apache.spark.sql.catalyst.expressions.LengthOfJsonArray', isTemporary=True),\n",
       " Function(name='json_object_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonObjectKeys', isTemporary=True),\n",
       " Function(name='json_tuple', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonTuple', isTemporary=True),\n",
       " Function(name='kurtosis', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis', isTemporary=True),\n",
       " Function(name='lag', description=None, className='org.apache.spark.sql.catalyst.expressions.Lag', isTemporary=True),\n",
       " Function(name='last', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name='last_day', description=None, className='org.apache.spark.sql.catalyst.expressions.LastDay', isTemporary=True),\n",
       " Function(name='last_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name='lcase', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name='lead', description=None, className='org.apache.spark.sql.catalyst.expressions.Lead', isTemporary=True),\n",
       " Function(name='least', description=None, className='org.apache.spark.sql.catalyst.expressions.Least', isTemporary=True),\n",
       " Function(name='left', description=None, className='org.apache.spark.sql.catalyst.expressions.Left', isTemporary=True),\n",
       " Function(name='length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name='levenshtein', description=None, className='org.apache.spark.sql.catalyst.expressions.Levenshtein', isTemporary=True),\n",
       " Function(name='like', description=None, className='org.apache.spark.sql.catalyst.expressions.Like', isTemporary=True),\n",
       " Function(name='ln', description=None, className='org.apache.spark.sql.catalyst.expressions.Log', isTemporary=True),\n",
       " Function(name='locate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name='log', description=None, className='org.apache.spark.sql.catalyst.expressions.Logarithm', isTemporary=True),\n",
       " Function(name='log10', description=None, className='org.apache.spark.sql.catalyst.expressions.Log10', isTemporary=True),\n",
       " Function(name='log1p', description=None, className='org.apache.spark.sql.catalyst.expressions.Log1p', isTemporary=True),\n",
       " Function(name='log2', description=None, className='org.apache.spark.sql.catalyst.expressions.Log2', isTemporary=True),\n",
       " Function(name='lower', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name='lpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLPad', isTemporary=True),\n",
       " Function(name='ltrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimLeft', isTemporary=True),\n",
       " Function(name='make_date', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeDate', isTemporary=True),\n",
       " Function(name='make_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeInterval', isTemporary=True),\n",
       " Function(name='make_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeTimestamp', isTemporary=True),\n",
       " Function(name='map', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateMap', isTemporary=True),\n",
       " Function(name='map_concat', description=None, className='org.apache.spark.sql.catalyst.expressions.MapConcat', isTemporary=True),\n",
       " Function(name='map_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapEntries', isTemporary=True),\n",
       " Function(name='map_filter', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFilter', isTemporary=True),\n",
       " Function(name='map_from_arrays', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromArrays', isTemporary=True),\n",
       " Function(name='map_from_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromEntries', isTemporary=True),\n",
       " Function(name='map_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.MapKeys', isTemporary=True),\n",
       " Function(name='map_values', description=None, className='org.apache.spark.sql.catalyst.expressions.MapValues', isTemporary=True),\n",
       " Function(name='map_zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.MapZipWith', isTemporary=True),\n",
       " Function(name='max', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Max', isTemporary=True),\n",
       " Function(name='max_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MaxBy', isTemporary=True),\n",
       " Function(name='md5', description=None, className='org.apache.spark.sql.catalyst.expressions.Md5', isTemporary=True),\n",
       " Function(name='mean', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name='min', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Min', isTemporary=True),\n",
       " Function(name='min_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MinBy', isTemporary=True),\n",
       " Function(name='minute', description=None, className='org.apache.spark.sql.catalyst.expressions.Minute', isTemporary=True),\n",
       " Function(name='mod', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name='monotonically_increasing_id', description=None, className='org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID', isTemporary=True),\n",
       " Function(name='month', description=None, className='org.apache.spark.sql.catalyst.expressions.Month', isTemporary=True),\n",
       " Function(name='months_between', description=None, className='org.apache.spark.sql.catalyst.expressions.MonthsBetween', isTemporary=True),\n",
       " Function(name='named_struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
       " Function(name='nanvl', description=None, className='org.apache.spark.sql.catalyst.expressions.NaNvl', isTemporary=True),\n",
       " Function(name='negative', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryMinus', isTemporary=True),\n",
       " Function(name='next_day', description=None, className='org.apache.spark.sql.catalyst.expressions.NextDay', isTemporary=True),\n",
       " Function(name='not', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name='now', description=None, className='org.apache.spark.sql.catalyst.expressions.Now', isTemporary=True),\n",
       " Function(name='nth_value', description=None, className='org.apache.spark.sql.catalyst.expressions.NthValue', isTemporary=True),\n",
       " Function(name='ntile', description=None, className='org.apache.spark.sql.catalyst.expressions.NTile', isTemporary=True),\n",
       " Function(name='nullif', description=None, className='org.apache.spark.sql.catalyst.expressions.NullIf', isTemporary=True),\n",
       " Function(name='nvl', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True),\n",
       " Function(name='nvl2', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl2', isTemporary=True),\n",
       " Function(name='octet_length', description=None, className='org.apache.spark.sql.catalyst.expressions.OctetLength', isTemporary=True),\n",
       " Function(name='or', description=None, className='org.apache.spark.sql.catalyst.expressions.Or', isTemporary=True),\n",
       " Function(name='overlay', description=None, className='org.apache.spark.sql.catalyst.expressions.Overlay', isTemporary=True),\n",
       " Function(name='parse_url', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseUrl', isTemporary=True),\n",
       " Function(name='percent_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.PercentRank', isTemporary=True),\n",
       " Function(name='percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Percentile', isTemporary=True),\n",
       " Function(name='percentile_approx', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name='pi', description=None, className='org.apache.spark.sql.catalyst.expressions.Pi', isTemporary=True),\n",
       " Function(name='pmod', description=None, className='org.apache.spark.sql.catalyst.expressions.Pmod', isTemporary=True),\n",
       " Function(name='posexplode', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name='posexplode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name='position', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name='positive', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryPositive', isTemporary=True),\n",
       " Function(name='pow', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name='power', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name='printf', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name='quarter', description=None, className='org.apache.spark.sql.catalyst.expressions.Quarter', isTemporary=True),\n",
       " Function(name='radians', description=None, className='org.apache.spark.sql.catalyst.expressions.ToRadians', isTemporary=True),\n",
       " Function(name='raise_error', description=None, className='org.apache.spark.sql.catalyst.expressions.RaiseError', isTemporary=True),\n",
       " Function(name='rand', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
       " Function(name='randn', description=None, className='org.apache.spark.sql.catalyst.expressions.Randn', isTemporary=True),\n",
       " Function(name='random', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
       " Function(name='rank', description=None, className='org.apache.spark.sql.catalyst.expressions.Rank', isTemporary=True),\n",
       " Function(name='reflect', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name='regexp_extract', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtract', isTemporary=True),\n",
       " Function(name='regexp_extract_all', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtractAll', isTemporary=True),\n",
       " Function(name='regexp_replace', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpReplace', isTemporary=True),\n",
       " Function(name='repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRepeat', isTemporary=True),\n",
       " Function(name='replace', description=None, className='org.apache.spark.sql.catalyst.expressions.StringReplace', isTemporary=True),\n",
       " Function(name='reverse', description=None, className='org.apache.spark.sql.catalyst.expressions.Reverse', isTemporary=True),\n",
       " Function(name='right', description=None, className='org.apache.spark.sql.catalyst.expressions.Right', isTemporary=True),\n",
       " Function(name='rint', description=None, className='org.apache.spark.sql.catalyst.expressions.Rint', isTemporary=True),\n",
       " Function(name='rlike', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
       " Function(name='rollup', description=None, className='org.apache.spark.sql.catalyst.expressions.Rollup', isTemporary=True),\n",
       " Function(name='round', description=None, className='org.apache.spark.sql.catalyst.expressions.Round', isTemporary=True),\n",
       " Function(name='row_number', description=None, className='org.apache.spark.sql.catalyst.expressions.RowNumber', isTemporary=True),\n",
       " Function(name='rpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRPad', isTemporary=True),\n",
       " Function(name='rtrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimRight', isTemporary=True),\n",
       " Function(name='schema_of_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfCsv', isTemporary=True),\n",
       " Function(name='schema_of_json', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfJson', isTemporary=True),\n",
       " Function(name='second', description=None, className='org.apache.spark.sql.catalyst.expressions.Second', isTemporary=True),\n",
       " Function(name='sentences', description=None, className='org.apache.spark.sql.catalyst.expressions.Sentences', isTemporary=True),\n",
       " Function(name='sequence', description=None, className='org.apache.spark.sql.catalyst.expressions.Sequence', isTemporary=True),\n",
       " Function(name='sha', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name='sha1', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name='sha2', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha2', isTemporary=True),\n",
       " Function(name='shiftleft', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftLeft', isTemporary=True),\n",
       " Function(name='shiftright', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRight', isTemporary=True),\n",
       " Function(name='shiftrightunsigned', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned', isTemporary=True),\n",
       " Function(name='shuffle', description=None, className='org.apache.spark.sql.catalyst.expressions.Shuffle', isTemporary=True),\n",
       " Function(name='sign', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name='signum', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name='sin', description=None, className='org.apache.spark.sql.catalyst.expressions.Sin', isTemporary=True),\n",
       " Function(name='sinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Sinh', isTemporary=True),\n",
       " Function(name='size', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name='skewness', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Skewness', isTemporary=True),\n",
       " Function(name='slice', description=None, className='org.apache.spark.sql.catalyst.expressions.Slice', isTemporary=True),\n",
       " Function(name='smallint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='some', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
       " Function(name='sort_array', description=None, className='org.apache.spark.sql.catalyst.expressions.SortArray', isTemporary=True),\n",
       " Function(name='soundex', description=None, className='org.apache.spark.sql.catalyst.expressions.SoundEx', isTemporary=True),\n",
       " Function(name='space', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSpace', isTemporary=True),\n",
       " Function(name='spark_partition_id', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkPartitionID', isTemporary=True),\n",
       " Function(name='split', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSplit', isTemporary=True),\n",
       " Function(name='sqrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Sqrt', isTemporary=True),\n",
       " Function(name='stack', description=None, className='org.apache.spark.sql.catalyst.expressions.Stack', isTemporary=True),\n",
       " Function(name='std', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='stddev', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='stddev_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop', isTemporary=True),\n",
       " Function(name='stddev_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name='str_to_map', description=None, className='org.apache.spark.sql.catalyst.expressions.StringToMap', isTemporary=True),\n",
       " Function(name='string', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
       " Function(name='substr', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name='substring', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name='substring_index', description=None, className='org.apache.spark.sql.catalyst.expressions.SubstringIndex', isTemporary=True),\n",
       " Function(name='sum', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Sum', isTemporary=True),\n",
       " Function(name='tan', description=None, className='org.apache.spark.sql.catalyst.expressions.Tan', isTemporary=True),\n",
       " Function(name='tanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Tanh', isTemporary=True),\n",
       " Function(name='timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='timestamp_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.MicrosToTimestamp', isTemporary=True),\n",
       " Function(name='timestamp_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.MillisToTimestamp', isTemporary=True),\n",
       " Function(name='timestamp_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.SecondsToTimestamp', isTemporary=True),\n",
       " Function(name='tinyint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name='to_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToCsv', isTemporary=True),\n",
       " Function(name='to_date', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToDate', isTemporary=True),\n",
       " Function(name='to_json', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToJson', isTemporary=True),\n",
       " Function(name='to_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToTimestamp', isTemporary=True),\n",
       " Function(name='to_unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp', isTemporary=True),\n",
       " Function(name='to_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp', isTemporary=True),\n",
       " Function(name='transform', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayTransform', isTemporary=True),\n",
       " Function(name='transform_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformKeys', isTemporary=True),\n",
       " Function(name='transform_values', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformValues', isTemporary=True),\n",
       " Function(name='translate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTranslate', isTemporary=True),\n",
       " Function(name='trim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrim', isTemporary=True),\n",
       " Function(name='trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncDate', isTemporary=True),\n",
       " Function(name='typeof', description=None, className='org.apache.spark.sql.catalyst.expressions.TypeOf', isTemporary=True),\n",
       " Function(name='ucase', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name='unbase64', description=None, className='org.apache.spark.sql.catalyst.expressions.UnBase64', isTemporary=True),\n",
       " Function(name='unhex', description=None, className='org.apache.spark.sql.catalyst.expressions.Unhex', isTemporary=True),\n",
       " Function(name='unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixDate', isTemporary=True),\n",
       " Function(name='unix_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMicros', isTemporary=True),\n",
       " Function(name='unix_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMillis', isTemporary=True),\n",
       " Function(name='unix_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixSeconds', isTemporary=True),\n",
       " Function(name='unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixTimestamp', isTemporary=True),\n",
       " Function(name='upper', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name='uuid', description=None, className='org.apache.spark.sql.catalyst.expressions.Uuid', isTemporary=True),\n",
       " Function(name='var_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop', isTemporary=True),\n",
       " Function(name='var_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name='variance', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name='version', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkVersion', isTemporary=True),\n",
       " Function(name='weekday', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekDay', isTemporary=True),\n",
       " Function(name='weekofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekOfYear', isTemporary=True),\n",
       " Function(name='when', description=None, className='org.apache.spark.sql.catalyst.expressions.CaseWhen', isTemporary=True),\n",
       " Function(name='width_bucket', description=None, className='org.apache.spark.sql.catalyst.expressions.WidthBucket', isTemporary=True),\n",
       " Function(name='window', description=None, className='org.apache.spark.sql.catalyst.expressions.TimeWindow', isTemporary=True),\n",
       " Function(name='xpath', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathList', isTemporary=True),\n",
       " Function(name='xpath_boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean', isTemporary=True),\n",
       " Function(name='xpath_double', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name='xpath_float', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathFloat', isTemporary=True),\n",
       " Function(name='xpath_int', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathInt', isTemporary=True),\n",
       " Function(name='xpath_long', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathLong', isTemporary=True),\n",
       " Function(name='xpath_number', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name='xpath_short', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathShort', isTemporary=True),\n",
       " Function(name='xpath_string', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathString', isTemporary=True),\n",
       " Function(name='xxhash64', description=None, className='org.apache.spark.sql.catalyst.expressions.XxHash64', isTemporary=True),\n",
       " Function(name='year', description=None, className='org.apache.spark.sql.catalyst.expressions.Year', isTemporary=True),\n",
       " Function(name='zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.ZipWith', isTemporary=True),\n",
       " Function(name='|', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseOr', isTemporary=True),\n",
       " Function(name='~', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseNot', isTemporary=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listFunctions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e61beda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {username}_airlines CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce1c7b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e64296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_airlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb580fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_airlines'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05cb2652",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_codes_path = f\"/user/{username}/airtraffic_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "822c5b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {username}_airlines.airport_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b747c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_df =spark. \\\n",
    "    read. \\\n",
    "    csv(airports_codes_path,\n",
    "       sep='\\t',\n",
    "       header=True,\n",
    "       inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad6f2687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_codes_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36e46fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|      City|State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|   BC| Canada| YXX|\n",
      "|  Aberdeen|   SD|    USA| ABR|\n",
      "|   Abilene|   TX|    USA| ABI|\n",
      "|     Akron|   OH|    USA| CAK|\n",
      "|   Alamosa|   CO|    USA| ALS|\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "def9c4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(airport_codes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbfa7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_df.write.saveAsTable(\"airport_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "553fb8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes = spark.read.table(\"airport_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5287f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(airport_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fed90623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                 |comment|\n",
      "+----------------------------+------------------------------------------------------------------------------------------+-------+\n",
      "|City                        |string                                                                                    |null   |\n",
      "|State                       |string                                                                                    |null   |\n",
      "|Country                     |string                                                                                    |null   |\n",
      "|IATA                        |string                                                                                    |null   |\n",
      "|                            |                                                                                          |       |\n",
      "|# Detailed Table Information|                                                                                          |       |\n",
      "|Database                    |itv011204_airlines                                                                        |       |\n",
      "|Table                       |airport_codes                                                                             |       |\n",
      "|Owner                       |itv011204                                                                                 |       |\n",
      "|Created Time                |Thu Feb 15 09:15:24 EST 2024                                                              |       |\n",
      "|Last Access                 |UNKNOWN                                                                                   |       |\n",
      "|Created By                  |Spark 3.1.2                                                                               |       |\n",
      "|Type                        |MANAGED                                                                                   |       |\n",
      "|Provider                    |parquet                                                                                   |       |\n",
      "|Statistics                  |9048 bytes                                                                                |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv011204/warehouse/itv011204_airlines.db/airport_codes|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                               |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                             |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                            |       |\n",
      "+----------------------------+------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FORMATTED airport_codes\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f331946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- IATA: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96aa36d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|State|count|\n",
      "+-----+-----+\n",
      "|   BC|   22|\n",
      "|   SD|    7|\n",
      "|   NY|   18|\n",
      "|   NM|    9|\n",
      "|   NE|    9|\n",
      "|   MI|   18|\n",
      "|  NWT|    4|\n",
      "|   NC|   10|\n",
      "|   NJ|    3|\n",
      "|   MD|    3|\n",
      "|   WV|    8|\n",
      "|   MN|    8|\n",
      "|   IL|   12|\n",
      "|   ID|    6|\n",
      "|   IA|    8|\n",
      "|   MO|    8|\n",
      "|   SC|    6|\n",
      "|   VA|    7|\n",
      "|  PEI|    1|\n",
      "|   TN|    6|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes.\\\n",
    "    groupBy('State'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38278fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,lit, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b01a2439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|State|AirportCount|\n",
      "+-----+------------+\n",
      "|   CA|          29|\n",
      "|   TX|          26|\n",
      "|   AK|          25|\n",
      "|   BC|          22|\n",
      "|   NY|          18|\n",
      "|   MI|          18|\n",
      "|   FL|          18|\n",
      "|   ON|          18|\n",
      "|   MT|          14|\n",
      "|   PA|          13|\n",
      "|   PQ|          13|\n",
      "|   IL|          12|\n",
      "|   CO|          12|\n",
      "|   WY|          10|\n",
      "|   NC|          10|\n",
      "|   NM|           9|\n",
      "|   NE|           9|\n",
      "|   GA|           9|\n",
      "|   KS|           9|\n",
      "|   WA|           9|\n",
      "+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes.\\\n",
    "    groupBy('State'). \\\n",
    "    agg(count(lit(1)).alias('AirportCount')). \\\n",
    "    orderBy(col(\"AirportCount\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2024b40f",
   "metadata": {},
   "source": [
    "### 221 Create Spark Metastore Partitioned Tables using Data Frame APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45be9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.sql.ui.port','0'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Exploring spark catalog'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e301db64",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a table based on the dataset in a data source.\n",
       "\n",
       "It returns the DataFrame associated with the table.\n",
       "\n",
       "The data source is specified by the ``source`` and a set of ``options``.\n",
       "If ``source`` is not specified, the default data source configured by\n",
       "``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\n",
       "created from the data at the given path. Otherwise a managed table is created.\n",
       "\n",
       "Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
       "created table.\n",
       "\n",
       ".. versionadded:: 2.2.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "\n",
       ".. versionchanged:: 3.1\n",
       "   Added the ``description`` parameter.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog.createTable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ea8a83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a10284af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions','2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df998e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f'DROP DATABASE IF EXISTS {username}_retail CASCADE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22a559d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f'CREATE DATABASE IF NOT EXISTS {username}_retail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c598003",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_retail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf72f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_path  ='/public/retail_db/orders'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c7616e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 hdfs supergroup    2999944 2021-01-28 09:27 /public/retail_db/orders/part-00000\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /public/retail_db/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0cd4b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS orders_part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6297b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /user/`whoami`/retail_db/orders_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ac23184",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `/user/itv011204/retail_db/orders_part': No such file or directory\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\nhdfs dfs -rm -R /user/`whoami`/retail_db/orders_part\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-020c881010d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nhdfs dfs -rm -R /user/`whoami`/retail_db/orders_part\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2371\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2372\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\nhdfs dfs -rm -R /user/`whoami`/retail_db/orders_part\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -rm -R /user/`whoami`/retail_db/orders_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d59f8e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d74df73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark. \\\n",
    "    read. \\\n",
    "    csv(\n",
    "        orders_path,\n",
    "        schema = \"\"\"order_id INT, order_date DATE, \n",
    "                order_customer_id INT, order_status STRING\"\"\"\n",
    "    ). \\\n",
    "    withColumn('order_month',date_format(col('order_date'),'yyyyMM')). \\\n",
    "    write. \\\n",
    "    partitionBy('order_month'). \\\n",
    "    parquet(f'/user/{username}/retail_db/orders_part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a63dc71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 items\n",
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/_SUCCESS\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201307\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201308\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201309\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201310\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201311\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201312\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201401\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201402\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201403\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201404\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201405\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201406\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201407\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/`whoami`/retail_db/orders_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79f4ff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/_SUCCESS\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201307\n",
      "-rw-r--r--   3 itv011204 supergroup      14435 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201307/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201308\n",
      "-rw-r--r--   3 itv011204 supergroup      49997 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201308/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201309\n",
      "-rw-r--r--   3 itv011204 supergroup      51358 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201309/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201310\n",
      "-rw-r--r--   3 itv011204 supergroup      47051 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201310/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:46 /user/itv011204/retail_db/orders_part/order_month=201311\n",
      "-rw-r--r--   3 itv011204 supergroup      55949 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201311/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201312\n",
      "-rw-r--r--   3 itv011204 supergroup      51794 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201312/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201401\n",
      "-rw-r--r--   3 itv011204 supergroup      51938 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201401/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201402\n",
      "-rw-r--r--   3 itv011204 supergroup      49591 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201402/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201403\n",
      "-rw-r--r--   3 itv011204 supergroup      50816 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201403/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201404\n",
      "-rw-r--r--   3 itv011204 supergroup      49799 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201404/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201405\n",
      "-rw-r--r--   3 itv011204 supergroup      48183 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201405/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201406\n",
      "-rw-r--r--   3 itv011204 supergroup      46828 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201406/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201407\n",
      "-rw-r--r--   3 itv011204 supergroup      39605 2024-02-15 09:47 /user/itv011204/retail_db/orders_part/order_month=201407/part-00000-ea10ebc3-7549-4715-904e-075a4a0165aa.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls -R /user/`whoami`/retail_db/orders_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a805dec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|2013-07-25|            11599|         CLOSED|\n",
      "|       2|2013-07-25|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|            12111|       COMPLETE|\n",
      "|       4|2013-07-25|             8827|         CLOSED|\n",
      "|       5|2013-07-25|            11318|       COMPLETE|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/{username}/retail_db/orders_part/order_month=201307').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8779bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1533"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/{username}/retail_db/orders_part/order_month=201307').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a38f1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|   45168|2014-05-01|             2383|       COMPLETE|\n",
      "|   45169|2014-05-01|             7212|PENDING_PAYMENT|\n",
      "|   45170|2014-05-01|             2400|SUSPECTED_FRAUD|\n",
      "|   45171|2014-05-01|             9003|PENDING_PAYMENT|\n",
      "|   45172|2014-05-01|             2508|PENDING_PAYMENT|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/itv011204/retail_db/orders_part/order_month=201405').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "407e2b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5467"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/itv011204/retail_db/orders_part/order_month=201405').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5970e742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|   15488|2013-11-01|             8987|PENDING_PAYMENT|     201311|\n",
      "|   15489|2013-11-01|             5359|PENDING_PAYMENT|     201311|\n",
      "|   15490|2013-11-01|            10149|       COMPLETE|     201311|\n",
      "|   15491|2013-11-01|            10635|        ON_HOLD|     201311|\n",
      "|   15492|2013-11-01|             7784|PENDING_PAYMENT|     201311|\n",
      "|   15493|2013-11-01|             1104|        ON_HOLD|     201311|\n",
      "|   15494|2013-11-01|             7313|     PROCESSING|     201311|\n",
      "|   15495|2013-11-01|             7067|         CLOSED|     201311|\n",
      "|   15496|2013-11-01|            12153|PENDING_PAYMENT|     201311|\n",
      "|   15497|2013-11-01|            11115|PENDING_PAYMENT|     201311|\n",
      "|   15498|2013-11-01|            11195|       COMPLETE|     201311|\n",
      "|   15499|2013-11-01|             7113|         CLOSED|     201311|\n",
      "|   15500|2013-11-01|             6780|PENDING_PAYMENT|     201311|\n",
      "|   15501|2013-11-01|             9703|        ON_HOLD|     201311|\n",
      "|   15502|2013-11-01|            10009|       COMPLETE|     201311|\n",
      "|   15503|2013-11-01|             6521|PENDING_PAYMENT|     201311|\n",
      "|   15504|2013-11-01|            10601|PENDING_PAYMENT|     201311|\n",
      "|   15505|2013-11-01|             1068|PENDING_PAYMENT|     201311|\n",
      "|   15506|2013-11-01|             2742|PENDING_PAYMENT|     201311|\n",
      "|   15507|2013-11-01|             3503|       COMPLETE|     201311|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/itv011204/retail_db/orders_part').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c5ca300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(f'/user/itv011204/retail_db/orders_part').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42013354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>order_id</th><th>order_date</th><th>order_customer_id</th><th>order_status</th><th>order_month</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+----------+-----------------+------------+-----------+\n",
       "|order_id|order_date|order_customer_id|order_status|order_month|\n",
       "+--------+----------+-----------------+------------+-----------+\n",
       "+--------+----------+-----------------+------------+-----------+"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark. \\\n",
    "    catalog. \\\n",
    "    createTable('orders_part',\n",
    "        path=f'/user/{username}/retail_db/orders_part',\n",
    "        source='parquet'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42cdddb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+------------+-----------+\n",
      "|order_id|order_date|order_customer_id|order_status|order_month|\n",
      "+--------+----------+-----------------+------------+-----------+\n",
      "+--------+----------+-----------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table('orders_part').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f1f07ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|partition|\n",
      "+---------+\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW PARTITIONS orders_part\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "755bd159",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.recoverPartitions('orders_part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "892451ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         partition|\n",
      "+------------------+\n",
      "|order_month=201307|\n",
      "|order_month=201308|\n",
      "|order_month=201309|\n",
      "|order_month=201310|\n",
      "|order_month=201311|\n",
      "|order_month=201312|\n",
      "|order_month=201401|\n",
      "|order_month=201402|\n",
      "|order_month=201403|\n",
      "|order_month=201404|\n",
      "|order_month=201405|\n",
      "|order_month=201406|\n",
      "|order_month=201407|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW PARTITIONS orders_part\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "051bac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|   15488|2013-11-01|             8987|PENDING_PAYMENT|     201311|\n",
      "|   15489|2013-11-01|             5359|PENDING_PAYMENT|     201311|\n",
      "|   15490|2013-11-01|            10149|       COMPLETE|     201311|\n",
      "|   15491|2013-11-01|            10635|        ON_HOLD|     201311|\n",
      "|   15492|2013-11-01|             7784|PENDING_PAYMENT|     201311|\n",
      "|   15493|2013-11-01|             1104|        ON_HOLD|     201311|\n",
      "|   15494|2013-11-01|             7313|     PROCESSING|     201311|\n",
      "|   15495|2013-11-01|             7067|         CLOSED|     201311|\n",
      "|   15496|2013-11-01|            12153|PENDING_PAYMENT|     201311|\n",
      "|   15497|2013-11-01|            11115|PENDING_PAYMENT|     201311|\n",
      "|   15498|2013-11-01|            11195|       COMPLETE|     201311|\n",
      "|   15499|2013-11-01|             7113|         CLOSED|     201311|\n",
      "|   15500|2013-11-01|             6780|PENDING_PAYMENT|     201311|\n",
      "|   15501|2013-11-01|             9703|        ON_HOLD|     201311|\n",
      "|   15502|2013-11-01|            10009|       COMPLETE|     201311|\n",
      "|   15503|2013-11-01|             6521|PENDING_PAYMENT|     201311|\n",
      "|   15504|2013-11-01|            10601|PENDING_PAYMENT|     201311|\n",
      "|   15505|2013-11-01|             1068|PENDING_PAYMENT|     201311|\n",
      "|   15506|2013-11-01|             2742|PENDING_PAYMENT|     201311|\n",
      "|   15507|2013-11-01|             3503|       COMPLETE|     201311|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table('orders_part').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "497b2430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|order_month|count(1)|\n",
      "+-----------+--------+\n",
      "|     201405|    5467|\n",
      "|     201308|    5680|\n",
      "|     201404|    5657|\n",
      "|     201311|    6381|\n",
      "|     201401|    5908|\n",
      "|     201309|    5841|\n",
      "|     201312|    5892|\n",
      "|     201403|    5778|\n",
      "|     201402|    5635|\n",
      "|     201310|    5335|\n",
      "|     201406|    5308|\n",
      "|     201407|    4468|\n",
      "|     201307|    1533|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT order_month, count(1) FROM orders_part GROUP BY order_month\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4d471c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+-------+\n",
      "|col_name               |data_type|comment|\n",
      "+-----------------------+---------+-------+\n",
      "|order_id               |int      |null   |\n",
      "|order_date             |date     |null   |\n",
      "|order_customer_id      |int      |null   |\n",
      "|order_status           |string   |null   |\n",
      "|order_month            |int      |null   |\n",
      "|# Partition Information|         |       |\n",
      "|# col_name             |data_type|comment|\n",
      "|order_month            |int      |null   |\n",
      "+-----------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE orders_part\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a682c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|order_month|count|\n",
      "+-----------+-----+\n",
      "|     201311| 6381|\n",
      "|     201401| 5908|\n",
      "|     201309| 5841|\n",
      "|     201308| 5680|\n",
      "|     201404| 5657|\n",
      "|     201405| 5467|\n",
      "|     201310| 5335|\n",
      "|     201406| 5308|\n",
      "|     201407| 4468|\n",
      "|     201403| 5778|\n",
      "|     201402| 5635|\n",
      "|     201307| 1533|\n",
      "|     201312| 5892|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table('orders_part'). \\\n",
    "    groupBy('order_month'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7a8e01",
   "metadata": {},
   "source": [
    "### 222 Saving as Spark Metastore Partitioned table using Data Frame APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5069cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.sql.ui.port','0'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Exploring spark catalog'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a57fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions','2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af458ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username}_retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa9d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_retail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1497fc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_retail'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bf28907",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_path = '/public/retail_db/orders'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ce00ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 hdfs supergroup    2999944 2021-01-28 09:27 /public/retail_db/orders/part-00000\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /public/retail_db/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592c24b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "014-06-12 00:00:00.0,4229,PENDING\n",
      "68861,2014-06-13 00:00:00.0,3031,PENDING_PAYMENT\n",
      "68862,2014-06-15 00:00:00.0,7326,PROCESSING\n",
      "68863,2014-06-16 00:00:00.0,3361,CLOSED\n",
      "68864,2014-06-18 00:00:00.0,9634,ON_HOLD\n",
      "68865,2014-06-19 00:00:00.0,4567,SUSPECTED_FRAUD\n",
      "68866,2014-06-20 00:00:00.0,3890,PENDING_PAYMENT\n",
      "68867,2014-06-23 00:00:00.0,869,CANCELED\n",
      "68868,2014-06-24 00:00:00.0,10184,PENDING\n",
      "68869,2014-06-25 00:00:00.0,7456,PROCESSING\n",
      "68870,2014-06-26 00:00:00.0,3343,COMPLETE\n",
      "68871,2014-06-28 00:00:00.0,4960,PENDING\n",
      "68872,2014-06-29 00:00:00.0,3354,COMPLETE\n",
      "68873,2014-06-30 00:00:00.0,4545,PENDING\n",
      "68874,2014-07-03 00:00:00.0,1601,COMPLETE\n",
      "68875,2014-07-04 00:00:00.0,10637,ON_HOLD\n",
      "68876,2014-07-06 00:00:00.0,4124,COMPLETE\n",
      "68877,2014-07-07 00:00:00.0,9692,ON_HOLD\n",
      "68878,2014-07-08 00:00:00.0,6753,COMPLETE\n",
      "68879,2014-07-09 00:00:00.0,778,COMPLETE\n",
      "68880,2014-07-13 00:00:00.0,1117,COMPLETE\n",
      "68881,2014-07-19 00:00:00.0,2518,PENDING_PAYMENT\n",
      "68882,2014-07-22 00:00:00.0,10000,ON_HOLD\n",
      "68883,2014-07-23 00:00:00.0,5533,COMPLETE\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -tail /public/retail_db/orders/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a21e34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS orders_part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38b229e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7285c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark. \\\n",
    "    read. \\\n",
    "    csv(orders_path,\n",
    "        schema = \"\"\"order_id INT, order_date DATE,\n",
    "                order_customer_id INT, order_status STRING\n",
    "                \"\"\"\n",
    "    ). \\\n",
    "    withColumn('order_month',date_format('order_date','yyyyMM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ede643d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_month: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63fd12b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|       1|2013-07-25|            11599|         CLOSED|     201307|\n",
      "|       2|2013-07-25|              256|PENDING_PAYMENT|     201307|\n",
      "|       3|2013-07-25|            12111|       COMPLETE|     201307|\n",
      "|       4|2013-07-25|             8827|         CLOSED|     201307|\n",
      "|       5|2013-07-25|            11318|       COMPLETE|     201307|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05ef1f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders. \\\n",
    "    write. \\\n",
    "    saveAsTable(\n",
    "        'orders_part',\n",
    "        mode='overwrite',\n",
    "        partitionBy='order_month'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d70e563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/_SUCCESS\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201307\n",
      "-rw-r--r--   3 itv011204 supergroup      14435 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201307/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201308\n",
      "-rw-r--r--   3 itv011204 supergroup      49997 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201308/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201309\n",
      "-rw-r--r--   3 itv011204 supergroup      51358 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201309/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201310\n",
      "-rw-r--r--   3 itv011204 supergroup      47051 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201310/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201311\n",
      "-rw-r--r--   3 itv011204 supergroup      55949 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201311/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201312\n",
      "-rw-r--r--   3 itv011204 supergroup      51794 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201312/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201401\n",
      "-rw-r--r--   3 itv011204 supergroup      51938 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201401/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201402\n",
      "-rw-r--r--   3 itv011204 supergroup      49591 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201402/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201403\n",
      "-rw-r--r--   3 itv011204 supergroup      50816 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201403/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201404\n",
      "-rw-r--r--   3 itv011204 supergroup      49799 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201404/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201405\n",
      "-rw-r--r--   3 itv011204 supergroup      48183 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201405/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201406\n",
      "-rw-r--r--   3 itv011204 supergroup      46828 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201406/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n",
      "drwxr-xr-x   - itv011204 supergroup          0 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201407\n",
      "-rw-r--r--   3 itv011204 supergroup      39605 2024-02-15 10:25 /user/itv011204/warehouse/itv011204_retail.db/orders_part/order_month=201407/part-00000-31ef1390-ce85-4d27-878a-3da475261aa5.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "\n",
    "hdfs dfs -ls -R /user/`whoami`/warehouse/`whoami`_retail.db/orders_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d25052cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+\n",
      "|order_id|order_date|order_customer_id|   order_status|\n",
      "+--------+----------+-----------------+---------------+\n",
      "|       1|2013-07-25|            11599|         CLOSED|\n",
      "|       2|2013-07-25|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|            12111|       COMPLETE|\n",
      "|       4|2013-07-25|             8827|         CLOSED|\n",
      "|       5|2013-07-25|            11318|       COMPLETE|\n",
      "+--------+----------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f\"/user/{username}/warehouse/{username}_retail.db/orders_part/order_month=201307\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff30a053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|   15488|2013-11-01|             8987|PENDING_PAYMENT|     201311|\n",
      "|   15489|2013-11-01|             5359|PENDING_PAYMENT|     201311|\n",
      "|   15490|2013-11-01|            10149|       COMPLETE|     201311|\n",
      "|   15491|2013-11-01|            10635|        ON_HOLD|     201311|\n",
      "|   15492|2013-11-01|             7784|PENDING_PAYMENT|     201311|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f\"/user/{username}/warehouse/{username}_retail.db/orders_part\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d0ff0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         partition|\n",
      "+------------------+\n",
      "|order_month=201307|\n",
      "|order_month=201308|\n",
      "|order_month=201309|\n",
      "|order_month=201310|\n",
      "|order_month=201311|\n",
      "|order_month=201312|\n",
      "|order_month=201401|\n",
      "|order_month=201402|\n",
      "|order_month=201403|\n",
      "|order_month=201404|\n",
      "|order_month=201405|\n",
      "|order_month=201406|\n",
      "|order_month=201407|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW PARTITIONS orders_part\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fa8729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|order_id|order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "|   15488|2013-11-01|             8987|PENDING_PAYMENT|     201311|\n",
      "|   15489|2013-11-01|             5359|PENDING_PAYMENT|     201311|\n",
      "|   15490|2013-11-01|            10149|       COMPLETE|     201311|\n",
      "|   15491|2013-11-01|            10635|        ON_HOLD|     201311|\n",
      "|   15492|2013-11-01|             7784|PENDING_PAYMENT|     201311|\n",
      "+--------+----------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table('orders_part').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81485206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|order_month|count(1)|\n",
      "+-----------+--------+\n",
      "|     201403|    5778|\n",
      "|     201308|    5680|\n",
      "|     201404|    5657|\n",
      "|     201406|    5308|\n",
      "|     201402|    5635|\n",
      "+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT order_month, count(1) from orders_part GROUP BY order_month\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c935a776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|order_month|count|\n",
      "+-----------+-----+\n",
      "|     201406| 5308|\n",
      "|     201403| 5778|\n",
      "|     201308| 5680|\n",
      "|     201404| 5657|\n",
      "|     201311| 6381|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark. \\\n",
    "    read. \\\n",
    "    table(\"orders_part\"). \\\n",
    "    groupBy('order_month'). \\\n",
    "    count(). \\\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33536070",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 223 Creating Temporary views on top of Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f00eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.sql.warehouse.dir',f'/user/{username}/warehouse'). \\\n",
    "        config('spark.sql.ui.port','0'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout','6000'). \\\n",
    "        config('spark.driver.memory','6g'). \\\n",
    "        config('spark.executor.memory','6g'). \\\n",
    "        config('spark.dynamicAllocation.minExecutors', '4'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Section 18 Exploring spark catalog'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c281f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions','2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edb31391",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(f'{username}_airlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d61b12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204_airlines'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bb71788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='itv011204_airlines', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b0cbf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_cods_path = f\"/public/airlines_all/airport-codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f094733",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_df = spark. \\\n",
    "    read. \\\n",
    "    csv(\n",
    "        airport_cods_path,\n",
    "        sep = '\\t',\n",
    "        header = True,\n",
    "        inferSchema = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4edb187d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_codes_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0eb975fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- IATA: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17efe453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|      City|State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|   BC| Canada| YXX|\n",
      "|  Aberdeen|   SD|    USA| ABR|\n",
      "|   Abilene|   TX|    USA| ABI|\n",
      "|     Akron|   OH|    USA| CAK|\n",
      "|   Alamosa|   CO|    USA| ALS|\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5cbd3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='itv011204_airlines', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16be550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_df.createTempView('airport_codes_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa52cc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='airport_codes', database='itv011204_airlines', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='airport_codes_v', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e351081",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes = spark.read.table('airport_codes_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6de0f774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|state|count|\n",
      "+-----+-----+\n",
      "|   BC|   22|\n",
      "|   SD|    7|\n",
      "|   NY|   18|\n",
      "|   NM|    9|\n",
      "|   NE|    9|\n",
      "|   MI|   18|\n",
      "|  NWT|    4|\n",
      "|   NC|   10|\n",
      "|   NJ|    3|\n",
      "|   MD|    3|\n",
      "|   WV|    8|\n",
      "|   MN|    8|\n",
      "|   IL|   12|\n",
      "|   ID|    6|\n",
      "|   IA|    8|\n",
      "|   MO|    8|\n",
      "|   SC|    6|\n",
      "|   VA|    7|\n",
      "|  PEI|    1|\n",
      "|   TN|    6|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes. \\\n",
    "    groupBy(\"state\"). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae4233aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|state|airport_count|\n",
      "+-----+-------------+\n",
      "|   CA|           29|\n",
      "|   TX|           26|\n",
      "|   AK|           25|\n",
      "|   BC|           22|\n",
      "|   NY|           18|\n",
      "|   ON|           18|\n",
      "|   MI|           18|\n",
      "|   FL|           18|\n",
      "|   MT|           14|\n",
      "|   PA|           13|\n",
      "|   PQ|           13|\n",
      "|   IL|           12|\n",
      "|   CO|           12|\n",
      "|   NC|           10|\n",
      "|   WY|           10|\n",
      "|   NE|            9|\n",
      "|   WI|            9|\n",
      "|   WA|            9|\n",
      "|   GA|            9|\n",
      "|   NM|            9|\n",
      "+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT state, count(1) airport_count FROM airport_codes_v GROUP BY state ORDER BY airport_count desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c921a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
