{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa3cbbd",
   "metadata": {},
   "source": [
    "## Section 15 Pyspark Processing Column Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b35304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf8c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc4bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders= spark.read.csv(\n",
    "    '/public/retail_db/orders',\n",
    "    schema='order_id INT, order_date STRING, order_customer_id INT, order_status STRING'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3426ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37f1097e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5585a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4372a882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.select('*', date_format('order_date','yyyyMM').alias('order_month')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cafb1c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.withColumn('order_month', date_format('order_date','yyyyMM')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c736a5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|   25876|2014-01-01 00:00:...|             3414|PENDING_PAYMENT|\n",
      "|   25877|2014-01-01 00:00:...|             5549|PENDING_PAYMENT|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter\n",
    "\n",
    "orders.filter(date_format('order_date','yyyyMM')==201401).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62248e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|order_month|count|\n",
      "+-----------+-----+\n",
      "|     201401| 5908|\n",
      "|     201405| 5467|\n",
      "+-----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupBy\n",
    "\n",
    "orders.groupBy(date_format('order_date','yyyyMM').alias('order_month')). \\\n",
    "    count(). \\\n",
    "    show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f8e0d",
   "metadata": {},
   "source": [
    "### 167 Create Dummy Data Frame to explore Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60531a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c28212",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l, \"dummy STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879b2d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dummy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97470470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1e1adba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2024-02-06|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "df.select(current_date()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894c3a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2024-02-06|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date().alias(\"current_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e904ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [\n",
    "    (1, \"Scott\", \"Tiger\", 1000.0, \n",
    "      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "    ),\n",
    "     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "     ),\n",
    "     (3, \"Nick\", \"Junior\", 750.0, \n",
    "      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "     ),\n",
    "     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "     )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4cf900f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ff57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.createDataFrame(employees,\n",
    "            schema=\"\"\"employee_id INT, first_name STRING, last_name STRING,\n",
    "            salary FLOAT, nationality STRING,\n",
    "            phone STRING, ssn STRING \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48d08f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "351b6a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|nationality   |phone           |ssn        |\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|1          |Scott     |Tiger    |1000.0|united states |+1 123 456 7890 |123 45 6789|\n",
      "|2          |Henry     |Ford     |1250.0|India         |+91 234 567 8901|456 78 9123|\n",
      "|3          |Nick      |Junior   |750.0 |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|4          |Bill      |Gomes    |1500.0|AUSTRALIA     |+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e2e0c",
   "metadata": {},
   "source": [
    "### Categories of Predefined functions used in Spark DataFrame columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc589d9",
   "metadata": {},
   "source": [
    "There are approximately 300 functions under pyspark.sql.functions. At a higher level they can be grouped into a few categories.\n",
    "\n",
    "* String Manipulation Functions\n",
    "\n",
    "    * Case Conversion - lower, upper\n",
    "    * Getting Length - length\n",
    "    * Extracting substrings - substring, split\n",
    "    * Trimming - trim, ltrim, rtrim\n",
    "    * Padding - lpad, rpad\n",
    "    * Concatenating string - concat, concat_ws\n",
    "\n",
    "* Date Manipulation Functions\n",
    "\n",
    "    * Getting current date and time - current_date, current_timestamp\n",
    "    * Date Arithmetic - date_add, date_sub, datediff, months_between, add_months, next_day\n",
    "    * Beginning and Ending Date or Time - last_day, trunc, date_trunc\n",
    "    * Formatting Date - date_format\n",
    "    * Extracting Information - dayofyear, dayofmonth, dayofweek, year, month\n",
    "\n",
    "* Aggregate Functions\n",
    "\n",
    "    * count, countDistinct\n",
    "    * sum, avg\n",
    "    * min, max\n",
    "\n",
    "* Other Functions - We will explore depending on the use cases.\n",
    "\n",
    "    * CASE and WHEN\n",
    "    * CAST for type casting\n",
    "    * Functions to manage special types such as ARRAY, MAP, STRUCT type columns\n",
    "\n",
    "* Many others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd580cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 169 Col and lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c338261",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [\n",
    "    (1, \"Scott\", \"Tiger\", 1000.0, \n",
    "      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "    ),\n",
    "     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "     ),\n",
    "     (3, \"Nick\", \"Junior\", 750.0, \n",
    "      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "     ),\n",
    "     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "     )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9547f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.createDataFrame(employees,\n",
    "            schema=\"\"\"employee_id INT, first_name STRING, last_name STRING,\n",
    "            salary FLOAT, nationality STRING,\n",
    "            phone STRING, ssn STRING \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b5cdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(\"first_name\", \"last_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ffdc3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|   nationality|count|\n",
      "+--------------+-----+\n",
      "|         India|    1|\n",
      "|united KINGDOM|    1|\n",
      "| united states|    1|\n",
      "|     AUSTRALIA|    1|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    groupBy(\"nationality\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f4d7af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(\"employee_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5847124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8406c117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(col(\"first_name\"), col(\"last_name\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d334d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75ab696b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n",
      "|upper(first_name)|upper(last_name)|\n",
      "+-----------------+----------------+\n",
      "|            SCOTT|           TIGER|\n",
      "|            HENRY|            FORD|\n",
      "|             NICK|          JUNIOR|\n",
      "|             BILL|           GOMES|\n",
      "+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(upper(\"first_name\"), upper(\"last_name\") ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71ad6bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n",
      "|upper(first_name)|upper(last_name)|\n",
      "+-----------------+----------------+\n",
      "|            SCOTT|           TIGER|\n",
      "|            HENRY|            FORD|\n",
      "|             NICK|          JUNIOR|\n",
      "|             BILL|           GOMES|\n",
      "+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(upper(col(\"first_name\")), upper(col(\"last_name\"))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c89a627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|upper(nationality)|count|\n",
      "+------------------+-----+\n",
      "|    UNITED KINGDOM|    1|\n",
      "|             INDIA|    1|\n",
      "|         AUSTRALIA|    1|\n",
      "|     UNITED STATES|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    groupBy(upper(col(\"nationality\"))). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3104e708",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'desc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-80ceecfbbea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0memployeesDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"employee_id\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'desc'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "employeesDF. \\\n",
    "    orderBy(\"employee_id\".desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cababcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(col(\"employee_id\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d00238fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(col(\"first_name\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c85118e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(employeesDF['first_name'].alias('first_name')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b17efc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(employeesDF['first_name'].alias('first_name').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc8b4e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(upper(employeesDF['first_name']).alias('first_name').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4550bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(\"*\"). \\\n",
    "    orderBy(upper(employeesDF['first_name']).alias('first_name').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42962a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+\n",
      "|employee_id|first_name|last_name|\n",
      "+-----------+----------+---------+\n",
      "|          1|     Scott|    Tiger|\n",
      "|          3|      Nick|   Junior|\n",
      "|          2|     Henry|     Ford|\n",
      "|          4|      Bill|    Gomes|\n",
      "+-----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(\"employee_id\",\"first_name\", \"last_name\"). \\\n",
    "    orderBy(upper(employeesDF['first_name']).alias('first_name').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "615b6a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef0a5eda",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`, `' given input columns: [employee_id, first_name, last_name, nationality, phone, salary, ssn];\n'Project [concat(first_name#1, ', , last_name#2) AS concat(first_name, , , last_name)#282]\n+- LogicalRDD [employee_id#0, first_name#1, last_name#2, salary#3, nationality#4, phone#5, ssn#6], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c130c2112055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memployeesDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first_name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"last_name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \"\"\"\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`, `' given input columns: [employee_id, first_name, last_name, nationality, phone, salary, ssn];\n'Project [concat(first_name#1, ', , last_name#2) AS concat(first_name, , , last_name)#282]\n+- LogicalRDD [employee_id#0, first_name#1, last_name#2, salary#3, nationality#4, phone#5, ssn#6], false\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(concat(col(\"first_name\"), \", \", col(\"last_name\"))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0e363e4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`, `' given input columns: [employee_id, first_name, last_name, nationality, phone, salary, ssn];\n'Project [concat(first_name#1, ', , last_name#2) AS concat(first_name, , , last_name)#290]\n+- LogicalRDD [employee_id#0, first_name#1, last_name#2, salary#3, nationality#4, phone#5, ssn#6], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-29771f2c0274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memployeesDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memployeesDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"first_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memployeesDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"last_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \"\"\"\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`, `' given input columns: [employee_id, first_name, last_name, nationality, phone, salary, ssn];\n'Project [concat(first_name#1, ', , last_name#2) AS concat(first_name, , , last_name)#290]\n+- LogicalRDD [employee_id#0, first_name#1, last_name#2, salary#3, nationality#4, phone#5, ssn#6], false\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(concat(employeesDF[\"first_name\"], \", \", employeesDF[\"last_name\"])). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7774a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5524bd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|concat(first_name, , , last_name)|\n",
      "+---------------------------------+\n",
      "|                     Scott, Tiger|\n",
      "|                      Henry, Ford|\n",
      "|                     Nick, Junior|\n",
      "|                      Bill, Gomes|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(concat(col(\"first_name\"), lit(\", \"), col(\"last_name\"))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc5776b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   full_name|\n",
      "+------------+\n",
      "|Scott, Tiger|\n",
      "| Henry, Ford|\n",
      "|Nick, Junior|\n",
      "| Bill, Gomes|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "employeesDF. \\\n",
    "    select(concat(col(\"first_name\"), lit(\", \"), col(\"last_name\")).alias(\"full_name\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7116f70",
   "metadata": {},
   "source": [
    "### 170 Common string Manipulation Functions for DataFrame columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e51c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5140e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [\n",
    "    (1, \"Scott\", \"Tiger\", 1000.0, \n",
    "      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "    ),\n",
    "     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "     ),\n",
    "     (3, \"Nick\", \"Junior\", 750.0, \n",
    "      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "     ),\n",
    "     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "     )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667ef6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.createDataFrame(employees,\n",
    "            schema=\"\"\"employee_id INT, first_name STRING, last_name STRING,\n",
    "            salary FLOAT, nationality STRING,\n",
    "            phone STRING, ssn STRING \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "324972a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn| full_name|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|ScottTiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123| HenryFord|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|NickJunior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118| BillGomes|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "employeesDF. \\\n",
    "    withColumn(\"full_name\", concat(\"first_name\", \"last_name\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "021747ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|   full_name|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|Scott, Tiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123| Henry, Ford|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|Nick, Junior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118| Bill, Gomes|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "employeesDF. \\\n",
    "    withColumn(\"full_name\", concat(\"first_name\",lit(\", \"), \"last_name\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f3bf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, upper, initcap, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "015c28d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "|employee_id|   nationality|nationality_upper|nationality_lower|nationality_initcap|nationality_length|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "|          1| united states|    UNITED STATES|    united states|      United States|                13|\n",
      "|          2|         India|            INDIA|            india|              India|                 5|\n",
      "|          3|united KINGDOM|   UNITED KINGDOM|   united kingdom|     United Kingdom|                14|\n",
      "|          4|     AUSTRALIA|        AUSTRALIA|        australia|          Australia|                 9|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(\"employee_id\", \"nationality\"). \\\n",
    "    withColumn(\"nationality_upper\",upper(col(\"nationality\"))). \\\n",
    "    withColumn(\"nationality_lower\",lower(col(\"nationality\"))). \\\n",
    "    withColumn(\"nationality_initcap\",initcap(col(\"nationality\"))). \\\n",
    "    withColumn(\"nationality_length\",length(col(\"nationality\"))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354150b",
   "metadata": {},
   "source": [
    "### 171 Extracting String using substring from Spark DataFrame columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0312e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Hello World\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "382a7d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6964164b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ell'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "647af50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c73e0777",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l, \"dummy STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc327601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "836dd4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|substring(Hello World, 7, 5)|\n",
      "+----------------------------+\n",
      "|                       World|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(substring(lit(\"Hello World\"), 7, 5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21210e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|substring(Hello World, -5, 5)|\n",
      "+-----------------------------+\n",
      "|                        World|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(substring(lit(\"Hello World\"), -5, 5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6368d68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th><th>phone</th><th>ssn</th></tr>\n",
       "<tr><td>1</td><td>Scott</td><td>Tiger</td><td>1000.0</td><td>united states</td><td>+1 123 456 7890</td><td>123 45 6789</td></tr>\n",
       "<tr><td>2</td><td>Henry</td><td>Ford</td><td>1250.0</td><td>India</td><td>+91 234 567 8901</td><td>456 78 9123</td></tr>\n",
       "<tr><td>3</td><td>Nick</td><td>Junior</td><td>750.0</td><td>united KINGDOM</td><td>+44 111 111 1111</td><td>222 33 4444</td></tr>\n",
       "<tr><td>4</td><td>Bill</td><td>Gomes</td><td>1500.0</td><td>AUSTRALIA</td><td>+61 987 654 3210</td><td>789 12 6118</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
       "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
       "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
       "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
       "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
       "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
       "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
       "+-----------+----------+---------+------+--------------+----------------+-----------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "023a0bcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cast'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6140225a24a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'cast'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring, lit, cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1a17046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|employee_id|           phone|        ssn|phone_last4|ssn_last4|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|          1| +1 123 456 7890|123 45 6789|       7890|     6789|\n",
      "|          2|+91 234 567 8901|456 78 9123|       8901|     9123|\n",
      "|          3|+44 111 111 1111|222 33 4444|       1111|     4444|\n",
      "|          4|+61 987 654 3210|789 12 6118|       3210|     6118|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(\"employee_id\",\"phone\",\"ssn\"). \\\n",
    "    withColumn(\"phone_last4\", substring(col(\"phone\"),-4,4).cast(\"int\")). \\\n",
    "    withColumn(\"ssn_last4\", substring(col(\"ssn\"),8,4).cast(\"int\")). \\\n",
    "    show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3acc5322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      " |-- phone_last4: integer (nullable = true)\n",
      " |-- ssn_last4: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(\"employee_id\",\"phone\",\"ssn\"). \\\n",
    "    withColumn(\"phone_last4\", substring(col(\"phone\"),-4,4).cast(\"int\")). \\\n",
    "    withColumn(\"ssn_last4\", substring(col(\"ssn\"),8,4).cast(\"int\")). \\\n",
    "    printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb08f0f",
   "metadata": {},
   "source": [
    "### 172 Extracting String using split from Spark DataFrame columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a4891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout',6000). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3824465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be11b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l,\"dummy STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d197d7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f0fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97aa4c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|split(Hello World, how are you,  , -1)|\n",
      "+--------------------------------------+\n",
      "|[Hello, World,, how, are, you]        |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(lit(\"Hello World, how are you\"), \" \")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a1a5b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|split(Hello World, how are you,  , -1)[2]|\n",
      "+-----------------------------------------+\n",
      "|how                                      |\n",
      "+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(lit(\"Hello World, how are you\"), \" \")[2]). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09c2b2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|col   |\n",
      "+------+\n",
      "|Hello |\n",
      "|World,|\n",
      "|how   |\n",
      "|are   |\n",
      "|you   |\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(explode(split(lit(\"Hello World, how are you\"), \" \"))). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e3eaa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|word  |\n",
      "+------+\n",
      "|Hello |\n",
      "|World,|\n",
      "|how   |\n",
      "|are   |\n",
      "|you   |\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(explode(split(lit(\"Hello World, how are you\"), \" \")).alias(\"word\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d39c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890,+1 234 567 8901\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111,+44 222 222 2222\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210,+61 876 543 2109\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab670f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.createDataFrame(employees,\n",
    "                schema=\"\"\"employee_id INT, first_name STRING, last_name STRING, \n",
    "                salary FLOAT, nationality STRING, phone_numbers STRING, ssn STRING\n",
    "                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a87785fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+---------------------------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|nationality   |phone_numbers                    |ssn        |\n",
      "+-----------+----------+---------+------+--------------+---------------------------------+-----------+\n",
      "|1          |Scott     |Tiger    |1000.0|united states |+1 123 456 7890,+1 234 567 8901  |123 45 6789|\n",
      "|2          |Henry     |Ford     |1250.0|India         |+91 234 567 8901                 |456 78 9123|\n",
      "|3          |Nick      |Junior   |750.0 |united KINGDOM|+44 111 111 1111,+44 222 222 2222|222 33 4444|\n",
      "|4          |Bill      |Gomes    |1500.0|AUSTRALIA     |+61 987 654 3210,+61 876 543 2109|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+---------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a1c4eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------------------+-----------+\n",
      "|first_name|last_name|phone_numbers                    |ssn        |\n",
      "+----------+---------+---------------------------------+-----------+\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|\n",
      "|Henry     |Ford     |+91 234 567 8901                 |456 78 9123|\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|\n",
      "+----------+---------+---------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8858fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------------------+-----------+------------------------------------+\n",
      "|first_name|last_name|phone_numbers                    |ssn        |phone_number                        |\n",
      "+----------+---------+---------------------------------+-----------+------------------------------------+\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|[+1 123 456 7890, +1 234 567 8901]  |\n",
      "|Henry     |Ford     |+91 234 567 8901                 |456 78 9123|[+91 234 567 8901]                  |\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|[+44 111 111 1111, +44 222 222 2222]|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|[+61 987 654 3210, +61 876 543 2109]|\n",
      "+----------+---------+---------------------------------+-----------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn'). \\\n",
    "    withColumn('phone_number',split('phone_numbers',\",\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0901794d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------------------+-----------+------------------------------------+\n",
      "|first_name|last_name|phone_numbers                    |ssn        |phone_number                        |\n",
      "+----------+---------+---------------------------------+-----------+------------------------------------+\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|[+1 123 456 7890, +1 234 567 8901]  |\n",
      "|Henry     |Ford     |+91 234 567 8901                 |456 78 9123|[+91 234 567 8901]                  |\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|[+44 111 111 1111, +44 222 222 2222]|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|[+61 987 654 3210, +61 876 543 2109]|\n",
      "+----------+---------+---------------------------------+-----------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn',split('phone_numbers',\",\").alias('phone_number')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bfc51e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "|first_name|last_name|phone_numbers                    |ssn        |phone_number    |\n",
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 123 456 7890 |\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 234 567 8901 |\n",
      "|Henry     |Ford     |+91 234 567 8901                 |456 78 9123|+91 234 567 8901|\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 111 111 1111|\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 222 222 2222|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 987 654 3210|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 876 543 2109|\n",
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn'). \\\n",
    "    withColumn('phone_number',explode(split('phone_numbers',\",\"))). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16e0f96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "|first_name|last_name|phone_numbers                    |ssn        |phone_number    |\n",
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 123 456 7890 |\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 234 567 8901 |\n",
      "|Henry     |Ford     |+91 234 567 8901                 |456 78 9123|+91 234 567 8901|\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 111 111 1111|\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 222 222 2222|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 987 654 3210|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 876 543 2109|\n",
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn',explode(split('phone_numbers',\",\")).alias('phone_number')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f17489ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Generators are not supported when it's nested in expressions, but got: split(explode(split(phone_numbers, ,, -1)),  , -1)[1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2814e9c2e271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memployeesDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'first_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'last_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'phone_numbers'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ssn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'phone_number'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'phone_numbers'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'area_code'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'phone_numbers'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'phone_last4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'phone_numbers'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ssn_last4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ssn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \"\"\"\n\u001b[1;32m   2454\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Generators are not supported when it's nested in expressions, but got: split(explode(split(phone_numbers, ,, -1)),  , -1)[1]"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn'). \\\n",
    "    withColumn('phone_number',explode(split('phone_numbers',\",\"))). \\\n",
    "    withColumn('area_code',split(explode(split('phone_numbers',\",\")),\" \")[1]). \\\n",
    "    withColumn('phone_last4',split(explode(split('phone_numbers',\",\")),\" \")[3]). \\\n",
    "    withColumn('ssn_last4',split('ssn',\" \")[3]). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de62a6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------------------+-----------+----------------+---------+-----------+---------+\n",
      "|first_name|last_name|phone_numbers                    |ssn        |phone_number    |area_code|phone_last4|ssn_last4|\n",
      "+----------+---------+---------------------------------+-----------+----------------+---------+-----------+---------+\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 123 456 7890 |123      |7890       |null     |\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 234 567 8901 |234      |8901       |null     |\n",
      "|Henry     |Ford     |+91 234 567 8901                 |456 78 9123|+91 234 567 8901|234      |8901       |null     |\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 111 111 1111|111      |1111       |null     |\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 222 222 2222|222      |2222       |null     |\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 987 654 3210|987      |3210       |null     |\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 876 543 2109|876      |2109       |null     |\n",
      "+----------+---------+---------------------------------+-----------+----------------+---------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn'). \\\n",
    "    withColumn('phone_number',explode(split('phone_numbers',\",\"))). \\\n",
    "    select('first_name','last_name', 'phone_numbers', 'ssn','phone_number').\\\n",
    "    withColumn('area_code',split('phone_number',\" \")[1]). \\\n",
    "    withColumn('phone_last4', split('phone_number', \" \")[3]). \\\n",
    "    withColumn('ssn_last4', split('ssn',\" \")[3]). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14168837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>first_name</th><th>last_name</th><th>phone_numbers</th><th>ssn</th><th>phone_number</th><th>area_code</th><th>phone_last4</th><th>ssn_last4</th></tr>\n",
       "<tr><td>Scott</td><td>Tiger</td><td>+1 123 456 7890,+...</td><td>123 45 6789</td><td>+1 123 456 7890</td><td>123</td><td>7890</td><td>6789</td></tr>\n",
       "<tr><td>Scott</td><td>Tiger</td><td>+1 123 456 7890,+...</td><td>123 45 6789</td><td>+1 234 567 8901</td><td>234</td><td>8901</td><td>6789</td></tr>\n",
       "<tr><td>Henry</td><td>Ford</td><td>+91 234 567 8901</td><td>456 78 9123</td><td>+91 234 567 8901</td><td>234</td><td>8901</td><td>9123</td></tr>\n",
       "<tr><td>Nick</td><td>Junior</td><td>+44 111 111 1111,...</td><td>222 33 4444</td><td>+44 111 111 1111</td><td>111</td><td>1111</td><td>4444</td></tr>\n",
       "<tr><td>Nick</td><td>Junior</td><td>+44 111 111 1111,...</td><td>222 33 4444</td><td>+44 222 222 2222</td><td>222</td><td>2222</td><td>4444</td></tr>\n",
       "<tr><td>Bill</td><td>Gomes</td><td>+61 987 654 3210,...</td><td>789 12 6118</td><td>+61 987 654 3210</td><td>987</td><td>3210</td><td>6118</td></tr>\n",
       "<tr><td>Bill</td><td>Gomes</td><td>+61 987 654 3210,...</td><td>789 12 6118</td><td>+61 876 543 2109</td><td>876</td><td>2109</td><td>6118</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+---------+--------------------+-----------+----------------+---------+-----------+---------+\n",
       "|first_name|last_name|       phone_numbers|        ssn|    phone_number|area_code|phone_last4|ssn_last4|\n",
       "+----------+---------+--------------------+-----------+----------------+---------+-----------+---------+\n",
       "|     Scott|    Tiger|+1 123 456 7890,+...|123 45 6789| +1 123 456 7890|      123|       7890|     6789|\n",
       "|     Scott|    Tiger|+1 123 456 7890,+...|123 45 6789| +1 234 567 8901|      234|       8901|     6789|\n",
       "|     Henry|     Ford|    +91 234 567 8901|456 78 9123|+91 234 567 8901|      234|       8901|     9123|\n",
       "|      Nick|   Junior|+44 111 111 1111,...|222 33 4444|+44 111 111 1111|      111|       1111|     4444|\n",
       "|      Nick|   Junior|+44 111 111 1111,...|222 33 4444|+44 222 222 2222|      222|       2222|     4444|\n",
       "|      Bill|    Gomes|+61 987 654 3210,...|789 12 6118|+61 987 654 3210|      987|       3210|     6118|\n",
       "|      Bill|    Gomes|+61 987 654 3210,...|789 12 6118|+61 876 543 2109|      876|       2109|     6118|\n",
       "+----------+---------+--------------------+-----------+----------------+---------+-----------+---------+"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn'). \\\n",
    "    withColumn('phone_number',explode(split('phone_numbers',\",\"))). \\\n",
    "    select('first_name','last_name', 'phone_numbers', 'ssn','phone_number').\\\n",
    "    withColumn('area_code',split('phone_number',\" \")[1].cast(\"int\")). \\\n",
    "    withColumn('phone_last4', split('phone_number', \" \")[3].cast(\"int\")). \\\n",
    "    withColumn('ssn_last4', split('ssn',\" \")[2].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dd7b48d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = employeesDF.select('first_name', 'last_name','phone_numbers','ssn'). \\\n",
    "                withColumn('phone_number',explode(split('phone_numbers',\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6d5b77b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "|first_name|last_name|phone_numbers                    |ssn        |phone_number    |\n",
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 123 456 7890 |\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 234 567 8901 |\n",
      "|Henry     |Ford     |+91 234 567 8901                 |456 78 9123|+91 234 567 8901|\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 111 111 1111|\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 222 222 2222|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 987 654 3210|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 876 543 2109|\n",
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "57025e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------------+-----------+---------+-----------+---------+\n",
      "|first_name|last_name|    phone_number|        ssn|area_code|phone_last4|ssn_last4|\n",
      "+----------+---------+----------------+-----------+---------+-----------+---------+\n",
      "|     Scott|    Tiger| +1 123 456 7890|123 45 6789|      123|       7890|     6789|\n",
      "|     Scott|    Tiger| +1 234 567 8901|123 45 6789|      234|       8901|     6789|\n",
      "|     Henry|     Ford|+91 234 567 8901|456 78 9123|      234|       8901|     9123|\n",
      "|      Nick|   Junior|+44 111 111 1111|222 33 4444|      111|       1111|     4444|\n",
      "|      Nick|   Junior|+44 222 222 2222|222 33 4444|      222|       2222|     4444|\n",
      "|      Bill|    Gomes|+61 987 654 3210|789 12 6118|      987|       3210|     6118|\n",
      "|      Bill|    Gomes|+61 876 543 2109|789 12 6118|      876|       2109|     6118|\n",
      "+----------+---------+----------------+-----------+---------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name','last_name','phone_number','ssn'). \\\n",
    "    withColumn('area_code',split('phone_number',\" \")[1].cast(\"int\")). \\\n",
    "    withColumn('phone_last4',split('phone_number',\" \")[3].cast(\"int\")). \\\n",
    "    withColumn('ssn_last4',split('ssn',\" \")[2].cast(\"int\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcc02ef0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'driver'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-aa29dc8ab574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'driver'"
     ]
    }
   ],
   "source": [
    "spark.driver.port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f5325a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>first_name</th><th>last_name</th><th>phone_numbers</th><th>ssn</th><th>phone_number</th></tr>\n",
       "<tr><td>Scott</td><td>Tiger</td><td>+1 123 456 7890,+...</td><td>123 45 6789</td><td>+1 123 456 7890</td></tr>\n",
       "<tr><td>Scott</td><td>Tiger</td><td>+1 123 456 7890,+...</td><td>123 45 6789</td><td>+1 234 567 8901</td></tr>\n",
       "<tr><td>Henry</td><td>Ford</td><td>+91 234 567 8901</td><td>456 78 9123</td><td>+91 234 567 8901</td></tr>\n",
       "<tr><td>Nick</td><td>Junior</td><td>+44 111 111 1111,...</td><td>222 33 4444</td><td>+44 111 111 1111</td></tr>\n",
       "<tr><td>Nick</td><td>Junior</td><td>+44 111 111 1111,...</td><td>222 33 4444</td><td>+44 222 222 2222</td></tr>\n",
       "<tr><td>Bill</td><td>Gomes</td><td>+61 987 654 3210,...</td><td>789 12 6118</td><td>+61 987 654 3210</td></tr>\n",
       "<tr><td>Bill</td><td>Gomes</td><td>+61 987 654 3210,...</td><td>789 12 6118</td><td>+61 876 543 2109</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+---------+--------------------+-----------+----------------+\n",
       "|first_name|last_name|       phone_numbers|        ssn|    phone_number|\n",
       "+----------+---------+--------------------+-----------+----------------+\n",
       "|     Scott|    Tiger|+1 123 456 7890,+...|123 45 6789| +1 123 456 7890|\n",
       "|     Scott|    Tiger|+1 123 456 7890,+...|123 45 6789| +1 234 567 8901|\n",
       "|     Henry|     Ford|    +91 234 567 8901|456 78 9123|+91 234 567 8901|\n",
       "|      Nick|   Junior|+44 111 111 1111,...|222 33 4444|+44 111 111 1111|\n",
       "|      Nick|   Junior|+44 111 111 1111,...|222 33 4444|+44 222 222 2222|\n",
       "|      Bill|    Gomes|+61 987 654 3210,...|789 12 6118|+61 987 654 3210|\n",
       "|      Bill|    Gomes|+61 987 654 3210,...|789 12 6118|+61 876 543 2109|\n",
       "+----------+---------+--------------------+-----------+----------------+"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb43a703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|first_name|last_name|count|\n",
      "+----------+---------+-----+\n",
      "|      Nick|   Junior|    2|\n",
      "|     Henry|     Ford|    1|\n",
      "|      Bill|    Gomes|    2|\n",
      "|     Scott|    Tiger|    2|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.groupBy('first_name','last_name'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d154c43",
   "metadata": {},
   "source": [
    "### 173 Padding characters around Strings in Spark DataFrame Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb162227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout',6000). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5f93026",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b76601f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l,'dummy STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b2ec6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "372836ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5d6a17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Hello World!</th></tr>\n",
       "<tr><td>Hello World!</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+\n",
       "|Hello World!|\n",
       "+------------+\n",
       "|Hello World!|\n",
       "+------------+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(lit(\"Hello World!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "355ab752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>Hello World!</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+\n",
       "|       dummy|\n",
       "+------------+\n",
       "|Hello World!|\n",
       "+------------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(lit(\"Hello World!\").alias(\"dummy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "798020cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lpad, rpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3042d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     dummy|\n",
      "+----------+\n",
      "|-----Hello|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lpad(lit(\"Hello\"), 10, \"-\").alias(\"dummy\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4fcbd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "924621d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.createDataFrame(employees). \\\n",
    "    toDF(\"employee_id\", \"first_name\",\n",
    "        \"last_name\", \"salary\",\n",
    "        \"nationality\", \"phone_number\",\n",
    "        \"ssn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1d1b1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8a19a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>summary</th><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th><th>phone_number</th><th>ssn</th></tr>\n",
       "<tr><td>count</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td></tr>\n",
       "<tr><td>mean</td><td>2.5</td><td>null</td><td>null</td><td>1125.0</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>stddev</td><td>1.2909944487358056</td><td>null</td><td>null</td><td>322.7486121839514</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>min</td><td>1</td><td>Bill</td><td>Ford</td><td>750.0</td><td>AUSTRALIA</td><td>+1 123 456 7890</td><td>123 45 6789</td></tr>\n",
       "<tr><td>max</td><td>4</td><td>Scott</td><td>Tiger</td><td>1500.0</td><td>united states</td><td>+91 234 567 8901</td><td>789 12 6118</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+------------------+----------+---------+-----------------+-------------+----------------+-----------+\n",
       "|summary|       employee_id|first_name|last_name|           salary|  nationality|    phone_number|        ssn|\n",
       "+-------+------------------+----------+---------+-----------------+-------------+----------------+-----------+\n",
       "|  count|                 4|         4|        4|                4|            4|               4|          4|\n",
       "|   mean|               2.5|      null|     null|           1125.0|         null|            null|       null|\n",
       "| stddev|1.2909944487358056|      null|     null|322.7486121839514|         null|            null|       null|\n",
       "|    min|                 1|      Bill|     Ford|            750.0|    AUSTRALIA| +1 123 456 7890|123 45 6789|\n",
       "|    max|                 4|     Scott|    Tiger|           1500.0|united states|+91 234 567 8901|789 12 6118|\n",
       "+-------+------------------+----------+---------+-----------------+-------------+----------------+-----------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a5800b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th><th>phone_number</th><th>ssn</th></tr>\n",
       "<tr><td>1</td><td>Scott</td><td>Tiger</td><td>1000.0</td><td>united states</td><td>+1 123 456 7890</td><td>123 45 6789</td></tr>\n",
       "<tr><td>2</td><td>Henry</td><td>Ford</td><td>1250.0</td><td>India</td><td>+91 234 567 8901</td><td>456 78 9123</td></tr>\n",
       "<tr><td>3</td><td>Nick</td><td>Junior</td><td>750.0</td><td>united KINGDOM</td><td>+44 111 111 1111</td><td>222 33 4444</td></tr>\n",
       "<tr><td>4</td><td>Bill</td><td>Gomes</td><td>1500.0</td><td>AUSTRALIA</td><td>+61 987 654 3210</td><td>789 12 6118</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[employee_id: bigint, first_name: string, last_name: string, salary: double, nationality: string, phone_number: string, ssn: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca60c86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['employee_id',\n",
       " 'first_name',\n",
       " 'last_name',\n",
       " 'salary',\n",
       " 'nationality',\n",
       " 'phone_number',\n",
       " 'ssn']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ac54e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|lpad(employee_id, 5, 0)|\n",
      "+-----------------------+\n",
      "|                  00001|\n",
      "|                  00002|\n",
      "|                  00003|\n",
      "|                  00004|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(lpad(\"employee_id\",5,\"0\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6654c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>lpad(employee_id, 5, 0)</th><th>rpad(first_name, 10, -)</th><th>rpad(last_name, 10, -)</th><th>lpad(salary, 10, 0)</th><th>rpad(nationality, 15, -)</th><th>rpad(phone_number, 17, -)</th><th>ssn</th></tr>\n",
       "<tr><td>00001</td><td>Scott-----</td><td>Tiger-----</td><td>00001000.0</td><td>united states--</td><td>+1 123 456 7890--</td><td>123 45 6789</td></tr>\n",
       "<tr><td>00002</td><td>Henry-----</td><td>Ford------</td><td>00001250.0</td><td>India----------</td><td>+91 234 567 8901-</td><td>456 78 9123</td></tr>\n",
       "<tr><td>00003</td><td>Nick------</td><td>Junior----</td><td>00000750.0</td><td>united KINGDOM-</td><td>+44 111 111 1111-</td><td>222 33 4444</td></tr>\n",
       "<tr><td>00004</td><td>Bill------</td><td>Gomes-----</td><td>00001500.0</td><td>AUSTRALIA------</td><td>+61 987 654 3210-</td><td>789 12 6118</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------------+-----------------------+----------------------+-------------------+------------------------+-------------------------+-----------+\n",
       "|lpad(employee_id, 5, 0)|rpad(first_name, 10, -)|rpad(last_name, 10, -)|lpad(salary, 10, 0)|rpad(nationality, 15, -)|rpad(phone_number, 17, -)|        ssn|\n",
       "+-----------------------+-----------------------+----------------------+-------------------+------------------------+-------------------------+-----------+\n",
       "|                  00001|             Scott-----|            Tiger-----|         00001000.0|         united states--|        +1 123 456 7890--|123 45 6789|\n",
       "|                  00002|             Henry-----|            Ford------|         00001250.0|         India----------|        +91 234 567 8901-|456 78 9123|\n",
       "|                  00003|             Nick------|            Junior----|         00000750.0|         united KINGDOM-|        +44 111 111 1111-|222 33 4444|\n",
       "|                  00004|             Bill------|            Gomes-----|         00001500.0|         AUSTRALIA------|        +61 987 654 3210-|789 12 6118|\n",
       "+-----------------------+-----------------------+----------------------+-------------------+------------------------+-------------------------+-----------+"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.select(\n",
    "    lpad(\"employee_id\",5,\"0\"),\n",
    "    rpad(\"first_name\",10,\"-\"),\n",
    "    rpad(\"last_name\",10,\"-\"),\n",
    "    lpad(\"salary\",10,\"0\"),\n",
    "    rpad(\"nationality\",15,\"-\"),\n",
    "    rpad(\"phone_number\",17,\"-\"),\n",
    "    \"ssn\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3614989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d630b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "empFixedDF = employeesDF.select(\n",
    "    concat(\n",
    "    lpad(\"employee_id\",5,\"0\"),\n",
    "    rpad(\"first_name\",10,\"-\"),\n",
    "    rpad(\"last_name\",10,\"-\"),\n",
    "    lpad(\"salary\",10,\"0\"),\n",
    "    rpad(\"nationality\",15,\"-\"),\n",
    "    rpad(\"phone_number\",17,\"-\"),\n",
    "    \"ssn\"\n",
    "    ).alias(\"employees\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ab9336a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "|employees                                                                     |\n",
      "+------------------------------------------------------------------------------+\n",
      "|00001Scott-----Tiger-----00001000.0united states--+1 123 456 7890--123 45 6789|\n",
      "|00002Henry-----Ford------00001250.0India----------+91 234 567 8901-456 78 9123|\n",
      "|00003Nick------Junior----00000750.0united KINGDOM-+44 111 111 1111-222 33 4444|\n",
      "|00004Bill------Gomes-----00001500.0AUSTRALIA------+61 987 654 3210-789 12 6118|\n",
      "+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empFixedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3371d152",
   "metadata": {},
   "source": [
    "### 174 Trimming characters from strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "373e73e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ltrim, rtrim, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b0c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout',6000). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d289169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [(\"    Hello.    \",)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9de57c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l).toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2958d068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ltrim, rtrim, trim, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6cf718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|         dummy|     ltrim|\n",
      "+--------------+----------+\n",
      "|    Hello.    |Hello.    |\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ltrim\",ltrim(\"dummy\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "697c2eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|         dummy|     ltrim|\n",
      "+--------------+----------+\n",
      "|    Hello.    |Hello.    |\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ltrim\",ltrim(col(\"dummy\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "433075eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------+------+\n",
      "|         dummy|     ltrim|     rtrim|  trim|\n",
      "+--------------+----------+----------+------+\n",
      "|    Hello.    |Hello.    |    Hello.|Hello.|\n",
      "+--------------+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ltrim\",ltrim(col(\"dummy\"))). \\\n",
    "    withColumn(\"rtrim\", rtrim(col(\"dummy\"))). \\\n",
    "    withColumn(\"trim\", trim(col(\"dummy\"))). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0e8c4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|function_desc                                                                |\n",
      "+-----------------------------------------------------------------------------+\n",
      "|Function: rtrim                                                              |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.StringTrimRight             |\n",
      "|Usage: \n",
      "    rtrim(str) - Removes the trailing space characters from `str`.\n",
      "  |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe function rtrim\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68f01769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|0000000000000_msdian|\n",
      "|0000000009874_retail|\n",
      "|          00000_2_db|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdb76f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a10db73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+------+\n",
      "|         dummy|     ltrim|    rtrim|  trim|\n",
      "+--------------+----------+---------+------+\n",
      "|    Hello.    |Hello.    |    Hello|Hello.|\n",
      "+--------------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ltrim\", expr(\"ltrim(dummy)\")). \\\n",
    "    withColumn(\"rtrim\", expr(\"rtrim('.',rtrim(dummy))\")). \\\n",
    "    withColumn(\"trim\", trim(col(\"dummy\"))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0308e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Function: trim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.StringTrim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Usage: \n",
      "    trim(str) - Removes the leading and trailing space characters from `str`.\n",
      "\n",
      "    trim(BOTH FROM str) - Removes the leading and trailing space characters from `str`.\n",
      "\n",
      "    trim(LEADING FROM str) - Removes the leading space characters from `str`.\n",
      "\n",
      "    trim(TRAILING FROM str) - Removes the trailing space characters from `str`.\n",
      "\n",
      "    trim(trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\n",
      "\n",
      "    trim(BOTH trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\n",
      "\n",
      "    trim(LEADING trimStr FROM str) - Remove the leading `trimStr` characters from `str`.\n",
      "\n",
      "    trim(TRAILING trimStr FROM str) - Remove the trailing `trimStr` characters from `str`.\n",
      "  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe function trim\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05edde20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+------+\n",
      "|dummy         |ltrim     |rtrim    |trim  |\n",
      "+--------------+----------+---------+------+\n",
      "|    Hello.    |Hello.    |    Hello|Hello.|\n",
      "+--------------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ltrim\",expr(\"trim(LEADING ' ' FROM dummy)\")). \\\n",
    "    withColumn(\"rtrim\",expr(\"trim(TRAILING '.' from rtrim(dummy))\")). \\\n",
    "    withColumn(\"trim\", expr(\"trim(BOTH ' ' FROM dummy)\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1168f",
   "metadata": {},
   "source": [
    "### 175 Date and Time Manipulation Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f07c2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout',6000). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e22ee24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe89c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l).toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71e8eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473c80c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e69a22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cde092f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|2024-02-09    |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "365c27ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|current_timestamp()    |\n",
      "+-----------------------+\n",
      "|2024-02-09 13:32:59.513|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_timestamp()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f949df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_date, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cb62532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|to_date   |\n",
      "+----------+\n",
      "|2024-02-09|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('20240209'), 'yyyyMMdd').alias('to_date')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acafb6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|to_date   |\n",
      "+----------+\n",
      "|2024-02-09|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('20240209 1725'), 'yyyyMMdd HHmm').alias('to_date')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deab4997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|to_timestamp       |\n",
      "+-------------------+\n",
      "|2024-02-09 17:25:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('20240209 1725'), 'yyyyMMdd HHmm').alias('to_timestamp')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f31648",
   "metadata": {},
   "source": [
    "### 176 Date and Time ARithmetic on Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1661c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port',0). \\\n",
    "    config('spark.sql.warehouse.dir',f\"/user/{username}/warehouse\"). \\\n",
    "    config('spark.shuffle.io.connectionTimeOut',6000). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f\"{username} | Pyspark Processing column Data\"). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5380eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b023877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes,schema = \"date STRING, time STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb010a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91227975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da58df1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_add in module pyspark.sql.functions:\n",
      "\n",
      "date_add(start, days)\n",
      "    Returns the date that is `days` days after `start`\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
      "    [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24658dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_sub in module pyspark.sql.functions:\n",
      "\n",
      "date_sub(start, days)\n",
      "    Returns the date that is `days` days before `start`\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_sub(df.dt, 1).alias('prev_date')).collect()\n",
      "    [Row(prev_date=datetime.date(2015, 4, 7))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79cad5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+-------------+-------------+-------------+\n",
      "|date      |time                   |date_add_date|time_add_date|date_sub_date|time_sub_date|\n",
      "+----------+-----------------------+-------------+-------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-03-10   |2014-03-10   |2014-02-18   |2014-02-18   |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-03-10   |2016-03-10   |2016-02-19   |2016-02-19   |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-11-10   |2018-01-10   |2017-10-21   |2017-12-21   |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-12-10   |2019-09-10   |2019-11-20   |2019-08-21   |\n",
      "+----------+-----------------------+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"date_add_date\",date_add(\"date\",10)). \\\n",
    "    withColumn(\"time_add_date\", date_add(\"time\",10)). \\\n",
    "    withColumn(\"date_sub_date\",date_sub(\"date\",10)). \\\n",
    "    withColumn(\"time_sub_date\", date_sub(\"time\",10)). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64314bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date,current_timestamp, datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc0d0339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+-------------+\n",
      "|date      |time                   |datediff_date|datediff_time|\n",
      "+----------+-----------------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|3634         |3634         |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2903         |2903         |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2293         |2232         |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|1533         |1624         |\n",
      "+----------+-----------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"datediff_date\",datediff(current_date(),\"date\")). \\\n",
    "    withColumn(\"datediff_time\",datediff(current_timestamp(), \"time\")). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf3ff499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import months_between, add_months, round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54e94338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function months_between in module pyspark.sql.functions:\n",
      "\n",
      "months_between(date1, date2, roundOff=True)\n",
      "    Returns number of months between dates date1 and date2.\n",
      "    If date1 is later than date2, then the result is positive.\n",
      "    If date1 and date2 are on the same day of month, or both are the last day of month,\n",
      "    returns an integer (time of day will be ignored).\n",
      "    The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n",
      "    >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n",
      "    [Row(months=3.94959677)]\n",
      "    >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n",
      "    [Row(months=3.9495967741935485)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(months_between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b648f0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "|date      |time                   |months_between_date|months_between_time|add_months_date|add_months_time|\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|119.42             |119.41             |2014-05-28     |2014-05-28     |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|95.39              |95.38              |2016-05-29     |2016-05-29     |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|75.32              |73.31              |2018-01-31     |2018-03-31     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|50.35              |53.33              |2020-02-29     |2019-11-30     |\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"months_between_date\",round(months_between(current_date(),\"date\"),2)). \\\n",
    "    withColumn(\"months_between_time\", round(months_between(current_timestamp(),\"time\"),2)). \\\n",
    "    withColumn(\"add_months_date\",add_months(\"date\",3)). \\\n",
    "    withColumn(\"add_months_time\", add_months(\"time\",3)). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4d1db",
   "metadata": {},
   "source": [
    "### 177 Using Date and Time Trunc functions on spark Data Frame columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "864ea8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c4ec31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\",0). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/user/{username}/warehouse\"). \\\n",
    "    config(\"spark.shuffle.io.connectionTimeOut\",6000). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f\"{username} | Python Processing Column Data\"). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5e3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78644bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l).toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbf4d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50ab0635",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e246712",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes,\"date STRING, time STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9019d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c668fa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------+\n",
      "|date      |time                   |date_trunc|time_trunc|\n",
      "+----------+-----------------------+----------+----------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-01|2014-01-01|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-01|2016-01-01|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-01|2017-01-01|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-01|2019-01-01|\n",
      "+----------+-----------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"date_trunc\",trunc(\"date\",\"MM\")). \\\n",
    "    withColumn(\"time_trunc\", trunc(\"time\", \"yy\")). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07571b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a852f8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+\n",
      "|date      |time                   |date_trunc         |time_trunc         |\n",
      "+----------+-----------------------+-------------------+-------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-01 00:00:00|2014-01-01 00:00:00|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-01 00:00:00|2016-01-01 00:00:00|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-01 00:00:00|2017-01-01 00:00:00|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-01 00:00:00|2019-01-01 00:00:00|\n",
      "+----------+-----------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"date_trunc\",date_trunc(\"MM\",\"date\")).\\\n",
    "    withColumn(\"time_trunc\",date_trunc(\"yy\",\"time\")). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36be4ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+-------------------+\n",
      "|date      |time                   |date_dt            |time_dt            |time_dt1           |\n",
      "+----------+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-28 00:00:00|2014-02-28 10:00:00|2014-02-28 00:00:00|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-29 00:00:00|2016-02-29 08:00:00|2016-02-29 00:00:00|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-31 00:00:00|2017-12-31 11:00:00|2017-12-31 00:00:00|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-30 00:00:00|2019-08-31 00:00:00|2019-08-31 00:00:00|\n",
      "+----------+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"date_dt\",date_trunc(\"Hour\",\"date\")). \\\n",
    "    withColumn(\"time_dt\",date_trunc(\"Hour\",\"time\")). \\\n",
    "    withColumn(\"time_dt1\",date_trunc(\"dd\",\"time\")). \\\n",
    "show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4556470",
   "metadata": {},
   "source": [
    "###  Date and Time Extract functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a583985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3691b4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\",0). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/user/{username}/warehouse\"). \\\n",
    "    config(\"spark.shuffle.io.connectionTimeOut\",6000). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f\"{username} | Python Processing Data columns\"). \\\n",
    "    master(\"yarn\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739e30a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87c76e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g02.itversity.com:36319\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>itv011204 | Python Processing Data columns</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f88aec5c898>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a479eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e5f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l,schema=\"dummy STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca3020b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a74ac474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import weekofyear, dayofmonth, dayofweek, dayofyear, year, month\n",
    "\n",
    "from pyspark.sql.functions import current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b5abad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------------------+--------------------------------+------------+------------+-----------+---------+\n",
      "|current_date|year(current_date() AS `year`)|month(current_date() AS `month`)|week_of_year|day_of_month|day_of_year|dayofweek|\n",
      "+------------+------------------------------+--------------------------------+------------+------------+-----------+---------+\n",
      "|2024-02-10  |2024                          |2                               |6           |10          |41         |7        |\n",
      "+------------+------------------------------+--------------------------------+------------+------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date().alias(\"current_date\"),\n",
    "         year(current_date().alias(\"year\")),\n",
    "         month(current_date().alias(\"month\")),\n",
    "         weekofyear(current_date()).alias(\"week_of_year\"),\n",
    "         dayofmonth(current_date()).alias(\"day_of_month\"),\n",
    "         dayofyear(current_date()).alias(\"day_of_year\"),\n",
    "         dayofweek(current_date()).alias(\"dayofweek\")). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d58606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, hour, minute, second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9a54192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+------+------+\n",
      "|current_timestamp      |hour|minute|second|\n",
      "+-----------------------+----+------+------+\n",
      "|2024-02-10 05:20:50.786|5   |20    |50    |\n",
      "+-----------------------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_timestamp().alias(\"current_timestamp\"),\n",
    "         hour(current_timestamp()).alias(\"hour\"),\n",
    "         minute(current_timestamp()).alias(\"minute\"),\n",
    "         second(current_timestamp()).alias(\"second\")).\\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd01ed27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+-----+----------+----+------+------+\n",
      "|current_timestamp      |year|month|dayofmonth|hour|minute|second|\n",
      "+-----------------------+----+-----+----------+----+------+------+\n",
      "|2024-02-10 05:23:12.585|2024|2    |10        |5   |23    |12    |\n",
      "+-----------------------+----+-----+----------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_timestamp().alias(\"current_timestamp\"),\n",
    "         year(current_timestamp()).alias(\"year\"),\n",
    "         month(current_timestamp()).alias(\"month\"),\n",
    "         dayofmonth(current_timestamp()).alias(\"dayofmonth\"),\n",
    "         hour(current_timestamp()).alias(\"hour\"),\n",
    "         minute(current_timestamp()).alias(\"minute\"),\n",
    "         second(current_timestamp()).alias(\"second\")).\\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce71cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1143013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout',6000). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0831d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\",0). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/user/{username}/warehouse\"). \\\n",
    "    config(\"spark.shuffle.io.connectionTimeout\",6000). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f\"{username} | Python Processing Data columns\"). \\\n",
    "    master(\"yarn\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e2181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
