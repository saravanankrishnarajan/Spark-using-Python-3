{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3f359b",
   "metadata": {},
   "source": [
    "## Section 15 Pyspark Processing Column Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cb04db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25fab967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85154968",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders= spark.read.csv(\n",
    "    '/public/retail_db/orders',\n",
    "    schema='order_id INT, order_date STRING, order_customer_id INT, order_status STRING'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef051d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f0e4660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf45db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c609282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.select('*', date_format('order_date','yyyyMM').alias('order_month')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "796d3455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.withColumn('order_month', date_format('order_date','yyyyMM')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ca58307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|   25876|2014-01-01 00:00:...|             3414|PENDING_PAYMENT|\n",
      "|   25877|2014-01-01 00:00:...|             5549|PENDING_PAYMENT|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter\n",
    "\n",
    "orders.filter(date_format('order_date','yyyyMM')==201401).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3ea9b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|order_month|count|\n",
      "+-----------+-----+\n",
      "|     201401| 5908|\n",
      "|     201405| 5467|\n",
      "+-----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupBy\n",
    "\n",
    "orders.groupBy(date_format('order_date','yyyyMM').alias('order_month')). \\\n",
    "    count(). \\\n",
    "    show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32588515",
   "metadata": {},
   "source": [
    "### 167 Create Dummy Data Frame to explore Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11d00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b4a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l, \"dummy STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0cd14bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dummy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d8d6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "927c7b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2024-02-06|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "df.select(current_date()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "837e3666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2024-02-06|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date().alias(\"current_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ce075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [\n",
    "    (1, \"Scott\", \"Tiger\", 1000.0, \n",
    "      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "    ),\n",
    "     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "     ),\n",
    "     (3, \"Nick\", \"Junior\", 750.0, \n",
    "      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "     ),\n",
    "     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "     )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25771a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cab9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.createDataFrame(employees,\n",
    "            schema=\"\"\"employee_id INT, first_name STRING, last_name STRING,\n",
    "            salary FLOAT, nationality STRING,\n",
    "            phone STRING, ssn STRING \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "832c181e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6090c7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|nationality   |phone           |ssn        |\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|1          |Scott     |Tiger    |1000.0|united states |+1 123 456 7890 |123 45 6789|\n",
      "|2          |Henry     |Ford     |1250.0|India         |+91 234 567 8901|456 78 9123|\n",
      "|3          |Nick      |Junior   |750.0 |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|4          |Bill      |Gomes    |1500.0|AUSTRALIA     |+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373abd5",
   "metadata": {},
   "source": [
    "### Categories of Predefined functions used in Spark DataFrame columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1267f7de",
   "metadata": {},
   "source": [
    "There are approximately 300 functions under pyspark.sql.functions. At a higher level they can be grouped into a few categories.\n",
    "\n",
    "* String Manipulation Functions\n",
    "\n",
    "    * Case Conversion - lower, upper\n",
    "    * Getting Length - length\n",
    "    * Extracting substrings - substring, split\n",
    "    * Trimming - trim, ltrim, rtrim\n",
    "    * Padding - lpad, rpad\n",
    "    * Concatenating string - concat, concat_ws\n",
    "\n",
    "* Date Manipulation Functions\n",
    "\n",
    "    * Getting current date and time - current_date, current_timestamp\n",
    "    * Date Arithmetic - date_add, date_sub, datediff, months_between, add_months, next_day\n",
    "    * Beginning and Ending Date or Time - last_day, trunc, date_trunc\n",
    "    * Formatting Date - date_format\n",
    "    * Extracting Information - dayofyear, dayofmonth, dayofweek, year, month\n",
    "\n",
    "* Aggregate Functions\n",
    "\n",
    "    * count, countDistinct\n",
    "    * sum, avg\n",
    "    * min, max\n",
    "\n",
    "* Other Functions - We will explore depending on the use cases.\n",
    "\n",
    "    * CASE and WHEN\n",
    "    * CAST for type casting\n",
    "    * Functions to manage special types such as ARRAY, MAP, STRUCT type columns\n",
    "\n",
    "* Many others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fa51e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 169 Col and lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a5176cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [\n",
    "    (1, \"Scott\", \"Tiger\", 1000.0, \n",
    "      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "    ),\n",
    "     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "     ),\n",
    "     (3, \"Nick\", \"Junior\", 750.0, \n",
    "      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "     ),\n",
    "     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "     )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919ad469",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.createDataFrame(employees,\n",
    "            schema=\"\"\"employee_id INT, first_name STRING, last_name STRING,\n",
    "            salary FLOAT, nationality STRING,\n",
    "            phone STRING, ssn STRING \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59de72bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(\"first_name\", \"last_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f69959c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|   nationality|count|\n",
      "+--------------+-----+\n",
      "|         India|    1|\n",
      "|united KINGDOM|    1|\n",
      "| united states|    1|\n",
      "|     AUSTRALIA|    1|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    groupBy(\"nationality\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d4c133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(\"employee_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bff78f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e457045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(col(\"first_name\"), col(\"last_name\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd50ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c10bcb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n",
      "|upper(first_name)|upper(last_name)|\n",
      "+-----------------+----------------+\n",
      "|            SCOTT|           TIGER|\n",
      "|            HENRY|            FORD|\n",
      "|             NICK|          JUNIOR|\n",
      "|             BILL|           GOMES|\n",
      "+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(upper(\"first_name\"), upper(\"last_name\") ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a2772a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n",
      "|upper(first_name)|upper(last_name)|\n",
      "+-----------------+----------------+\n",
      "|            SCOTT|           TIGER|\n",
      "|            HENRY|            FORD|\n",
      "|             NICK|          JUNIOR|\n",
      "|             BILL|           GOMES|\n",
      "+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(upper(col(\"first_name\")), upper(col(\"last_name\"))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff2fcff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|upper(nationality)|count|\n",
      "+------------------+-----+\n",
      "|    UNITED KINGDOM|    1|\n",
      "|             INDIA|    1|\n",
      "|         AUSTRALIA|    1|\n",
      "|     UNITED STATES|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    groupBy(upper(col(\"nationality\"))). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99318976",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'desc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-80ceecfbbea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0memployeesDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"employee_id\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'desc'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "employeesDF. \\\n",
    "    orderBy(\"employee_id\".desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85c36e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(col(\"employee_id\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4de95da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(col(\"first_name\").desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2332dc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(employeesDF['first_name'].alias('first_name')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "258c6e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(employeesDF['first_name'].alias('first_name').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f316602c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(upper(employeesDF['first_name']).alias('first_name').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3897c55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(\"*\"). \\\n",
    "    orderBy(upper(employeesDF['first_name']).alias('first_name').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b51d13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+\n",
      "|employee_id|first_name|last_name|\n",
      "+-----------+----------+---------+\n",
      "|          1|     Scott|    Tiger|\n",
      "|          3|      Nick|   Junior|\n",
      "|          2|     Henry|     Ford|\n",
      "|          4|      Bill|    Gomes|\n",
      "+-----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(\"employee_id\",\"first_name\", \"last_name\"). \\\n",
    "    orderBy(upper(employeesDF['first_name']).alias('first_name').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78c24b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d27b7ac5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`, `' given input columns: [employee_id, first_name, last_name, nationality, phone, salary, ssn];\n'Project [concat(first_name#1, ', , last_name#2) AS concat(first_name, , , last_name)#282]\n+- LogicalRDD [employee_id#0, first_name#1, last_name#2, salary#3, nationality#4, phone#5, ssn#6], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c130c2112055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memployeesDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first_name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"last_name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \"\"\"\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`, `' given input columns: [employee_id, first_name, last_name, nationality, phone, salary, ssn];\n'Project [concat(first_name#1, ', , last_name#2) AS concat(first_name, , , last_name)#282]\n+- LogicalRDD [employee_id#0, first_name#1, last_name#2, salary#3, nationality#4, phone#5, ssn#6], false\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(concat(col(\"first_name\"), \", \", col(\"last_name\"))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57453a2b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`, `' given input columns: [employee_id, first_name, last_name, nationality, phone, salary, ssn];\n'Project [concat(first_name#1, ', , last_name#2) AS concat(first_name, , , last_name)#290]\n+- LogicalRDD [employee_id#0, first_name#1, last_name#2, salary#3, nationality#4, phone#5, ssn#6], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-29771f2c0274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memployeesDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memployeesDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"first_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memployeesDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"last_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \"\"\"\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`, `' given input columns: [employee_id, first_name, last_name, nationality, phone, salary, ssn];\n'Project [concat(first_name#1, ', , last_name#2) AS concat(first_name, , , last_name)#290]\n+- LogicalRDD [employee_id#0, first_name#1, last_name#2, salary#3, nationality#4, phone#5, ssn#6], false\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(concat(employeesDF[\"first_name\"], \", \", employeesDF[\"last_name\"])). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eb5986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8394d674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|concat(first_name, , , last_name)|\n",
      "+---------------------------------+\n",
      "|                     Scott, Tiger|\n",
      "|                      Henry, Ford|\n",
      "|                     Nick, Junior|\n",
      "|                      Bill, Gomes|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(concat(col(\"first_name\"), lit(\", \"), col(\"last_name\"))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40811692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   full_name|\n",
      "+------------+\n",
      "|Scott, Tiger|\n",
      "| Henry, Ford|\n",
      "|Nick, Junior|\n",
      "| Bill, Gomes|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "employeesDF. \\\n",
    "    select(concat(col(\"first_name\"), lit(\", \"), col(\"last_name\")).alias(\"full_name\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756ebe3",
   "metadata": {},
   "source": [
    "### 170 Common string Manipulation Functions for DataFrame columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc8737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e48ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [\n",
    "    (1, \"Scott\", \"Tiger\", 1000.0, \n",
    "      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "    ),\n",
    "     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "     ),\n",
    "     (3, \"Nick\", \"Junior\", 750.0, \n",
    "      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "     ),\n",
    "     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "     )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc78c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.createDataFrame(employees,\n",
    "            schema=\"\"\"employee_id INT, first_name STRING, last_name STRING,\n",
    "            salary FLOAT, nationality STRING,\n",
    "            phone STRING, ssn STRING \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8163e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn| full_name|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|ScottTiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123| HenryFord|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|NickJunior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118| BillGomes|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "employeesDF. \\\n",
    "    withColumn(\"full_name\", concat(\"first_name\", \"last_name\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c3bffb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|   full_name|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|Scott, Tiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123| Henry, Ford|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|Nick, Junior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118| Bill, Gomes|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "employeesDF. \\\n",
    "    withColumn(\"full_name\", concat(\"first_name\",lit(\", \"), \"last_name\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d07cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, upper, initcap, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc50f1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "|employee_id|   nationality|nationality_upper|nationality_lower|nationality_initcap|nationality_length|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "|          1| united states|    UNITED STATES|    united states|      United States|                13|\n",
      "|          2|         India|            INDIA|            india|              India|                 5|\n",
      "|          3|united KINGDOM|   UNITED KINGDOM|   united kingdom|     United Kingdom|                14|\n",
      "|          4|     AUSTRALIA|        AUSTRALIA|        australia|          Australia|                 9|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(\"employee_id\", \"nationality\"). \\\n",
    "    withColumn(\"nationality_upper\",upper(col(\"nationality\"))). \\\n",
    "    withColumn(\"nationality_lower\",lower(col(\"nationality\"))). \\\n",
    "    withColumn(\"nationality_initcap\",initcap(col(\"nationality\"))). \\\n",
    "    withColumn(\"nationality_length\",length(col(\"nationality\"))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b4658",
   "metadata": {},
   "source": [
    "### 171 Extracting String using substring from Spark DataFrame columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b44a6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Hello World\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c146b234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c30ed1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ell'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a182c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c15eb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l, \"dummy STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7ec6fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e09efa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|substring(Hello World, 7, 5)|\n",
      "+----------------------------+\n",
      "|                       World|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(substring(lit(\"Hello World\"), 7, 5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fb3ed17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|substring(Hello World, -5, 5)|\n",
      "+-----------------------------+\n",
      "|                        World|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(substring(lit(\"Hello World\"), -5, 5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9337ae14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th><th>phone</th><th>ssn</th></tr>\n",
       "<tr><td>1</td><td>Scott</td><td>Tiger</td><td>1000.0</td><td>united states</td><td>+1 123 456 7890</td><td>123 45 6789</td></tr>\n",
       "<tr><td>2</td><td>Henry</td><td>Ford</td><td>1250.0</td><td>India</td><td>+91 234 567 8901</td><td>456 78 9123</td></tr>\n",
       "<tr><td>3</td><td>Nick</td><td>Junior</td><td>750.0</td><td>united KINGDOM</td><td>+44 111 111 1111</td><td>222 33 4444</td></tr>\n",
       "<tr><td>4</td><td>Bill</td><td>Gomes</td><td>1500.0</td><td>AUSTRALIA</td><td>+61 987 654 3210</td><td>789 12 6118</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
       "|employee_id|first_name|last_name|salary|   nationality|           phone|        ssn|\n",
       "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
       "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
       "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
       "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
       "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
       "+-----------+----------+---------+------+--------------+----------------+-----------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47bdbfad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cast'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6140225a24a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'cast'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring, lit, cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e3fcc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|employee_id|           phone|        ssn|phone_last4|ssn_last4|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|          1| +1 123 456 7890|123 45 6789|       7890|     6789|\n",
      "|          2|+91 234 567 8901|456 78 9123|       8901|     9123|\n",
      "|          3|+44 111 111 1111|222 33 4444|       1111|     4444|\n",
      "|          4|+61 987 654 3210|789 12 6118|       3210|     6118|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(\"employee_id\",\"phone\",\"ssn\"). \\\n",
    "    withColumn(\"phone_last4\", substring(col(\"phone\"),-4,4).cast(\"int\")). \\\n",
    "    withColumn(\"ssn_last4\", substring(col(\"ssn\"),8,4).cast(\"int\")). \\\n",
    "    show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "060fd1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      " |-- phone_last4: integer (nullable = true)\n",
      " |-- ssn_last4: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    select(\"employee_id\",\"phone\",\"ssn\"). \\\n",
    "    withColumn(\"phone_last4\", substring(col(\"phone\"),-4,4).cast(\"int\")). \\\n",
    "    withColumn(\"ssn_last4\", substring(col(\"ssn\"),8,4).cast(\"int\")). \\\n",
    "    printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c381d4d",
   "metadata": {},
   "source": [
    "### 172 Extracting String using split from Spark DataFrame columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ba01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout',6000). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f3b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca2499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l,\"dummy STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "771f21a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009a6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49727e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|split(Hello World, how are you,  , -1)|\n",
      "+--------------------------------------+\n",
      "|[Hello, World,, how, are, you]        |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(lit(\"Hello World, how are you\"), \" \")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c760b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|split(Hello World, how are you,  , -1)[2]|\n",
      "+-----------------------------------------+\n",
      "|how                                      |\n",
      "+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(lit(\"Hello World, how are you\"), \" \")[2]). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61f85fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|col   |\n",
      "+------+\n",
      "|Hello |\n",
      "|World,|\n",
      "|how   |\n",
      "|are   |\n",
      "|you   |\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(explode(split(lit(\"Hello World, how are you\"), \" \"))). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf28c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|word  |\n",
      "+------+\n",
      "|Hello |\n",
      "|World,|\n",
      "|how   |\n",
      "|are   |\n",
      "|you   |\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(explode(split(lit(\"Hello World, how are you\"), \" \")).alias(\"word\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "597b8a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890,+1 234 567 8901\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111,+44 222 222 2222\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210,+61 876 543 2109\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd61ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.createDataFrame(employees,\n",
    "                schema=\"\"\"employee_id INT, first_name STRING, last_name STRING, \n",
    "                salary FLOAT, nationality STRING, phone_numbers STRING, ssn STRING\n",
    "                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a66ff4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+---------------------------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|nationality   |phone_numbers                    |ssn        |\n",
      "+-----------+----------+---------+------+--------------+---------------------------------+-----------+\n",
      "|1          |Scott     |Tiger    |1000.0|united states |+1 123 456 7890,+1 234 567 8901  |123 45 6789|\n",
      "|2          |Henry     |Ford     |1250.0|India         |+91 234 567 8901                 |456 78 9123|\n",
      "|3          |Nick      |Junior   |750.0 |united KINGDOM|+44 111 111 1111,+44 222 222 2222|222 33 4444|\n",
      "|4          |Bill      |Gomes    |1500.0|AUSTRALIA     |+61 987 654 3210,+61 876 543 2109|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+---------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3379f2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------------------+-----------+\n",
      "|first_name|last_name|phone_numbers                    |ssn        |\n",
      "+----------+---------+---------------------------------+-----------+\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|\n",
      "|Henry     |Ford     |+91 234 567 8901                 |456 78 9123|\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|\n",
      "+----------+---------+---------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83d2ed12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------------------+-----------+------------------------------------+\n",
      "|first_name|last_name|phone_numbers                    |ssn        |phone_number                        |\n",
      "+----------+---------+---------------------------------+-----------+------------------------------------+\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|[+1 123 456 7890, +1 234 567 8901]  |\n",
      "|Henry     |Ford     |+91 234 567 8901                 |456 78 9123|[+91 234 567 8901]                  |\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|[+44 111 111 1111, +44 222 222 2222]|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|[+61 987 654 3210, +61 876 543 2109]|\n",
      "+----------+---------+---------------------------------+-----------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn'). \\\n",
    "    withColumn('phone_number',split('phone_numbers',\",\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d393de37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------------------+-----------+------------------------------------+\n",
      "|first_name|last_name|phone_numbers                    |ssn        |phone_number                        |\n",
      "+----------+---------+---------------------------------+-----------+------------------------------------+\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|[+1 123 456 7890, +1 234 567 8901]  |\n",
      "|Henry     |Ford     |+91 234 567 8901                 |456 78 9123|[+91 234 567 8901]                  |\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|[+44 111 111 1111, +44 222 222 2222]|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|[+61 987 654 3210, +61 876 543 2109]|\n",
      "+----------+---------+---------------------------------+-----------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn',split('phone_numbers',\",\").alias('phone_number')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fe4264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "|first_name|last_name|phone_numbers                    |ssn        |phone_number    |\n",
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 123 456 7890 |\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 234 567 8901 |\n",
      "|Henry     |Ford     |+91 234 567 8901                 |456 78 9123|+91 234 567 8901|\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 111 111 1111|\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 222 222 2222|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 987 654 3210|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 876 543 2109|\n",
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn'). \\\n",
    "    withColumn('phone_number',explode(split('phone_numbers',\",\"))). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bd6e748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "|first_name|last_name|phone_numbers                    |ssn        |phone_number    |\n",
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 123 456 7890 |\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 234 567 8901 |\n",
      "|Henry     |Ford     |+91 234 567 8901                 |456 78 9123|+91 234 567 8901|\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 111 111 1111|\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 222 222 2222|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 987 654 3210|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 876 543 2109|\n",
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn',explode(split('phone_numbers',\",\")).alias('phone_number')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13f94850",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Generators are not supported when it's nested in expressions, but got: split(explode(split(phone_numbers, ,, -1)),  , -1)[1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2814e9c2e271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memployeesDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'first_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'last_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'phone_numbers'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ssn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'phone_number'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'phone_numbers'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'area_code'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'phone_numbers'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'phone_last4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'phone_numbers'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ssn_last4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ssn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \"\"\"\n\u001b[1;32m   2454\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Generators are not supported when it's nested in expressions, but got: split(explode(split(phone_numbers, ,, -1)),  , -1)[1]"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn'). \\\n",
    "    withColumn('phone_number',explode(split('phone_numbers',\",\"))). \\\n",
    "    withColumn('area_code',split(explode(split('phone_numbers',\",\")),\" \")[1]). \\\n",
    "    withColumn('phone_last4',split(explode(split('phone_numbers',\",\")),\" \")[3]). \\\n",
    "    withColumn('ssn_last4',split('ssn',\" \")[3]). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7fd3ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------------------+-----------+----------------+---------+-----------+---------+\n",
      "|first_name|last_name|phone_numbers                    |ssn        |phone_number    |area_code|phone_last4|ssn_last4|\n",
      "+----------+---------+---------------------------------+-----------+----------------+---------+-----------+---------+\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 123 456 7890 |123      |7890       |null     |\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 234 567 8901 |234      |8901       |null     |\n",
      "|Henry     |Ford     |+91 234 567 8901                 |456 78 9123|+91 234 567 8901|234      |8901       |null     |\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 111 111 1111|111      |1111       |null     |\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 222 222 2222|222      |2222       |null     |\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 987 654 3210|987      |3210       |null     |\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 876 543 2109|876      |2109       |null     |\n",
      "+----------+---------+---------------------------------+-----------+----------------+---------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn'). \\\n",
    "    withColumn('phone_number',explode(split('phone_numbers',\",\"))). \\\n",
    "    select('first_name','last_name', 'phone_numbers', 'ssn','phone_number').\\\n",
    "    withColumn('area_code',split('phone_number',\" \")[1]). \\\n",
    "    withColumn('phone_last4', split('phone_number', \" \")[3]). \\\n",
    "    withColumn('ssn_last4', split('ssn',\" \")[3]). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c5023f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>first_name</th><th>last_name</th><th>phone_numbers</th><th>ssn</th><th>phone_number</th><th>area_code</th><th>phone_last4</th><th>ssn_last4</th></tr>\n",
       "<tr><td>Scott</td><td>Tiger</td><td>+1 123 456 7890,+...</td><td>123 45 6789</td><td>+1 123 456 7890</td><td>123</td><td>7890</td><td>6789</td></tr>\n",
       "<tr><td>Scott</td><td>Tiger</td><td>+1 123 456 7890,+...</td><td>123 45 6789</td><td>+1 234 567 8901</td><td>234</td><td>8901</td><td>6789</td></tr>\n",
       "<tr><td>Henry</td><td>Ford</td><td>+91 234 567 8901</td><td>456 78 9123</td><td>+91 234 567 8901</td><td>234</td><td>8901</td><td>9123</td></tr>\n",
       "<tr><td>Nick</td><td>Junior</td><td>+44 111 111 1111,...</td><td>222 33 4444</td><td>+44 111 111 1111</td><td>111</td><td>1111</td><td>4444</td></tr>\n",
       "<tr><td>Nick</td><td>Junior</td><td>+44 111 111 1111,...</td><td>222 33 4444</td><td>+44 222 222 2222</td><td>222</td><td>2222</td><td>4444</td></tr>\n",
       "<tr><td>Bill</td><td>Gomes</td><td>+61 987 654 3210,...</td><td>789 12 6118</td><td>+61 987 654 3210</td><td>987</td><td>3210</td><td>6118</td></tr>\n",
       "<tr><td>Bill</td><td>Gomes</td><td>+61 987 654 3210,...</td><td>789 12 6118</td><td>+61 876 543 2109</td><td>876</td><td>2109</td><td>6118</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+---------+--------------------+-----------+----------------+---------+-----------+---------+\n",
       "|first_name|last_name|       phone_numbers|        ssn|    phone_number|area_code|phone_last4|ssn_last4|\n",
       "+----------+---------+--------------------+-----------+----------------+---------+-----------+---------+\n",
       "|     Scott|    Tiger|+1 123 456 7890,+...|123 45 6789| +1 123 456 7890|      123|       7890|     6789|\n",
       "|     Scott|    Tiger|+1 123 456 7890,+...|123 45 6789| +1 234 567 8901|      234|       8901|     6789|\n",
       "|     Henry|     Ford|    +91 234 567 8901|456 78 9123|+91 234 567 8901|      234|       8901|     9123|\n",
       "|      Nick|   Junior|+44 111 111 1111,...|222 33 4444|+44 111 111 1111|      111|       1111|     4444|\n",
       "|      Nick|   Junior|+44 111 111 1111,...|222 33 4444|+44 222 222 2222|      222|       2222|     4444|\n",
       "|      Bill|    Gomes|+61 987 654 3210,...|789 12 6118|+61 987 654 3210|      987|       3210|     6118|\n",
       "|      Bill|    Gomes|+61 987 654 3210,...|789 12 6118|+61 876 543 2109|      876|       2109|     6118|\n",
       "+----------+---------+--------------------+-----------+----------------+---------+-----------+---------+"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.select('first_name', 'last_name','phone_numbers','ssn'). \\\n",
    "    withColumn('phone_number',explode(split('phone_numbers',\",\"))). \\\n",
    "    select('first_name','last_name', 'phone_numbers', 'ssn','phone_number').\\\n",
    "    withColumn('area_code',split('phone_number',\" \")[1].cast(\"int\")). \\\n",
    "    withColumn('phone_last4', split('phone_number', \" \")[3].cast(\"int\")). \\\n",
    "    withColumn('ssn_last4', split('ssn',\" \")[2].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "12bbd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = employeesDF.select('first_name', 'last_name','phone_numbers','ssn'). \\\n",
    "                withColumn('phone_number',explode(split('phone_numbers',\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "921f4e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "|first_name|last_name|phone_numbers                    |ssn        |phone_number    |\n",
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 123 456 7890 |\n",
      "|Scott     |Tiger    |+1 123 456 7890,+1 234 567 8901  |123 45 6789|+1 234 567 8901 |\n",
      "|Henry     |Ford     |+91 234 567 8901                 |456 78 9123|+91 234 567 8901|\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 111 111 1111|\n",
      "|Nick      |Junior   |+44 111 111 1111,+44 222 222 2222|222 33 4444|+44 222 222 2222|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 987 654 3210|\n",
      "|Bill      |Gomes    |+61 987 654 3210,+61 876 543 2109|789 12 6118|+61 876 543 2109|\n",
      "+----------+---------+---------------------------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9cd19ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------------+-----------+---------+-----------+---------+\n",
      "|first_name|last_name|    phone_number|        ssn|area_code|phone_last4|ssn_last4|\n",
      "+----------+---------+----------------+-----------+---------+-----------+---------+\n",
      "|     Scott|    Tiger| +1 123 456 7890|123 45 6789|      123|       7890|     6789|\n",
      "|     Scott|    Tiger| +1 234 567 8901|123 45 6789|      234|       8901|     6789|\n",
      "|     Henry|     Ford|+91 234 567 8901|456 78 9123|      234|       8901|     9123|\n",
      "|      Nick|   Junior|+44 111 111 1111|222 33 4444|      111|       1111|     4444|\n",
      "|      Nick|   Junior|+44 222 222 2222|222 33 4444|      222|       2222|     4444|\n",
      "|      Bill|    Gomes|+61 987 654 3210|789 12 6118|      987|       3210|     6118|\n",
      "|      Bill|    Gomes|+61 876 543 2109|789 12 6118|      876|       2109|     6118|\n",
      "+----------+---------+----------------+-----------+---------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('first_name','last_name','phone_number','ssn'). \\\n",
    "    withColumn('area_code',split('phone_number',\" \")[1].cast(\"int\")). \\\n",
    "    withColumn('phone_last4',split('phone_number',\" \")[3].cast(\"int\")). \\\n",
    "    withColumn('ssn_last4',split('ssn',\" \")[2].cast(\"int\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a971251e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'driver'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-aa29dc8ab574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'driver'"
     ]
    }
   ],
   "source": [
    "spark.driver.port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "76046ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>first_name</th><th>last_name</th><th>phone_numbers</th><th>ssn</th><th>phone_number</th></tr>\n",
       "<tr><td>Scott</td><td>Tiger</td><td>+1 123 456 7890,+...</td><td>123 45 6789</td><td>+1 123 456 7890</td></tr>\n",
       "<tr><td>Scott</td><td>Tiger</td><td>+1 123 456 7890,+...</td><td>123 45 6789</td><td>+1 234 567 8901</td></tr>\n",
       "<tr><td>Henry</td><td>Ford</td><td>+91 234 567 8901</td><td>456 78 9123</td><td>+91 234 567 8901</td></tr>\n",
       "<tr><td>Nick</td><td>Junior</td><td>+44 111 111 1111,...</td><td>222 33 4444</td><td>+44 111 111 1111</td></tr>\n",
       "<tr><td>Nick</td><td>Junior</td><td>+44 111 111 1111,...</td><td>222 33 4444</td><td>+44 222 222 2222</td></tr>\n",
       "<tr><td>Bill</td><td>Gomes</td><td>+61 987 654 3210,...</td><td>789 12 6118</td><td>+61 987 654 3210</td></tr>\n",
       "<tr><td>Bill</td><td>Gomes</td><td>+61 987 654 3210,...</td><td>789 12 6118</td><td>+61 876 543 2109</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+---------+--------------------+-----------+----------------+\n",
       "|first_name|last_name|       phone_numbers|        ssn|    phone_number|\n",
       "+----------+---------+--------------------+-----------+----------------+\n",
       "|     Scott|    Tiger|+1 123 456 7890,+...|123 45 6789| +1 123 456 7890|\n",
       "|     Scott|    Tiger|+1 123 456 7890,+...|123 45 6789| +1 234 567 8901|\n",
       "|     Henry|     Ford|    +91 234 567 8901|456 78 9123|+91 234 567 8901|\n",
       "|      Nick|   Junior|+44 111 111 1111,...|222 33 4444|+44 111 111 1111|\n",
       "|      Nick|   Junior|+44 111 111 1111,...|222 33 4444|+44 222 222 2222|\n",
       "|      Bill|    Gomes|+61 987 654 3210,...|789 12 6118|+61 987 654 3210|\n",
       "|      Bill|    Gomes|+61 987 654 3210,...|789 12 6118|+61 876 543 2109|\n",
       "+----------+---------+--------------------+-----------+----------------+"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d07a366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n",
      "|first_name|last_name|count|\n",
      "+----------+---------+-----+\n",
      "|      Nick|   Junior|    2|\n",
      "|     Henry|     Ford|    1|\n",
      "|      Bill|    Gomes|    2|\n",
      "|     Scott|    Tiger|    2|\n",
      "+----------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.groupBy('first_name','last_name'). \\\n",
    "    count(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846dea1e",
   "metadata": {},
   "source": [
    "### 173 Padding characters around Strings in Spark DataFrame Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb65152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout',6000). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af011a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a66aebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l,'dummy STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcae426d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d4fdd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e26270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Hello World!</th></tr>\n",
       "<tr><td>Hello World!</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+\n",
       "|Hello World!|\n",
       "+------------+\n",
       "|Hello World!|\n",
       "+------------+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(lit(\"Hello World!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84d05af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>Hello World!</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+\n",
       "|       dummy|\n",
       "+------------+\n",
       "|Hello World!|\n",
       "+------------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(lit(\"Hello World!\").alias(\"dummy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f6fc4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lpad, rpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53fbbe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     dummy|\n",
      "+----------+\n",
      "|-----Hello|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lpad(lit(\"Hello\"), 10, \"-\").alias(\"dummy\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "687e262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad207fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.createDataFrame(employees). \\\n",
    "    toDF(\"employee_id\", \"first_name\",\n",
    "        \"last_name\", \"salary\",\n",
    "        \"nationality\", \"phone_number\",\n",
    "        \"ssn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc549ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96a0f767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>summary</th><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th><th>phone_number</th><th>ssn</th></tr>\n",
       "<tr><td>count</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td></tr>\n",
       "<tr><td>mean</td><td>2.5</td><td>null</td><td>null</td><td>1125.0</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>stddev</td><td>1.2909944487358056</td><td>null</td><td>null</td><td>322.7486121839514</td><td>null</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>min</td><td>1</td><td>Bill</td><td>Ford</td><td>750.0</td><td>AUSTRALIA</td><td>+1 123 456 7890</td><td>123 45 6789</td></tr>\n",
       "<tr><td>max</td><td>4</td><td>Scott</td><td>Tiger</td><td>1500.0</td><td>united states</td><td>+91 234 567 8901</td><td>789 12 6118</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+------------------+----------+---------+-----------------+-------------+----------------+-----------+\n",
       "|summary|       employee_id|first_name|last_name|           salary|  nationality|    phone_number|        ssn|\n",
       "+-------+------------------+----------+---------+-----------------+-------------+----------------+-----------+\n",
       "|  count|                 4|         4|        4|                4|            4|               4|          4|\n",
       "|   mean|               2.5|      null|     null|           1125.0|         null|            null|       null|\n",
       "| stddev|1.2909944487358056|      null|     null|322.7486121839514|         null|            null|       null|\n",
       "|    min|                 1|      Bill|     Ford|            750.0|    AUSTRALIA| +1 123 456 7890|123 45 6789|\n",
       "|    max|                 4|     Scott|    Tiger|           1500.0|united states|+91 234 567 8901|789 12 6118|\n",
       "+-------+------------------+----------+---------+-----------------+-------------+----------------+-----------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61fb5869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee_id</th><th>first_name</th><th>last_name</th><th>salary</th><th>nationality</th><th>phone_number</th><th>ssn</th></tr>\n",
       "<tr><td>1</td><td>Scott</td><td>Tiger</td><td>1000.0</td><td>united states</td><td>+1 123 456 7890</td><td>123 45 6789</td></tr>\n",
       "<tr><td>2</td><td>Henry</td><td>Ford</td><td>1250.0</td><td>India</td><td>+91 234 567 8901</td><td>456 78 9123</td></tr>\n",
       "<tr><td>3</td><td>Nick</td><td>Junior</td><td>750.0</td><td>united KINGDOM</td><td>+44 111 111 1111</td><td>222 33 4444</td></tr>\n",
       "<tr><td>4</td><td>Bill</td><td>Gomes</td><td>1500.0</td><td>AUSTRALIA</td><td>+61 987 654 3210</td><td>789 12 6118</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[employee_id: bigint, first_name: string, last_name: string, salary: double, nationality: string, phone_number: string, ssn: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e4de30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['employee_id',\n",
       " 'first_name',\n",
       " 'last_name',\n",
       " 'salary',\n",
       " 'nationality',\n",
       " 'phone_number',\n",
       " 'ssn']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fcd9229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|lpad(employee_id, 5, 0)|\n",
      "+-----------------------+\n",
      "|                  00001|\n",
      "|                  00002|\n",
      "|                  00003|\n",
      "|                  00004|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(lpad(\"employee_id\",5,\"0\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef4921af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>lpad(employee_id, 5, 0)</th><th>rpad(first_name, 10, -)</th><th>rpad(last_name, 10, -)</th><th>lpad(salary, 10, 0)</th><th>rpad(nationality, 15, -)</th><th>rpad(phone_number, 17, -)</th><th>ssn</th></tr>\n",
       "<tr><td>00001</td><td>Scott-----</td><td>Tiger-----</td><td>00001000.0</td><td>united states--</td><td>+1 123 456 7890--</td><td>123 45 6789</td></tr>\n",
       "<tr><td>00002</td><td>Henry-----</td><td>Ford------</td><td>00001250.0</td><td>India----------</td><td>+91 234 567 8901-</td><td>456 78 9123</td></tr>\n",
       "<tr><td>00003</td><td>Nick------</td><td>Junior----</td><td>00000750.0</td><td>united KINGDOM-</td><td>+44 111 111 1111-</td><td>222 33 4444</td></tr>\n",
       "<tr><td>00004</td><td>Bill------</td><td>Gomes-----</td><td>00001500.0</td><td>AUSTRALIA------</td><td>+61 987 654 3210-</td><td>789 12 6118</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------------+-----------------------+----------------------+-------------------+------------------------+-------------------------+-----------+\n",
       "|lpad(employee_id, 5, 0)|rpad(first_name, 10, -)|rpad(last_name, 10, -)|lpad(salary, 10, 0)|rpad(nationality, 15, -)|rpad(phone_number, 17, -)|        ssn|\n",
       "+-----------------------+-----------------------+----------------------+-------------------+------------------------+-------------------------+-----------+\n",
       "|                  00001|             Scott-----|            Tiger-----|         00001000.0|         united states--|        +1 123 456 7890--|123 45 6789|\n",
       "|                  00002|             Henry-----|            Ford------|         00001250.0|         India----------|        +91 234 567 8901-|456 78 9123|\n",
       "|                  00003|             Nick------|            Junior----|         00000750.0|         united KINGDOM-|        +44 111 111 1111-|222 33 4444|\n",
       "|                  00004|             Bill------|            Gomes-----|         00001500.0|         AUSTRALIA------|        +61 987 654 3210-|789 12 6118|\n",
       "+-----------------------+-----------------------+----------------------+-------------------+------------------------+-------------------------+-----------+"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.select(\n",
    "    lpad(\"employee_id\",5,\"0\"),\n",
    "    rpad(\"first_name\",10,\"-\"),\n",
    "    rpad(\"last_name\",10,\"-\"),\n",
    "    lpad(\"salary\",10,\"0\"),\n",
    "    rpad(\"nationality\",15,\"-\"),\n",
    "    rpad(\"phone_number\",17,\"-\"),\n",
    "    \"ssn\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9c07edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3420db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "empFixedDF = employeesDF.select(\n",
    "    concat(\n",
    "    lpad(\"employee_id\",5,\"0\"),\n",
    "    rpad(\"first_name\",10,\"-\"),\n",
    "    rpad(\"last_name\",10,\"-\"),\n",
    "    lpad(\"salary\",10,\"0\"),\n",
    "    rpad(\"nationality\",15,\"-\"),\n",
    "    rpad(\"phone_number\",17,\"-\"),\n",
    "    \"ssn\"\n",
    "    ).alias(\"employees\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe0674e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "|employees                                                                     |\n",
      "+------------------------------------------------------------------------------+\n",
      "|00001Scott-----Tiger-----00001000.0united states--+1 123 456 7890--123 45 6789|\n",
      "|00002Henry-----Ford------00001250.0India----------+91 234 567 8901-456 78 9123|\n",
      "|00003Nick------Junior----00000750.0united KINGDOM-+44 111 111 1111-222 33 4444|\n",
      "|00004Bill------Gomes-----00001500.0AUSTRALIA------+61 987 654 3210-789 12 6118|\n",
      "+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empFixedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f36a2",
   "metadata": {},
   "source": [
    "### 174 Trimming characters from strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11c25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ltrim, rtrim, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b8b9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout',6000). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3139b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [(\"    Hello.    \",)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cada189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l).toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac50e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ltrim, rtrim, trim, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e913daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|         dummy|     ltrim|\n",
      "+--------------+----------+\n",
      "|    Hello.    |Hello.    |\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ltrim\",ltrim(\"dummy\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc5d17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|         dummy|     ltrim|\n",
      "+--------------+----------+\n",
      "|    Hello.    |Hello.    |\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ltrim\",ltrim(col(\"dummy\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5cd2fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------+------+\n",
      "|         dummy|     ltrim|     rtrim|  trim|\n",
      "+--------------+----------+----------+------+\n",
      "|    Hello.    |Hello.    |    Hello.|Hello.|\n",
      "+--------------+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ltrim\",ltrim(col(\"dummy\"))). \\\n",
    "    withColumn(\"rtrim\", rtrim(col(\"dummy\"))). \\\n",
    "    withColumn(\"trim\", trim(col(\"dummy\"))). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa7a7e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|function_desc                                                                |\n",
      "+-----------------------------------------------------------------------------+\n",
      "|Function: rtrim                                                              |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.StringTrimRight             |\n",
      "|Usage: \n",
      "    rtrim(str) - Removes the trailing space characters from `str`.\n",
      "  |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe function rtrim\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1872d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|0000000000000_msdian|\n",
      "|0000000009874_retail|\n",
      "|          00000_2_db|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5390b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dd2f254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+------+\n",
      "|         dummy|     ltrim|    rtrim|  trim|\n",
      "+--------------+----------+---------+------+\n",
      "|    Hello.    |Hello.    |    Hello|Hello.|\n",
      "+--------------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ltrim\", expr(\"ltrim(dummy)\")). \\\n",
    "    withColumn(\"rtrim\", expr(\"rtrim('.',rtrim(dummy))\")). \\\n",
    "    withColumn(\"trim\", trim(col(\"dummy\"))). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff7d9073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Function: trim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.StringTrim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Usage: \n",
      "    trim(str) - Removes the leading and trailing space characters from `str`.\n",
      "\n",
      "    trim(BOTH FROM str) - Removes the leading and trailing space characters from `str`.\n",
      "\n",
      "    trim(LEADING FROM str) - Removes the leading space characters from `str`.\n",
      "\n",
      "    trim(TRAILING FROM str) - Removes the trailing space characters from `str`.\n",
      "\n",
      "    trim(trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\n",
      "\n",
      "    trim(BOTH trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\n",
      "\n",
      "    trim(LEADING trimStr FROM str) - Remove the leading `trimStr` characters from `str`.\n",
      "\n",
      "    trim(TRAILING trimStr FROM str) - Remove the trailing `trimStr` characters from `str`.\n",
      "  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe function trim\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08513091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+------+\n",
      "|dummy         |ltrim     |rtrim    |trim  |\n",
      "+--------------+----------+---------+------+\n",
      "|    Hello.    |Hello.    |    Hello|Hello.|\n",
      "+--------------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ltrim\",expr(\"trim(LEADING ' ' FROM dummy)\")). \\\n",
    "    withColumn(\"rtrim\",expr(\"trim(TRAILING '.' from rtrim(dummy))\")). \\\n",
    "    withColumn(\"trim\", expr(\"trim(BOTH ' ' FROM dummy)\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd67c5ff",
   "metadata": {},
   "source": [
    "### 175 Date and Time Manipulation Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c9235f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port','0'). \\\n",
    "        config('spark.sql.warehouse.dir', '/user/{username}/warehouse'). \\\n",
    "        config('spark.shuffle.io.connectionTimeout',6000). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709caaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4cdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l).toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a930bb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c756885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29c4c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e37425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|2024-02-09    |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f76dd073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|current_timestamp()    |\n",
      "+-----------------------+\n",
      "|2024-02-09 13:32:59.513|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_timestamp()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b5e4865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_date, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e61e4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|to_date   |\n",
      "+----------+\n",
      "|2024-02-09|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('20240209'), 'yyyyMMdd').alias('to_date')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c700d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|to_date   |\n",
      "+----------+\n",
      "|2024-02-09|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('20240209 1725'), 'yyyyMMdd HHmm').alias('to_date')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72e9922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|to_timestamp       |\n",
      "+-------------------+\n",
      "|2024-02-09 17:25:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('20240209 1725'), 'yyyyMMdd HHmm').alias('to_timestamp')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14304a",
   "metadata": {},
   "source": [
    "### 176 Date and Time ARithmetic on Spark Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c9e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port',0). \\\n",
    "    config('spark.sql.warehouse.dir',f\"/user/{username}/warehouse\"). \\\n",
    "    config('spark.shuffle.io.connectionTimeOut',6000). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f\"{username} | Pyspark Processing column Data\"). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e550b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b309be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes,schema = \"date STRING, time STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0048317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb552df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba170efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_add in module pyspark.sql.functions:\n",
      "\n",
      "date_add(start, days)\n",
      "    Returns the date that is `days` days after `start`\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
      "    [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e98bd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_sub in module pyspark.sql.functions:\n",
      "\n",
      "date_sub(start, days)\n",
      "    Returns the date that is `days` days before `start`\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_sub(df.dt, 1).alias('prev_date')).collect()\n",
      "    [Row(prev_date=datetime.date(2015, 4, 7))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "900afb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+-------------+-------------+-------------+\n",
      "|date      |time                   |date_add_date|time_add_date|date_sub_date|time_sub_date|\n",
      "+----------+-----------------------+-------------+-------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-03-10   |2014-03-10   |2014-02-18   |2014-02-18   |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-03-10   |2016-03-10   |2016-02-19   |2016-02-19   |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-11-10   |2018-01-10   |2017-10-21   |2017-12-21   |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-12-10   |2019-09-10   |2019-11-20   |2019-08-21   |\n",
      "+----------+-----------------------+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"date_add_date\",date_add(\"date\",10)). \\\n",
    "    withColumn(\"time_add_date\", date_add(\"time\",10)). \\\n",
    "    withColumn(\"date_sub_date\",date_sub(\"date\",10)). \\\n",
    "    withColumn(\"time_sub_date\", date_sub(\"time\",10)). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9cdc2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date,current_timestamp, datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "300ce38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+-------------+\n",
      "|date      |time                   |datediff_date|datediff_time|\n",
      "+----------+-----------------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|3634         |3634         |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2903         |2903         |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2293         |2232         |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|1533         |1624         |\n",
      "+----------+-----------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"datediff_date\",datediff(current_date(),\"date\")). \\\n",
    "    withColumn(\"datediff_time\",datediff(current_timestamp(), \"time\")). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "713e968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import months_between, add_months, round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b56d9af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function months_between in module pyspark.sql.functions:\n",
      "\n",
      "months_between(date1, date2, roundOff=True)\n",
      "    Returns number of months between dates date1 and date2.\n",
      "    If date1 is later than date2, then the result is positive.\n",
      "    If date1 and date2 are on the same day of month, or both are the last day of month,\n",
      "    returns an integer (time of day will be ignored).\n",
      "    The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n",
      "    >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n",
      "    [Row(months=3.94959677)]\n",
      "    >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n",
      "    [Row(months=3.9495967741935485)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(months_between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0588707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "|date      |time                   |months_between_date|months_between_time|add_months_date|add_months_time|\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|119.42             |119.41             |2014-05-28     |2014-05-28     |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|95.39              |95.38              |2016-05-29     |2016-05-29     |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|75.32              |73.31              |2018-01-31     |2018-03-31     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|50.35              |53.33              |2020-02-29     |2019-11-30     |\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"months_between_date\",round(months_between(current_date(),\"date\"),2)). \\\n",
    "    withColumn(\"months_between_time\", round(months_between(current_timestamp(),\"time\"),2)). \\\n",
    "    withColumn(\"add_months_date\",add_months(\"date\",3)). \\\n",
    "    withColumn(\"add_months_time\", add_months(\"time\",3)). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a402619",
   "metadata": {},
   "source": [
    "### 177 Using Date and Time Trunc functions on spark Data Frame columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e4a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51325ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\",0). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/user/{username}/warehouse\"). \\\n",
    "    config(\"spark.shuffle.io.connectionTimeOut\",6000). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f\"{username} | Python Processing Column Data\"). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04387d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51154f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l).toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26e2521b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "060f038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb0e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes,\"date STRING, time STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9f4ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df04da94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------+\n",
      "|date      |time                   |date_trunc|time_trunc|\n",
      "+----------+-----------------------+----------+----------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-01|2014-01-01|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-01|2016-01-01|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-01|2017-01-01|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-01|2019-01-01|\n",
      "+----------+-----------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"date_trunc\",trunc(\"date\",\"MM\")). \\\n",
    "    withColumn(\"time_trunc\", trunc(\"time\", \"yy\")). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27301a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ceb4233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+\n",
      "|date      |time                   |date_trunc         |time_trunc         |\n",
      "+----------+-----------------------+-------------------+-------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-01 00:00:00|2014-01-01 00:00:00|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-01 00:00:00|2016-01-01 00:00:00|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-01 00:00:00|2017-01-01 00:00:00|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-01 00:00:00|2019-01-01 00:00:00|\n",
      "+----------+-----------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"date_trunc\",date_trunc(\"MM\",\"date\")).\\\n",
    "    withColumn(\"time_trunc\",date_trunc(\"yy\",\"time\")). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10fd6cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+-------------------+\n",
      "|date      |time                   |date_dt            |time_dt            |time_dt1           |\n",
      "+----------+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-28 00:00:00|2014-02-28 10:00:00|2014-02-28 00:00:00|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-29 00:00:00|2016-02-29 08:00:00|2016-02-29 00:00:00|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-31 00:00:00|2017-12-31 11:00:00|2017-12-31 00:00:00|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-30 00:00:00|2019-08-31 00:00:00|2019-08-31 00:00:00|\n",
      "+----------+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"date_dt\",date_trunc(\"Hour\",\"date\")). \\\n",
    "    withColumn(\"time_dt\",date_trunc(\"Hour\",\"time\")). \\\n",
    "    withColumn(\"time_dt1\",date_trunc(\"dd\",\"time\")). \\\n",
    "show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09aea4b",
   "metadata": {},
   "source": [
    "###  178 Date and Time Extract functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3367328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04cea553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f920907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\",0). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/user/{username}/warehouse\"). \\\n",
    "    config(\"spark.shuffle.io.connectionTimeout\",6000). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f\"{username} | Python Processing Data columns\"). \\\n",
    "    master(\"yarn\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3337981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5271c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l,schema=\"dummy STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad64db5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90dbcb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import weekofyear, dayofmonth, dayofweek, dayofyear, year, month\n",
    "\n",
    "from pyspark.sql.functions import current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ff53771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------------------+--------------------------------+------------+------------+-----------+---------+\n",
      "|current_date|year(current_date() AS `year`)|month(current_date() AS `month`)|week_of_year|day_of_month|day_of_year|dayofweek|\n",
      "+------------+------------------------------+--------------------------------+------------+------------+-----------+---------+\n",
      "|2024-02-10  |2024                          |2                               |6           |10          |41         |7        |\n",
      "+------------+------------------------------+--------------------------------+------------+------------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date().alias(\"current_date\"),\n",
    "         year(current_date().alias(\"year\")),\n",
    "         month(current_date().alias(\"month\")),\n",
    "         weekofyear(current_date()).alias(\"week_of_year\"),\n",
    "         dayofmonth(current_date()).alias(\"day_of_month\"),\n",
    "         dayofyear(current_date()).alias(\"day_of_year\"),\n",
    "         dayofweek(current_date()).alias(\"dayofweek\")). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2f9488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, hour, minute, second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fe09aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+------+------+\n",
      "|current_timestamp      |hour|minute|second|\n",
      "+-----------------------+----+------+------+\n",
      "|2024-02-10 11:14:50.558|11  |14    |50    |\n",
      "+-----------------------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_timestamp().alias(\"current_timestamp\"),\n",
    "         hour(current_timestamp()).alias(\"hour\"),\n",
    "         minute(current_timestamp()).alias(\"minute\"),\n",
    "         second(current_timestamp()).alias(\"second\")).\\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b63e5403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+-----+----------+----+------+------+\n",
      "|current_timestamp      |year|month|dayofmonth|hour|minute|second|\n",
      "+-----------------------+----+-----+----------+----+------+------+\n",
      "|2024-02-10 11:14:52.502|2024|2    |10        |11  |14    |52    |\n",
      "+-----------------------+----+-----+----------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_timestamp().alias(\"current_timestamp\"),\n",
    "         year(current_timestamp()).alias(\"year\"),\n",
    "         month(current_timestamp()).alias(\"month\"),\n",
    "         dayofmonth(current_timestamp()).alias(\"dayofmonth\"),\n",
    "         hour(current_timestamp()).alias(\"hour\"),\n",
    "         minute(current_timestamp()).alias(\"minute\"),\n",
    "         second(current_timestamp()).alias(\"second\")).\\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44b80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62bfa498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config(\"spark.ui.port\",0). \\\n",
    "    config(\"spark.sql.warehouse.dir\",f\"/user/{username}/warehouse\"). \\\n",
    "    config(\"spark.shuffle.io.connectionTimeout\",6000). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f\"{username} | Python Processing Data columns\"). \\\n",
    "    master(\"yarn\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e2283bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d99460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes).toDF(\"date\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "463e8b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 52352)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8359603a",
   "metadata": {},
   "source": [
    "### 179 to_date to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef37465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd6b3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'itv011204'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7095420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port',0). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse'). \\\n",
    "        config('spark.sql.io.connectionTimeout','6000'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Data Processing functions'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea01e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(20140228, \"28-Feb-2014 10:00:00.123\"),\n",
    "                     (20160229, \"20-Feb-2016 08:08:08.999\"),\n",
    "                     (20171031, \"31-Dec-2017 11:59:59.123\"),\n",
    "                     (20191130, \"31-Aug-2019 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f7a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes,schema=\"date BIGINT, time STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a8f3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+\n",
      "|date    |time                    |\n",
      "+--------+------------------------+\n",
      "|20140228|28-Feb-2014 10:00:00.123|\n",
      "|20160229|20-Feb-2016 08:08:08.999|\n",
      "|20171031|31-Dec-2017 11:59:59.123|\n",
      "|20191130|31-Aug-2019 00:00:00.000|\n",
      "+--------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12510320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: long (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "719a7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[('X',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fa02728",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l).toDF('dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "009aeff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>dummy</th></tr>\n",
       "<tr><td>X</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+\n",
       "|dummy|\n",
       "+-----+\n",
       "|    X|\n",
       "+-----+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "175ba86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3539d52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_date in module pyspark.sql.functions:\n",
      "\n",
      "to_date(col, format=None)\n",
      "    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n",
      "    using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
      "    is omitted. Equivalent to ``col.cast(\"date\")``.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 2.2.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "    [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "    [Row(date=datetime.date(1997, 2, 28))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4acc16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|to_date   |\n",
      "+----------+\n",
      "|2021-03-02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('20210302'),'yyyyMMdd').alias(\"to_date\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acded3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|to_date   |\n",
      "+----------+\n",
      "|2021-03-02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('2021061'),'yyyyDDD').alias(\"to_date\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a88a977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|to_date   |\n",
      "+----------+\n",
      "|2021-03-02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('02/03/2021'),'dd/MM/yyyy').alias('to_date')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97d71be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|to_date   |\n",
      "+----------+\n",
      "|2021-03-02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('02-03-2021'),'dd-MM-yyyy').alias('to_date')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8d197e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|to_date   |\n",
      "+----------+\n",
      "|2021-03-02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('02-Mar-2021'),'dd-MMM-yyyy').alias('to_date')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4605878e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|to_date   |\n",
      "+----------+\n",
      "|2021-03-02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('02-March-2021'),'dd-MMMM-yyyy').alias('to_date')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f00e089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "065f4cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|to_timestamp       |\n",
      "+-------------------+\n",
      "|2021-03-02 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('02-Mar-2021'),'dd-MMM-yyyy').alias('to_timestamp')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c78d04ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|to_timestamp       |\n",
      "+-------------------+\n",
      "|2021-03-02 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('02Mar2021'),'ddMMMyyyy').alias('to_timestamp')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb7468bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|to_timestamp       |\n",
      "+-------------------+\n",
      "|2021-03-02 17:30:15|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('02-Mar-2021 17:30:15'),'dd-MMM-yyyy HH:mm:ss').alias('to_timestamp')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23c1ccc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+\n",
      "|date    |time                    |\n",
      "+--------+------------------------+\n",
      "|20140228|28-Feb-2014 10:00:00.123|\n",
      "|20160229|20-Feb-2016 08:08:08.999|\n",
      "|20171031|31-Dec-2017 11:59:59.123|\n",
      "|20191130|31-Aug-2019 00:00:00.000|\n",
      "+--------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4588d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: long (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99e5978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41b05d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|to_date   |to_timestamp       |\n",
      "+----------+-------------------+\n",
      "|2014-02-28|2014-02-28 00:00:00|\n",
      "|2016-02-29|2016-02-29 00:00:00|\n",
      "|2017-10-31|2017-10-31 00:00:00|\n",
      "|2019-11-30|2019-11-30 00:00:00|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.select(\n",
    "            to_date(col('date').cast('string'),'yyyyMMdd').alias('to_date'),\n",
    "            to_timestamp(col('date').cast('string'),'yyyyMMdd').alias('to_timestamp')). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caaedaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+-----------------------+\n",
      "|to_date1  |to_date2  |to_timestamp1      |to_timestamp2          |\n",
      "+----------+----------+-------------------+-----------------------+\n",
      "|2014-02-28|2014-02-28|2014-02-28 00:00:00|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-20|2016-02-29 00:00:00|2016-02-20 08:08:08.999|\n",
      "|2017-10-31|2017-12-31|2017-10-31 00:00:00|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31|2019-11-30 00:00:00|2019-08-31 00:00:00    |\n",
      "+----------+----------+-------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.select(\n",
    "    to_date(col('date').cast('string'),'yyyyMMdd').alias('to_date1'),\n",
    "    to_date(col('time'),'dd-MMM-yyyy HH:mm:ss.SSS').alias('to_date2'),\n",
    "    to_timestamp(col('date').cast('string'),'yyyyMMdd').alias('to_timestamp1'),\n",
    "    to_timestamp(col('time'),'dd-MMM-yyyy HH:mm:ss.SSS').alias('to_timestamp2')\n",
    "). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a72d7",
   "metadata": {},
   "source": [
    "### 180 Using date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "707311ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ff80bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port',0). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse'). \\\n",
    "        config('spark.sql.io.connectionTimeout','6000'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Data Processing functions'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8561da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37dd80a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes, schema=\"date STRING, time STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e598e58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57e706b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59a36473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+-------+\n",
      "|date      |time                   |date_ym|time_ym|\n",
      "+----------+-----------------------+-------+-------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|201402 |201402 |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|201602 |201602 |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|201710 |201712 |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|201911 |201908 |\n",
      "+----------+-----------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn('date_ym',date_format('date','yyyyMM')). \\\n",
    "    withColumn('time_ym',date_format('time','yyyyMM')). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bde0192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+-------+\n",
      "|date      |time                   |date_ym|time_ym|\n",
      "+----------+-----------------------+-------+-------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|201402 |201402 |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|201602 |201602 |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|201710 |201712 |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|201911 |201908 |\n",
      "+----------+-----------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn('date_ym',date_format('date','yyyyMM').cast('int')). \\\n",
    "    withColumn('time_ym',date_format('time','yyyyMM').cast('int')). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "786ff268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+--------------+--------------+\n",
      "|date      |time                   |date_ym       |time_ym       |\n",
      "+----------+-----------------------+--------------+--------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|20140228000000|20140228100000|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|20160229000000|20160229080808|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|20171031000000|20171231115959|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|20191130000000|20190831000000|\n",
      "+----------+-----------------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn('date_ym',date_format('date','yyyyMMddHHmmss')). \\\n",
    "    withColumn('time_ym',date_format('time','yyyyMMddHHmmss')). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d92fac57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- date_ym: string (nullable = true)\n",
      " |-- time_ym: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn('date_ym',date_format('date','yyyyMMddHHmmss')). \\\n",
    "    withColumn('time_ym',date_format('time','yyyyMMddHHmmss')). \\\n",
    "printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9abd76de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+--------------+--------------+\n",
      "|date      |time                   |date_ym       |time_ym       |\n",
      "+----------+-----------------------+--------------+--------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|20140228000000|20140228100000|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|20160229000000|20160229080808|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|20171031000000|20171231115959|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|20191130000000|20190831000000|\n",
      "+----------+-----------------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn('date_ym',date_format('date','yyyyMMddHHmmss').cast('long')). \\\n",
    "    withColumn('time_ym',date_format('time','yyyyMMddHHmmss').cast('long')). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6fffd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- date_ym: string (nullable = true)\n",
      " |-- time_ym: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn('date_ym',date_format('date','yyyyDDD')). \\\n",
    "    withColumn('time_ym',date_format('time','yyyyDDD')). \\\n",
    "printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87c83cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+-------+\n",
      "|date      |time                   |date_ym|time_ym|\n",
      "+----------+-----------------------+-------+-------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014059|2014059|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016060|2016060|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017304|2017365|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019334|2019243|\n",
      "+----------+-----------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn('date_ym',date_format('date','yyyyDDD')). \\\n",
    "    withColumn('time_ym',date_format('time','yyyyDDD')). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "28cb5ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-----------------+-----------------+\n",
      "|date      |time                   |date_ym          |time_ym          |\n",
      "+----------+-----------------------+-----------------+-----------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|February 28, 2014|February 28, 2014|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|February 29, 2016|February 29, 2016|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|October 31, 2017 |December 31, 2017|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|November 30, 2019|August 31, 2019  |\n",
      "+----------+-----------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn('date_ym',date_format('date','MMMM d, yyyy')). \\\n",
    "    withColumn('time_ym',date_format('time','MMMM d, yyyy')). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b37fc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+--------+\n",
      "|date      |time                   |date_ym|time_ym |\n",
      "+----------+-----------------------+-------+--------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|Fri    |Friday  |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|Mon    |Monday  |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|Tue    |Sunday  |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|Sat    |Saturday|\n",
      "+----------+-----------------------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn('date_ym',date_format('date','EE')). \\\n",
    "    withColumn('time_ym',date_format('time','EEEE')). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3e254",
   "metadata": {},
   "source": [
    "### 181 Dealing with Unix Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c0dcdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc72e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port',0). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse'). \\\n",
    "        config('spark.sql.io.connectionTimeout','6000'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Data Processing functions'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5194c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3915a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(20140228, \"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (20160229, \"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (20171031, \"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (20191130, \"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c346bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes).toDF(\"dateid\",\"date\",\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f80db56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------------+\n",
      "|dateid  |date      |time                   |\n",
      "+--------+----------+-----------------------+\n",
      "|20140228|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|20160229|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|20171031|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|20191130|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+--------+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0ef9c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dateid: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40e79363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------------+------------+\n",
      "|dateid  |date      |time                   |unix_date_id|\n",
      "+--------+----------+-----------------------+------------+\n",
      "|20140228|2014-02-28|2014-02-28 10:00:00.123|1393563600  |\n",
      "|20160229|2016-02-29|2016-02-29 08:08:08.999|1456722000  |\n",
      "|20171031|2017-10-31|2017-12-31 11:59:59.123|1509422400  |\n",
      "|20191130|2019-11-30|2019-08-31 00:00:00.000|1575090000  |\n",
      "+--------+----------+-----------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"unix_date_id\",unix_timestamp(col('dateid').cast('string'),'yyyyMMdd')). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "094c613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------------+------------+----------+----------+\n",
      "|dateid  |date      |time                   |unix_date_id|unix_date |unix_time |\n",
      "+--------+----------+-----------------------+------------+----------+----------+\n",
      "|20140228|2014-02-28|2014-02-28 10:00:00.123|1393563600  |1393563600|1393599600|\n",
      "|20160229|2016-02-29|2016-02-29 08:08:08.999|1456722000  |1456722000|1456751288|\n",
      "|20171031|2017-10-31|2017-12-31 11:59:59.123|1509422400  |1509422400|1514739599|\n",
      "|20191130|2019-11-30|2019-08-31 00:00:00.000|1575090000  |1575090000|1567224000|\n",
      "+--------+----------+-----------------------+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "    withColumn(\"unix_date_id\",unix_timestamp(col('dateid').cast('string'),'yyyyMMdd')). \\\n",
    "    withColumn(\"unix_date\",unix_timestamp('date','yyyy-MM-dd')). \\\n",
    "    withColumn(\"unix_time\",unix_timestamp('time','yyyy-MM-dd HH:mm:ss.SSS')). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01b7af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cca622f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unixtimes = [(1393561800, ),\n",
    "             (1456713488, ),\n",
    "             (1514701799, ),\n",
    "             (1567189800, )\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d49fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "unixtimesDF = spark.createDataFrame(unixtimes).toDF(\"unixtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65b078ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  unixtime|\n",
      "+----------+\n",
      "|1393561800|\n",
      "|1456713488|\n",
      "|1514701799|\n",
      "|1567189800|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unixtimesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2d02ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|unixtime  |date      |\n",
      "+----------+----------+\n",
      "|1393561800|2014-02-27|\n",
      "|1456713488|2016-02-28|\n",
      "|1514701799|2017-12-31|\n",
      "|1567189800|2019-08-30|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unixtimesDF. \\\n",
    "    withColumn('date',from_unixtime('unixtime','yyyy-MM-dd')). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ebd7867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+-----------------+\n",
      "|unixtime  |date      |time               |date description |\n",
      "+----------+----------+-------------------+-----------------+\n",
      "|1393561800|2014-02-27|2014-02-27 23:30:00|February 27, 2014|\n",
      "|1456713488|2016-02-28|2016-02-28 21:38:08|February 28, 2016|\n",
      "|1514701799|2017-12-31|2017-12-31 01:29:59|December 31, 2017|\n",
      "|1567189800|2019-08-30|2019-08-30 14:30:00|August 30, 2019  |\n",
      "+----------+----------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unixtimesDF. \\\n",
    "    withColumn('date',from_unixtime('unixtime','yyyy-MM-dd')). \\\n",
    "    withColumn('time',from_unixtime('unixtime','yyyy-MM-dd HH:mm:ss')). \\\n",
    "    withColumn('date description',from_unixtime('unixtime','MMMM d, yyyy')). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "726f138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+\n",
      "|unixtime  |date      |time               |\n",
      "+----------+----------+-------------------+\n",
      "|1393561800|2014-02-27|2014-02-27 23:30:00|\n",
      "|1456713488|2016-02-28|2016-02-28 21:38:08|\n",
      "|1514701799|2017-12-31|2017-12-31 01:29:59|\n",
      "|1567189800|2019-08-30|2019-08-30 14:30:00|\n",
      "+----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unixtimesDF. \\\n",
    "    withColumn('date',from_unixtime('unixtime','yyyy-MM-dd')). \\\n",
    "    withColumn('time',from_unixtime('unixtime')). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a6577",
   "metadata": {},
   "source": [
    "### 182  dealing with Nulls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca95196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3977c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port',0). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse'). \\\n",
    "        config('spark.sql.io.connectionTimeout','6000'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Data Processing functions'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d29762d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, 10,\n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, None,\n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, '',\n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, 10,\n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63a1a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67d85c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|nationality   |phone_number    |ssn        |\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|1          |Scott     |Tiger    |1000.0|10   |united states |+1 123 456 7890 |123 45 6789|\n",
      "|2          |Henry     |Ford     |1250.0|null |India         |+91 234 567 8901|456 78 9123|\n",
      "|3          |Nick      |Junior   |750.0 |     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|4          |Bill      |Gomes    |1500.0|10   |AUSTRALIA     |+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66463dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77f9fc64",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: 0 of type <class 'int'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-36a96b2a09df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memployeesDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bonus'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bonus'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mcoalesce\u001b[0;34m(*cols)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \"\"\"\n\u001b[1;32m    869\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m     \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \"\"\"\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \"\"\"\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;34m\"For column literals, use 'lit', 'array', 'struct' or 'create_map' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \"function.\".format(col, type(col)))\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: 0 of type <class 'int'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus',coalesce('bonus',0)). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "568336ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a02f863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|employee_id|first_name|last_name|salary|bonus|nationality   |phone_number    |ssn        |bonus1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|1          |Scott     |Tiger    |1000.0|10   |united states |+1 123 456 7890 |123 45 6789|10    |\n",
      "|2          |Henry     |Ford     |1250.0|null |India         |+91 234 567 8901|456 78 9123|0     |\n",
      "|3          |Nick      |Junior   |750.0 |     |united KINGDOM|+44 111 111 1111|222 33 4444|      |\n",
      "|4          |Bill      |Gomes    |1500.0|10   |AUSTRALIA     |+61 987 654 3210|789 12 6118|10    |\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus1',coalesce('bonus',lit('0'))). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f6fc869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58c11905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|employee_id|first_name|last_name|salary|bonus|nationality   |phone_number    |ssn        |bonus1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|1          |Scott     |Tiger    |1000.0|10   |united states |+1 123 456 7890 |123 45 6789|10    |\n",
      "|2          |Henry     |Ford     |1250.0|null |India         |+91 234 567 8901|456 78 9123|null  |\n",
      "|3          |Nick      |Junior   |750.0 |     |united KINGDOM|+44 111 111 1111|222 33 4444|null  |\n",
      "|4          |Bill      |Gomes    |1500.0|10   |AUSTRALIA     |+61 987 654 3210|789 12 6118|10    |\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus1', col('bonus').cast('int')). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37e8d7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|employee_id|first_name|last_name|salary|bonus|nationality   |phone_number    |ssn        |bonus1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|1          |Scott     |Tiger    |1000.0|10   |united states |+1 123 456 7890 |123 45 6789|10    |\n",
      "|2          |Henry     |Ford     |1250.0|null |India         |+91 234 567 8901|456 78 9123|0     |\n",
      "|3          |Nick      |Junior   |750.0 |     |united KINGDOM|+44 111 111 1111|222 33 4444|0     |\n",
      "|4          |Bill      |Gomes    |1500.0|10   |AUSTRALIA     |+61 987 654 3210|789 12 6118|10    |\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus1',coalesce(col('bonus').cast('int'),lit('0'))). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "566679d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fe718bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|bonus1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|    10|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|     0|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|      |\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|    10|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus1', expr(\"nvl(bonus,0)\")). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce0a7add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|bonus1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|    10|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|     0|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|     0|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|    10|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus1', expr(\"nvl(nullif(bonus,''),0)\")). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "782b0250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|payment|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789| 1100.0|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123| 1250.0|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|  750.0|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118| 1650.0|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('payment', col('salary')+(col('salary')*coalesce(col('bonus').cast('int'), lit(0))/100)). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8cec5",
   "metadata": {},
   "source": [
    "### 183 Using CASE and WHEN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97fe0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060aa08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        config('spark.ui.port',0). \\\n",
    "        config('spark.sql.warehouse.dir', f'/user/{username}/warehouse'). \\\n",
    "        config('spark.sql.io.connectionTimeout','6000'). \\\n",
    "        enableHiveSupport(). \\\n",
    "        appName(f'{username} | Data Processing functions'). \\\n",
    "        master('yarn'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aaf3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, 10,\n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, None,\n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, '',\n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, 10,\n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c08a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ba1b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bae27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88b606da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|employee_id|first_name|last_name|salary|bonus|nationality   |phone_number    |ssn        |bonus1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|1          |Scott     |Tiger    |1000.0|10   |united states |+1 123 456 7890 |123 45 6789|10    |\n",
      "|2          |Henry     |Ford     |1250.0|null |India         |+91 234 567 8901|456 78 9123|0     |\n",
      "|3          |Nick      |Junior   |750.0 |     |united KINGDOM|+44 111 111 1111|222 33 4444|0     |\n",
      "|4          |Bill      |Gomes    |1500.0|10   |AUSTRALIA     |+61 987 654 3210|789 12 6118|10    |\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus1', coalesce(col('bonus').cast('int'),lit('0'))). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b22050d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- bonus: string (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      " |-- bonus1: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus1', coalesce(col('bonus').cast('int'),lit('0'))). \\\n",
    "printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d1b00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- bonus: string (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      " |-- bonus1: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus1', coalesce(col('bonus').cast('int'),lit(0))). \\\n",
    "printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88811caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|employee_id|first_name|last_name|salary|bonus|nationality   |phone_number    |ssn        |bonus1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|1          |Scott     |Tiger    |1000.0|10   |united states |+1 123 456 7890 |123 45 6789|10    |\n",
      "|2          |Henry     |Ford     |1250.0|null |India         |+91 234 567 8901|456 78 9123|0     |\n",
      "|3          |Nick      |Junior   |750.0 |     |united KINGDOM|+44 111 111 1111|222 33 4444|0     |\n",
      "|4          |Bill      |Gomes    |1500.0|10   |AUSTRALIA     |+61 987 654 3210|789 12 6118|10    |\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus1', coalesce(col('bonus').cast('int'),lit(0))). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d17264f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f0b8ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|employee_id|first_name|last_name|salary|bonus|nationality   |phone_number    |ssn        |bonus1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|1          |Scott     |Tiger    |1000.0|10   |united states |+1 123 456 7890 |123 45 6789|10    |\n",
      "|2          |Henry     |Ford     |1250.0|null |India         |+91 234 567 8901|456 78 9123|0     |\n",
      "|3          |Nick      |Junior   |750.0 |     |united KINGDOM|+44 111 111 1111|222 33 4444|0     |\n",
      "|4          |Bill      |Gomes    |1500.0|10   |AUSTRALIA     |+61 987 654 3210|789 12 6118|10    |\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus1',expr(\"\"\"\n",
    "                    CASE WHEN bonus IS NULL OR bonus='' THEN 0\n",
    "                    ELSE bonus\n",
    "                    END\n",
    "                    \"\"\")). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c767742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "265fa498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|employee_id|first_name|last_name|salary|bonus|nationality   |phone_number    |ssn        |bonus1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "|1          |Scott     |Tiger    |1000.0|10   |united states |+1 123 456 7890 |123 45 6789|10    |\n",
      "|2          |Henry     |Ford     |1250.0|null |India         |+91 234 567 8901|456 78 9123|0     |\n",
      "|3          |Nick      |Junior   |750.0 |     |united KINGDOM|+44 111 111 1111|222 33 4444|0     |\n",
      "|4          |Bill      |Gomes    |1500.0|10   |AUSTRALIA     |+61 987 654 3210|789 12 6118|10    |\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('bonus1', when((col('bonus').isNull()) | (col('bonus')==lit('')),0).otherwise(col('bonus'))). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d54eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = [\n",
    "    (1, 1),\n",
    "    (2, 13),\n",
    "    (3, 18),\n",
    "    (4, 60),\n",
    "    (5, 120),\n",
    "    (6, 0),\n",
    "    (7, 12),\n",
    "    (8, 160)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45401dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "personsDF = spark.createDataFrame(persons, schema='id INT, age INT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac4ccdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2| 13|\n",
      "|  3| 18|\n",
      "|  4| 60|\n",
      "|  5|120|\n",
      "|  6|  0|\n",
      "|  7| 12|\n",
      "|  8|160|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54915a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------------+\n",
      "| id|age|         category|\n",
      "+---+---+-----------------+\n",
      "|  1|  1|          New Bon|\n",
      "|  2| 13|          Toddler|\n",
      "|  3| 18|          Toddler|\n",
      "|  4| 60|              Kid|\n",
      "|  5|120|              Kid|\n",
      "|  6|  0|          New Bon|\n",
      "|  7| 12|           Infant|\n",
      "|  8|160|Teenager or Adult|\n",
      "+---+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF. \\\n",
    "    withColumn(\n",
    "        'category',\n",
    "        expr(\"\"\"\n",
    "            CASE\n",
    "            WHEN age BETWEEN 0 AND 2 THEN 'New Bon'\n",
    "            WHEN age > 2 AND age <=12 THEN 'Infant'\n",
    "            WHEN age > 12 AND age <= 48 THEN 'Toddler'\n",
    "            WHEN age >48  AND age <= 144 THEN 'Kid'\n",
    "            ELSE 'Teenager or Adult'\n",
    "            END\n",
    "        \"\"\")\n",
    "    ). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "943fdb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------------+\n",
      "|id |age|categry          |\n",
      "+---+---+-----------------+\n",
      "|1  |1  |New Born         |\n",
      "|2  |13 |Toddler          |\n",
      "|3  |18 |Toddler          |\n",
      "|4  |60 |Kid              |\n",
      "|5  |120|Kid              |\n",
      "|6  |0  |New Born         |\n",
      "|7  |12 |Infant           |\n",
      "|8  |160|Teenager or Adult|\n",
      "+---+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF. \\\n",
    "    withColumn('categry',\n",
    "               when(col('age').between(0,2),'New Born').\n",
    "               when((col('age')>2) & (col('age')<=12),'Infant').\n",
    "               when((col('age')>12) & (col('age')<=48),'Toddler').\n",
    "               when((col('age')>48) & (col('age')<=144), 'Kid').\n",
    "               otherwise('Teenager or Adult')\n",
    "              ). \\\n",
    "show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f8970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
